{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redesneurais.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeeRT9US27K"
      },
      "source": [
        "Redes neurais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdVWRZ16S4R4"
      },
      "source": [
        "base credit data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AvMe0r-S6RE"
      },
      "source": [
        "#Abrindo a biblioteca responsável pela implementação do algoritmo, temos:\n",
        "#Onde MLPClassifier significa \"multilayer perceptron\", ou seja, um perceptron com mais de uma camada\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_T5Ca3KcGD6"
      },
      "source": [
        "import pickle\n",
        "with open('credit.pkl', 'rb') as f:\n",
        "  x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-FnQ3P8cVCt",
        "outputId": "88a72400-7a8a-44a4-99ff-9b67b01647d9"
      },
      "source": [
        "x_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIyXk5x5cacp",
        "outputId": "a63dd507-29eb-443b-81e2-9c40421bea2f"
      },
      "source": [
        "x_credit_treinamento"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.3754462 ,  0.50631087,  0.10980934],\n",
              "       [ 1.45826409, -1.6489393 , -1.21501497],\n",
              "       [-0.79356829,  0.22531191, -0.43370226],\n",
              "       ...,\n",
              "       [ 0.21738243, -0.14704404,  1.40872498],\n",
              "       [ 0.58716195,  0.66435493,  0.67948086],\n",
              "       [ 0.68315357,  0.04084946,  1.91819744]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDCRamiecvCA",
        "outputId": "61ad5fb2-f9fc-4f36-a8ea-1b0f72f5d9d7"
      },
      "source": [
        "y_credit_treinamento"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9CP7AM0cz-F",
        "outputId": "3fd169f8-a686-4bbb-dce3-285aa161ac01"
      },
      "source": [
        "#os argumentos max_iter refere-se a quantidade máxima de iterações para o ajuste dos pesos da rede neural\n",
        "#o argumento tol o erro máximo tolerado, quanto o programa atingir essa variação de erro o programa encerra\n",
        "#Verbose = True para exibir os resultados de cada iteração\n",
        "neural_credit = MLPClassifier(max_iter=5000,tol=0.00001,verbose=True, solver ='adam', activation = 'relu',hidden_layer_sizes =(2,2))\n",
        "neural_credit.fit(x_credit_treinamento,y_credit_treinamento)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.33392274\n",
            "Iteration 2, loss = 2.28835966\n",
            "Iteration 3, loss = 2.24409141\n",
            "Iteration 4, loss = 2.20079767\n",
            "Iteration 5, loss = 2.15863844\n",
            "Iteration 6, loss = 2.11830628\n",
            "Iteration 7, loss = 2.07838352\n",
            "Iteration 8, loss = 2.04053097\n",
            "Iteration 9, loss = 2.00312356\n",
            "Iteration 10, loss = 1.96717410\n",
            "Iteration 11, loss = 1.93210570\n",
            "Iteration 12, loss = 1.89812443\n",
            "Iteration 13, loss = 1.86559867\n",
            "Iteration 14, loss = 1.83389865\n",
            "Iteration 15, loss = 1.80297231\n",
            "Iteration 16, loss = 1.77366035\n",
            "Iteration 17, loss = 1.74481134\n",
            "Iteration 18, loss = 1.71699267\n",
            "Iteration 19, loss = 1.69030335\n",
            "Iteration 20, loss = 1.66399429\n",
            "Iteration 21, loss = 1.63859926\n",
            "Iteration 22, loss = 1.61391590\n",
            "Iteration 23, loss = 1.58996152\n",
            "Iteration 24, loss = 1.56665528\n",
            "Iteration 25, loss = 1.54420657\n",
            "Iteration 26, loss = 1.52225393\n",
            "Iteration 27, loss = 1.50119543\n",
            "Iteration 28, loss = 1.48077264\n",
            "Iteration 29, loss = 1.46087892\n",
            "Iteration 30, loss = 1.44149914\n",
            "Iteration 31, loss = 1.42280259\n",
            "Iteration 32, loss = 1.40469296\n",
            "Iteration 33, loss = 1.38697563\n",
            "Iteration 34, loss = 1.36986870\n",
            "Iteration 35, loss = 1.35300850\n",
            "Iteration 36, loss = 1.33686462\n",
            "Iteration 37, loss = 1.32124040\n",
            "Iteration 38, loss = 1.30586069\n",
            "Iteration 39, loss = 1.29116277\n",
            "Iteration 40, loss = 1.27695182\n",
            "Iteration 41, loss = 1.26281017\n",
            "Iteration 42, loss = 1.24915516\n",
            "Iteration 43, loss = 1.23592396\n",
            "Iteration 44, loss = 1.22289426\n",
            "Iteration 45, loss = 1.21028916\n",
            "Iteration 46, loss = 1.19825669\n",
            "Iteration 47, loss = 1.18638213\n",
            "Iteration 48, loss = 1.17484222\n",
            "Iteration 49, loss = 1.16360075\n",
            "Iteration 50, loss = 1.15279352\n",
            "Iteration 51, loss = 1.14206278\n",
            "Iteration 52, loss = 1.13194089\n",
            "Iteration 53, loss = 1.12182391\n",
            "Iteration 54, loss = 1.11216984\n",
            "Iteration 55, loss = 1.10268155\n",
            "Iteration 56, loss = 1.09353081\n",
            "Iteration 57, loss = 1.08452003\n",
            "Iteration 58, loss = 1.07584223\n",
            "Iteration 59, loss = 1.06720049\n",
            "Iteration 60, loss = 1.05899376\n",
            "Iteration 61, loss = 1.05074074\n",
            "Iteration 62, loss = 1.04290480\n",
            "Iteration 63, loss = 1.03524763\n",
            "Iteration 64, loss = 1.02773935\n",
            "Iteration 65, loss = 1.02048102\n",
            "Iteration 66, loss = 1.01347980\n",
            "Iteration 67, loss = 1.00653050\n",
            "Iteration 68, loss = 0.99985108\n",
            "Iteration 69, loss = 0.99326990\n",
            "Iteration 70, loss = 0.98680460\n",
            "Iteration 71, loss = 0.98062897\n",
            "Iteration 72, loss = 0.97448819\n",
            "Iteration 73, loss = 0.96860232\n",
            "Iteration 74, loss = 0.96275543\n",
            "Iteration 75, loss = 0.95707116\n",
            "Iteration 76, loss = 0.95153329\n",
            "Iteration 77, loss = 0.94603732\n",
            "Iteration 78, loss = 0.94067431\n",
            "Iteration 79, loss = 0.93545677\n",
            "Iteration 80, loss = 0.93040215\n",
            "Iteration 81, loss = 0.92545776\n",
            "Iteration 82, loss = 0.92059741\n",
            "Iteration 83, loss = 0.91585442\n",
            "Iteration 84, loss = 0.91123714\n",
            "Iteration 85, loss = 0.90663330\n",
            "Iteration 86, loss = 0.90227407\n",
            "Iteration 87, loss = 0.89791413\n",
            "Iteration 88, loss = 0.89366672\n",
            "Iteration 89, loss = 0.88950437\n",
            "Iteration 90, loss = 0.88536905\n",
            "Iteration 91, loss = 0.88139618\n",
            "Iteration 92, loss = 0.87751251\n",
            "Iteration 93, loss = 0.87371174\n",
            "Iteration 94, loss = 0.86995406\n",
            "Iteration 95, loss = 0.86632510\n",
            "Iteration 96, loss = 0.86271102\n",
            "Iteration 97, loss = 0.85917411\n",
            "Iteration 98, loss = 0.85570795\n",
            "Iteration 99, loss = 0.85223346\n",
            "Iteration 100, loss = 0.84881923\n",
            "Iteration 101, loss = 0.84549560\n",
            "Iteration 102, loss = 0.84219837\n",
            "Iteration 103, loss = 0.83897926\n",
            "Iteration 104, loss = 0.83581456\n",
            "Iteration 105, loss = 0.83273962\n",
            "Iteration 106, loss = 0.82970221\n",
            "Iteration 107, loss = 0.82669091\n",
            "Iteration 108, loss = 0.82374560\n",
            "Iteration 109, loss = 0.82077950\n",
            "Iteration 110, loss = 0.81790351\n",
            "Iteration 111, loss = 0.81504576\n",
            "Iteration 112, loss = 0.81227464\n",
            "Iteration 113, loss = 0.80950238\n",
            "Iteration 114, loss = 0.80674589\n",
            "Iteration 115, loss = 0.80405378\n",
            "Iteration 116, loss = 0.80135494\n",
            "Iteration 117, loss = 0.79869010\n",
            "Iteration 118, loss = 0.79600886\n",
            "Iteration 119, loss = 0.79340220\n",
            "Iteration 120, loss = 0.79077470\n",
            "Iteration 121, loss = 0.78817169\n",
            "Iteration 122, loss = 0.78559788\n",
            "Iteration 123, loss = 0.78302208\n",
            "Iteration 124, loss = 0.78050169\n",
            "Iteration 125, loss = 0.77793087\n",
            "Iteration 126, loss = 0.77544313\n",
            "Iteration 127, loss = 0.77291478\n",
            "Iteration 128, loss = 0.77040982\n",
            "Iteration 129, loss = 0.76792851\n",
            "Iteration 130, loss = 0.76546020\n",
            "Iteration 131, loss = 0.76300270\n",
            "Iteration 132, loss = 0.76056213\n",
            "Iteration 133, loss = 0.75814551\n",
            "Iteration 134, loss = 0.75572329\n",
            "Iteration 135, loss = 0.75333206\n",
            "Iteration 136, loss = 0.75091655\n",
            "Iteration 137, loss = 0.74856068\n",
            "Iteration 138, loss = 0.74620112\n",
            "Iteration 139, loss = 0.74383763\n",
            "Iteration 140, loss = 0.74149291\n",
            "Iteration 141, loss = 0.73918343\n",
            "Iteration 142, loss = 0.73684299\n",
            "Iteration 143, loss = 0.73453423\n",
            "Iteration 144, loss = 0.73224291\n",
            "Iteration 145, loss = 0.72994822\n",
            "Iteration 146, loss = 0.72768547\n",
            "Iteration 147, loss = 0.72541608\n",
            "Iteration 148, loss = 0.72320051\n",
            "Iteration 149, loss = 0.72093914\n",
            "Iteration 150, loss = 0.71871409\n",
            "Iteration 151, loss = 0.71650336\n",
            "Iteration 152, loss = 0.71431321\n",
            "Iteration 153, loss = 0.71212061\n",
            "Iteration 154, loss = 0.70993627\n",
            "Iteration 155, loss = 0.70777808\n",
            "Iteration 156, loss = 0.70563535\n",
            "Iteration 157, loss = 0.70348647\n",
            "Iteration 158, loss = 0.70134664\n",
            "Iteration 159, loss = 0.69923277\n",
            "Iteration 160, loss = 0.69713529\n",
            "Iteration 161, loss = 0.69504025\n",
            "Iteration 162, loss = 0.69294248\n",
            "Iteration 163, loss = 0.69088267\n",
            "Iteration 164, loss = 0.68883538\n",
            "Iteration 165, loss = 0.68677517\n",
            "Iteration 166, loss = 0.68475928\n",
            "Iteration 167, loss = 0.68273907\n",
            "Iteration 168, loss = 0.68072042\n",
            "Iteration 169, loss = 0.67873569\n",
            "Iteration 170, loss = 0.67672674\n",
            "Iteration 171, loss = 0.67474898\n",
            "Iteration 172, loss = 0.67280760\n",
            "Iteration 173, loss = 0.67082737\n",
            "Iteration 174, loss = 0.66888594\n",
            "Iteration 175, loss = 0.66697472\n",
            "Iteration 176, loss = 0.66504972\n",
            "Iteration 177, loss = 0.66314959\n",
            "Iteration 178, loss = 0.66124275\n",
            "Iteration 179, loss = 0.65937152\n",
            "Iteration 180, loss = 0.65749508\n",
            "Iteration 181, loss = 0.65563186\n",
            "Iteration 182, loss = 0.65379307\n",
            "Iteration 183, loss = 0.65192449\n",
            "Iteration 184, loss = 0.65012464\n",
            "Iteration 185, loss = 0.64827367\n",
            "Iteration 186, loss = 0.64648354\n",
            "Iteration 187, loss = 0.64470100\n",
            "Iteration 188, loss = 0.64288374\n",
            "Iteration 189, loss = 0.64112560\n",
            "Iteration 190, loss = 0.63934712\n",
            "Iteration 191, loss = 0.63761932\n",
            "Iteration 192, loss = 0.63584914\n",
            "Iteration 193, loss = 0.63414927\n",
            "Iteration 194, loss = 0.63240545\n",
            "Iteration 195, loss = 0.63070051\n",
            "Iteration 196, loss = 0.62898585\n",
            "Iteration 197, loss = 0.62729351\n",
            "Iteration 198, loss = 0.62560156\n",
            "Iteration 199, loss = 0.62394326\n",
            "Iteration 200, loss = 0.62229510\n",
            "Iteration 201, loss = 0.62059804\n",
            "Iteration 202, loss = 0.61897667\n",
            "Iteration 203, loss = 0.61735433\n",
            "Iteration 204, loss = 0.61574105\n",
            "Iteration 205, loss = 0.61410661\n",
            "Iteration 206, loss = 0.61252377\n",
            "Iteration 207, loss = 0.61088381\n",
            "Iteration 208, loss = 0.60934330\n",
            "Iteration 209, loss = 0.60772486\n",
            "Iteration 210, loss = 0.60613549\n",
            "Iteration 211, loss = 0.60457789\n",
            "Iteration 212, loss = 0.60300331\n",
            "Iteration 213, loss = 0.60146023\n",
            "Iteration 214, loss = 0.59992442\n",
            "Iteration 215, loss = 0.59839805\n",
            "Iteration 216, loss = 0.59687185\n",
            "Iteration 217, loss = 0.59535849\n",
            "Iteration 218, loss = 0.59388870\n",
            "Iteration 219, loss = 0.59242333\n",
            "Iteration 220, loss = 0.59093507\n",
            "Iteration 221, loss = 0.58947466\n",
            "Iteration 222, loss = 0.58800620\n",
            "Iteration 223, loss = 0.58658596\n",
            "Iteration 224, loss = 0.58512540\n",
            "Iteration 225, loss = 0.58368613\n",
            "Iteration 226, loss = 0.58224701\n",
            "Iteration 227, loss = 0.58082524\n",
            "Iteration 228, loss = 0.57941276\n",
            "Iteration 229, loss = 0.57800146\n",
            "Iteration 230, loss = 0.57660472\n",
            "Iteration 231, loss = 0.57520235\n",
            "Iteration 232, loss = 0.57380967\n",
            "Iteration 233, loss = 0.57242946\n",
            "Iteration 234, loss = 0.57102821\n",
            "Iteration 235, loss = 0.56966855\n",
            "Iteration 236, loss = 0.56828628\n",
            "Iteration 237, loss = 0.56690830\n",
            "Iteration 238, loss = 0.56554236\n",
            "Iteration 239, loss = 0.56415244\n",
            "Iteration 240, loss = 0.56275202\n",
            "Iteration 241, loss = 0.56136811\n",
            "Iteration 242, loss = 0.55996952\n",
            "Iteration 243, loss = 0.55855180\n",
            "Iteration 244, loss = 0.55715416\n",
            "Iteration 245, loss = 0.55572105\n",
            "Iteration 246, loss = 0.55429536\n",
            "Iteration 247, loss = 0.55285128\n",
            "Iteration 248, loss = 0.55142481\n",
            "Iteration 249, loss = 0.54996815\n",
            "Iteration 250, loss = 0.54851587\n",
            "Iteration 251, loss = 0.54703462\n",
            "Iteration 252, loss = 0.54554554\n",
            "Iteration 253, loss = 0.54407401\n",
            "Iteration 254, loss = 0.54256530\n",
            "Iteration 255, loss = 0.54103663\n",
            "Iteration 256, loss = 0.53949945\n",
            "Iteration 257, loss = 0.53794370\n",
            "Iteration 258, loss = 0.53636776\n",
            "Iteration 259, loss = 0.53475775\n",
            "Iteration 260, loss = 0.53313920\n",
            "Iteration 261, loss = 0.53151127\n",
            "Iteration 262, loss = 0.52991182\n",
            "Iteration 263, loss = 0.52830890\n",
            "Iteration 264, loss = 0.52671157\n",
            "Iteration 265, loss = 0.52511930\n",
            "Iteration 266, loss = 0.52353474\n",
            "Iteration 267, loss = 0.52193967\n",
            "Iteration 268, loss = 0.52036103\n",
            "Iteration 269, loss = 0.51877020\n",
            "Iteration 270, loss = 0.51721948\n",
            "Iteration 271, loss = 0.51565959\n",
            "Iteration 272, loss = 0.51411923\n",
            "Iteration 273, loss = 0.51256393\n",
            "Iteration 274, loss = 0.51105980\n",
            "Iteration 275, loss = 0.50951370\n",
            "Iteration 276, loss = 0.50800545\n",
            "Iteration 277, loss = 0.50648377\n",
            "Iteration 278, loss = 0.50497361\n",
            "Iteration 279, loss = 0.50345925\n",
            "Iteration 280, loss = 0.50196826\n",
            "Iteration 281, loss = 0.50047278\n",
            "Iteration 282, loss = 0.49897659\n",
            "Iteration 283, loss = 0.49748434\n",
            "Iteration 284, loss = 0.49599544\n",
            "Iteration 285, loss = 0.49450003\n",
            "Iteration 286, loss = 0.49301864\n",
            "Iteration 287, loss = 0.49150823\n",
            "Iteration 288, loss = 0.49003036\n",
            "Iteration 289, loss = 0.48850645\n",
            "Iteration 290, loss = 0.48701373\n",
            "Iteration 291, loss = 0.48550414\n",
            "Iteration 292, loss = 0.48399646\n",
            "Iteration 293, loss = 0.48246182\n",
            "Iteration 294, loss = 0.48090735\n",
            "Iteration 295, loss = 0.47935266\n",
            "Iteration 296, loss = 0.47776009\n",
            "Iteration 297, loss = 0.47618021\n",
            "Iteration 298, loss = 0.47457374\n",
            "Iteration 299, loss = 0.47298055\n",
            "Iteration 300, loss = 0.47137229\n",
            "Iteration 301, loss = 0.46972675\n",
            "Iteration 302, loss = 0.46805748\n",
            "Iteration 303, loss = 0.46639562\n",
            "Iteration 304, loss = 0.46468820\n",
            "Iteration 305, loss = 0.46296601\n",
            "Iteration 306, loss = 0.46116454\n",
            "Iteration 307, loss = 0.45936739\n",
            "Iteration 308, loss = 0.45757373\n",
            "Iteration 309, loss = 0.45573205\n",
            "Iteration 310, loss = 0.45387393\n",
            "Iteration 311, loss = 0.45201042\n",
            "Iteration 312, loss = 0.45013152\n",
            "Iteration 313, loss = 0.44826716\n",
            "Iteration 314, loss = 0.44636042\n",
            "Iteration 315, loss = 0.44445812\n",
            "Iteration 316, loss = 0.44260753\n",
            "Iteration 317, loss = 0.44072358\n",
            "Iteration 318, loss = 0.43881276\n",
            "Iteration 319, loss = 0.43693748\n",
            "Iteration 320, loss = 0.43508865\n",
            "Iteration 321, loss = 0.43313593\n",
            "Iteration 322, loss = 0.43126297\n",
            "Iteration 323, loss = 0.42943346\n",
            "Iteration 324, loss = 0.42751675\n",
            "Iteration 325, loss = 0.42568551\n",
            "Iteration 326, loss = 0.42389838\n",
            "Iteration 327, loss = 0.42203055\n",
            "Iteration 328, loss = 0.42018575\n",
            "Iteration 329, loss = 0.41837709\n",
            "Iteration 330, loss = 0.41654305\n",
            "Iteration 331, loss = 0.41471334\n",
            "Iteration 332, loss = 0.41294061\n",
            "Iteration 333, loss = 0.41112727\n",
            "Iteration 334, loss = 0.40936006\n",
            "Iteration 335, loss = 0.40759352\n",
            "Iteration 336, loss = 0.40583264\n",
            "Iteration 337, loss = 0.40410541\n",
            "Iteration 338, loss = 0.40237201\n",
            "Iteration 339, loss = 0.40066709\n",
            "Iteration 340, loss = 0.39898321\n",
            "Iteration 341, loss = 0.39728955\n",
            "Iteration 342, loss = 0.39565343\n",
            "Iteration 343, loss = 0.39396373\n",
            "Iteration 344, loss = 0.39232666\n",
            "Iteration 345, loss = 0.39071760\n",
            "Iteration 346, loss = 0.38910240\n",
            "Iteration 347, loss = 0.38748365\n",
            "Iteration 348, loss = 0.38590438\n",
            "Iteration 349, loss = 0.38429415\n",
            "Iteration 350, loss = 0.38273364\n",
            "Iteration 351, loss = 0.38119208\n",
            "Iteration 352, loss = 0.37965332\n",
            "Iteration 353, loss = 0.37813377\n",
            "Iteration 354, loss = 0.37663481\n",
            "Iteration 355, loss = 0.37514430\n",
            "Iteration 356, loss = 0.37370413\n",
            "Iteration 357, loss = 0.37222931\n",
            "Iteration 358, loss = 0.37078019\n",
            "Iteration 359, loss = 0.36935983\n",
            "Iteration 360, loss = 0.36793130\n",
            "Iteration 361, loss = 0.36650737\n",
            "Iteration 362, loss = 0.36510242\n",
            "Iteration 363, loss = 0.36370863\n",
            "Iteration 364, loss = 0.36234039\n",
            "Iteration 365, loss = 0.36095493\n",
            "Iteration 366, loss = 0.35959042\n",
            "Iteration 367, loss = 0.35823038\n",
            "Iteration 368, loss = 0.35690156\n",
            "Iteration 369, loss = 0.35559153\n",
            "Iteration 370, loss = 0.35425697\n",
            "Iteration 371, loss = 0.35293520\n",
            "Iteration 372, loss = 0.35164563\n",
            "Iteration 373, loss = 0.35035505\n",
            "Iteration 374, loss = 0.34906940\n",
            "Iteration 375, loss = 0.34778630\n",
            "Iteration 376, loss = 0.34652429\n",
            "Iteration 377, loss = 0.34525349\n",
            "Iteration 378, loss = 0.34400233\n",
            "Iteration 379, loss = 0.34277614\n",
            "Iteration 380, loss = 0.34152442\n",
            "Iteration 381, loss = 0.34029190\n",
            "Iteration 382, loss = 0.33905954\n",
            "Iteration 383, loss = 0.33784035\n",
            "Iteration 384, loss = 0.33664783\n",
            "Iteration 385, loss = 0.33543713\n",
            "Iteration 386, loss = 0.33423535\n",
            "Iteration 387, loss = 0.33302905\n",
            "Iteration 388, loss = 0.33184794\n",
            "Iteration 389, loss = 0.33065488\n",
            "Iteration 390, loss = 0.32948468\n",
            "Iteration 391, loss = 0.32832241\n",
            "Iteration 392, loss = 0.32715622\n",
            "Iteration 393, loss = 0.32600730\n",
            "Iteration 394, loss = 0.32484433\n",
            "Iteration 395, loss = 0.32371570\n",
            "Iteration 396, loss = 0.32256686\n",
            "Iteration 397, loss = 0.32143529\n",
            "Iteration 398, loss = 0.32031709\n",
            "Iteration 399, loss = 0.31920319\n",
            "Iteration 400, loss = 0.31810044\n",
            "Iteration 401, loss = 0.31699514\n",
            "Iteration 402, loss = 0.31588472\n",
            "Iteration 403, loss = 0.31481052\n",
            "Iteration 404, loss = 0.31371469\n",
            "Iteration 405, loss = 0.31264130\n",
            "Iteration 406, loss = 0.31157816\n",
            "Iteration 407, loss = 0.31047783\n",
            "Iteration 408, loss = 0.30943102\n",
            "Iteration 409, loss = 0.30838865\n",
            "Iteration 410, loss = 0.30733945\n",
            "Iteration 411, loss = 0.30631218\n",
            "Iteration 412, loss = 0.30526204\n",
            "Iteration 413, loss = 0.30425778\n",
            "Iteration 414, loss = 0.30319774\n",
            "Iteration 415, loss = 0.30218624\n",
            "Iteration 416, loss = 0.30118449\n",
            "Iteration 417, loss = 0.30018512\n",
            "Iteration 418, loss = 0.29918675\n",
            "Iteration 419, loss = 0.29820270\n",
            "Iteration 420, loss = 0.29721567\n",
            "Iteration 421, loss = 0.29624637\n",
            "Iteration 422, loss = 0.29527724\n",
            "Iteration 423, loss = 0.29430546\n",
            "Iteration 424, loss = 0.29335495\n",
            "Iteration 425, loss = 0.29243759\n",
            "Iteration 426, loss = 0.29146415\n",
            "Iteration 427, loss = 0.29054049\n",
            "Iteration 428, loss = 0.28958355\n",
            "Iteration 429, loss = 0.28866482\n",
            "Iteration 430, loss = 0.28774307\n",
            "Iteration 431, loss = 0.28684129\n",
            "Iteration 432, loss = 0.28593295\n",
            "Iteration 433, loss = 0.28502821\n",
            "Iteration 434, loss = 0.28413693\n",
            "Iteration 435, loss = 0.28324822\n",
            "Iteration 436, loss = 0.28237307\n",
            "Iteration 437, loss = 0.28151060\n",
            "Iteration 438, loss = 0.28063840\n",
            "Iteration 439, loss = 0.27976691\n",
            "Iteration 440, loss = 0.27891342\n",
            "Iteration 441, loss = 0.27806489\n",
            "Iteration 442, loss = 0.27720322\n",
            "Iteration 443, loss = 0.27636917\n",
            "Iteration 444, loss = 0.27554352\n",
            "Iteration 445, loss = 0.27470248\n",
            "Iteration 446, loss = 0.27388290\n",
            "Iteration 447, loss = 0.27304948\n",
            "Iteration 448, loss = 0.27222876\n",
            "Iteration 449, loss = 0.27144476\n",
            "Iteration 450, loss = 0.27063041\n",
            "Iteration 451, loss = 0.26982466\n",
            "Iteration 452, loss = 0.26901214\n",
            "Iteration 453, loss = 0.26822425\n",
            "Iteration 454, loss = 0.26745201\n",
            "Iteration 455, loss = 0.26665380\n",
            "Iteration 456, loss = 0.26586839\n",
            "Iteration 457, loss = 0.26511011\n",
            "Iteration 458, loss = 0.26433549\n",
            "Iteration 459, loss = 0.26358566\n",
            "Iteration 460, loss = 0.26283042\n",
            "Iteration 461, loss = 0.26209227\n",
            "Iteration 462, loss = 0.26133407\n",
            "Iteration 463, loss = 0.26057611\n",
            "Iteration 464, loss = 0.25983417\n",
            "Iteration 465, loss = 0.25909305\n",
            "Iteration 466, loss = 0.25834927\n",
            "Iteration 467, loss = 0.25763323\n",
            "Iteration 468, loss = 0.25690185\n",
            "Iteration 469, loss = 0.25619090\n",
            "Iteration 470, loss = 0.25549214\n",
            "Iteration 471, loss = 0.25477969\n",
            "Iteration 472, loss = 0.25409178\n",
            "Iteration 473, loss = 0.25338539\n",
            "Iteration 474, loss = 0.25269913\n",
            "Iteration 475, loss = 0.25201962\n",
            "Iteration 476, loss = 0.25131787\n",
            "Iteration 477, loss = 0.25065564\n",
            "Iteration 478, loss = 0.24997668\n",
            "Iteration 479, loss = 0.24929852\n",
            "Iteration 480, loss = 0.24861895\n",
            "Iteration 481, loss = 0.24796630\n",
            "Iteration 482, loss = 0.24731407\n",
            "Iteration 483, loss = 0.24667005\n",
            "Iteration 484, loss = 0.24599094\n",
            "Iteration 485, loss = 0.24537864\n",
            "Iteration 486, loss = 0.24471659\n",
            "Iteration 487, loss = 0.24407367\n",
            "Iteration 488, loss = 0.24342007\n",
            "Iteration 489, loss = 0.24279157\n",
            "Iteration 490, loss = 0.24219772\n",
            "Iteration 491, loss = 0.24154383\n",
            "Iteration 492, loss = 0.24092857\n",
            "Iteration 493, loss = 0.24034377\n",
            "Iteration 494, loss = 0.23969263\n",
            "Iteration 495, loss = 0.23910470\n",
            "Iteration 496, loss = 0.23850997\n",
            "Iteration 497, loss = 0.23791834\n",
            "Iteration 498, loss = 0.23732479\n",
            "Iteration 499, loss = 0.23671301\n",
            "Iteration 500, loss = 0.23614171\n",
            "Iteration 501, loss = 0.23554714\n",
            "Iteration 502, loss = 0.23499038\n",
            "Iteration 503, loss = 0.23437369\n",
            "Iteration 504, loss = 0.23381855\n",
            "Iteration 505, loss = 0.23329895\n",
            "Iteration 506, loss = 0.23269943\n",
            "Iteration 507, loss = 0.23213359\n",
            "Iteration 508, loss = 0.23156866\n",
            "Iteration 509, loss = 0.23101404\n",
            "Iteration 510, loss = 0.23045239\n",
            "Iteration 511, loss = 0.22990317\n",
            "Iteration 512, loss = 0.22937452\n",
            "Iteration 513, loss = 0.22883697\n",
            "Iteration 514, loss = 0.22829137\n",
            "Iteration 515, loss = 0.22779411\n",
            "Iteration 516, loss = 0.22723583\n",
            "Iteration 517, loss = 0.22668893\n",
            "Iteration 518, loss = 0.22617692\n",
            "Iteration 519, loss = 0.22566660\n",
            "Iteration 520, loss = 0.22511799\n",
            "Iteration 521, loss = 0.22461852\n",
            "Iteration 522, loss = 0.22408140\n",
            "Iteration 523, loss = 0.22356375\n",
            "Iteration 524, loss = 0.22305808\n",
            "Iteration 525, loss = 0.22256492\n",
            "Iteration 526, loss = 0.22207962\n",
            "Iteration 527, loss = 0.22156051\n",
            "Iteration 528, loss = 0.22108380\n",
            "Iteration 529, loss = 0.22057428\n",
            "Iteration 530, loss = 0.22010416\n",
            "Iteration 531, loss = 0.21961512\n",
            "Iteration 532, loss = 0.21909879\n",
            "Iteration 533, loss = 0.21863935\n",
            "Iteration 534, loss = 0.21818751\n",
            "Iteration 535, loss = 0.21766337\n",
            "Iteration 536, loss = 0.21720047\n",
            "Iteration 537, loss = 0.21674914\n",
            "Iteration 538, loss = 0.21625946\n",
            "Iteration 539, loss = 0.21580319\n",
            "Iteration 540, loss = 0.21533162\n",
            "Iteration 541, loss = 0.21486827\n",
            "Iteration 542, loss = 0.21440141\n",
            "Iteration 543, loss = 0.21398755\n",
            "Iteration 544, loss = 0.21349610\n",
            "Iteration 545, loss = 0.21307210\n",
            "Iteration 546, loss = 0.21258355\n",
            "Iteration 547, loss = 0.21215790\n",
            "Iteration 548, loss = 0.21173772\n",
            "Iteration 549, loss = 0.21126336\n",
            "Iteration 550, loss = 0.21085625\n",
            "Iteration 551, loss = 0.21040545\n",
            "Iteration 552, loss = 0.20997205\n",
            "Iteration 553, loss = 0.20953897\n",
            "Iteration 554, loss = 0.20912053\n",
            "Iteration 555, loss = 0.20871262\n",
            "Iteration 556, loss = 0.20828202\n",
            "Iteration 557, loss = 0.20785549\n",
            "Iteration 558, loss = 0.20751270\n",
            "Iteration 559, loss = 0.20703108\n",
            "Iteration 560, loss = 0.20664959\n",
            "Iteration 561, loss = 0.20618724\n",
            "Iteration 562, loss = 0.20576969\n",
            "Iteration 563, loss = 0.20540838\n",
            "Iteration 564, loss = 0.20498952\n",
            "Iteration 565, loss = 0.20457221\n",
            "Iteration 566, loss = 0.20421914\n",
            "Iteration 567, loss = 0.20377022\n",
            "Iteration 568, loss = 0.20338904\n",
            "Iteration 569, loss = 0.20301078\n",
            "Iteration 570, loss = 0.20259253\n",
            "Iteration 571, loss = 0.20224001\n",
            "Iteration 572, loss = 0.20186557\n",
            "Iteration 573, loss = 0.20144401\n",
            "Iteration 574, loss = 0.20106356\n",
            "Iteration 575, loss = 0.20070555\n",
            "Iteration 576, loss = 0.20031385\n",
            "Iteration 577, loss = 0.19998306\n",
            "Iteration 578, loss = 0.19959952\n",
            "Iteration 579, loss = 0.19919591\n",
            "Iteration 580, loss = 0.19883875\n",
            "Iteration 581, loss = 0.19845538\n",
            "Iteration 582, loss = 0.19808646\n",
            "Iteration 583, loss = 0.19775325\n",
            "Iteration 584, loss = 0.19735943\n",
            "Iteration 585, loss = 0.19700491\n",
            "Iteration 586, loss = 0.19667988\n",
            "Iteration 587, loss = 0.19628778\n",
            "Iteration 588, loss = 0.19594655\n",
            "Iteration 589, loss = 0.19558761\n",
            "Iteration 590, loss = 0.19524362\n",
            "Iteration 591, loss = 0.19487353\n",
            "Iteration 592, loss = 0.19452784\n",
            "Iteration 593, loss = 0.19416769\n",
            "Iteration 594, loss = 0.19382056\n",
            "Iteration 595, loss = 0.19348527\n",
            "Iteration 596, loss = 0.19318019\n",
            "Iteration 597, loss = 0.19280453\n",
            "Iteration 598, loss = 0.19245689\n",
            "Iteration 599, loss = 0.19214683\n",
            "Iteration 600, loss = 0.19178881\n",
            "Iteration 601, loss = 0.19145816\n",
            "Iteration 602, loss = 0.19117273\n",
            "Iteration 603, loss = 0.19084162\n",
            "Iteration 604, loss = 0.19049359\n",
            "Iteration 605, loss = 0.19018111\n",
            "Iteration 606, loss = 0.18987641\n",
            "Iteration 607, loss = 0.18961614\n",
            "Iteration 608, loss = 0.18919637\n",
            "Iteration 609, loss = 0.18891765\n",
            "Iteration 610, loss = 0.18860439\n",
            "Iteration 611, loss = 0.18825599\n",
            "Iteration 612, loss = 0.18795374\n",
            "Iteration 613, loss = 0.18764612\n",
            "Iteration 614, loss = 0.18733187\n",
            "Iteration 615, loss = 0.18701451\n",
            "Iteration 616, loss = 0.18671124\n",
            "Iteration 617, loss = 0.18642832\n",
            "Iteration 618, loss = 0.18610678\n",
            "Iteration 619, loss = 0.18581891\n",
            "Iteration 620, loss = 0.18551512\n",
            "Iteration 621, loss = 0.18523519\n",
            "Iteration 622, loss = 0.18490466\n",
            "Iteration 623, loss = 0.18464542\n",
            "Iteration 624, loss = 0.18431368\n",
            "Iteration 625, loss = 0.18404253\n",
            "Iteration 626, loss = 0.18374246\n",
            "Iteration 627, loss = 0.18346559\n",
            "Iteration 628, loss = 0.18321131\n",
            "Iteration 629, loss = 0.18288848\n",
            "Iteration 630, loss = 0.18262288\n",
            "Iteration 631, loss = 0.18234122\n",
            "Iteration 632, loss = 0.18204574\n",
            "Iteration 633, loss = 0.18178095\n",
            "Iteration 634, loss = 0.18147491\n",
            "Iteration 635, loss = 0.18119974\n",
            "Iteration 636, loss = 0.18094067\n",
            "Iteration 637, loss = 0.18066125\n",
            "Iteration 638, loss = 0.18041266\n",
            "Iteration 639, loss = 0.18011269\n",
            "Iteration 640, loss = 0.17983557\n",
            "Iteration 641, loss = 0.17957682\n",
            "Iteration 642, loss = 0.17933947\n",
            "Iteration 643, loss = 0.17905213\n",
            "Iteration 644, loss = 0.17875409\n",
            "Iteration 645, loss = 0.17849089\n",
            "Iteration 646, loss = 0.17826844\n",
            "Iteration 647, loss = 0.17798367\n",
            "Iteration 648, loss = 0.17773275\n",
            "Iteration 649, loss = 0.17747435\n",
            "Iteration 650, loss = 0.17720769\n",
            "Iteration 651, loss = 0.17693202\n",
            "Iteration 652, loss = 0.17670406\n",
            "Iteration 653, loss = 0.17644048\n",
            "Iteration 654, loss = 0.17617636\n",
            "Iteration 655, loss = 0.17595598\n",
            "Iteration 656, loss = 0.17567484\n",
            "Iteration 657, loss = 0.17549818\n",
            "Iteration 658, loss = 0.17519587\n",
            "Iteration 659, loss = 0.17494032\n",
            "Iteration 660, loss = 0.17468126\n",
            "Iteration 661, loss = 0.17444567\n",
            "Iteration 662, loss = 0.17419812\n",
            "Iteration 663, loss = 0.17396765\n",
            "Iteration 664, loss = 0.17370374\n",
            "Iteration 665, loss = 0.17349098\n",
            "Iteration 666, loss = 0.17323834\n",
            "Iteration 667, loss = 0.17300279\n",
            "Iteration 668, loss = 0.17278265\n",
            "Iteration 669, loss = 0.17253028\n",
            "Iteration 670, loss = 0.17229960\n",
            "Iteration 671, loss = 0.17207092\n",
            "Iteration 672, loss = 0.17186471\n",
            "Iteration 673, loss = 0.17158461\n",
            "Iteration 674, loss = 0.17135752\n",
            "Iteration 675, loss = 0.17116093\n",
            "Iteration 676, loss = 0.17091346\n",
            "Iteration 677, loss = 0.17069620\n",
            "Iteration 678, loss = 0.17044222\n",
            "Iteration 679, loss = 0.17021468\n",
            "Iteration 680, loss = 0.17001128\n",
            "Iteration 681, loss = 0.16976291\n",
            "Iteration 682, loss = 0.16955867\n",
            "Iteration 683, loss = 0.16935560\n",
            "Iteration 684, loss = 0.16910644\n",
            "Iteration 685, loss = 0.16889648\n",
            "Iteration 686, loss = 0.16868832\n",
            "Iteration 687, loss = 0.16845727\n",
            "Iteration 688, loss = 0.16825947\n",
            "Iteration 689, loss = 0.16803476\n",
            "Iteration 690, loss = 0.16781152\n",
            "Iteration 691, loss = 0.16762364\n",
            "Iteration 692, loss = 0.16740138\n",
            "Iteration 693, loss = 0.16718352\n",
            "Iteration 694, loss = 0.16697407\n",
            "Iteration 695, loss = 0.16675929\n",
            "Iteration 696, loss = 0.16660199\n",
            "Iteration 697, loss = 0.16634520\n",
            "Iteration 698, loss = 0.16616564\n",
            "Iteration 699, loss = 0.16596696\n",
            "Iteration 700, loss = 0.16574494\n",
            "Iteration 701, loss = 0.16553102\n",
            "Iteration 702, loss = 0.16541495\n",
            "Iteration 703, loss = 0.16517838\n",
            "Iteration 704, loss = 0.16491882\n",
            "Iteration 705, loss = 0.16470757\n",
            "Iteration 706, loss = 0.16457813\n",
            "Iteration 707, loss = 0.16432607\n",
            "Iteration 708, loss = 0.16415482\n",
            "Iteration 709, loss = 0.16395213\n",
            "Iteration 710, loss = 0.16374150\n",
            "Iteration 711, loss = 0.16355686\n",
            "Iteration 712, loss = 0.16341694\n",
            "Iteration 713, loss = 0.16318398\n",
            "Iteration 714, loss = 0.16298782\n",
            "Iteration 715, loss = 0.16279368\n",
            "Iteration 716, loss = 0.16261382\n",
            "Iteration 717, loss = 0.16241172\n",
            "Iteration 718, loss = 0.16224894\n",
            "Iteration 719, loss = 0.16202373\n",
            "Iteration 720, loss = 0.16183513\n",
            "Iteration 721, loss = 0.16166404\n",
            "Iteration 722, loss = 0.16148849\n",
            "Iteration 723, loss = 0.16133850\n",
            "Iteration 724, loss = 0.16109893\n",
            "Iteration 725, loss = 0.16092966\n",
            "Iteration 726, loss = 0.16076264\n",
            "Iteration 727, loss = 0.16057208\n",
            "Iteration 728, loss = 0.16042825\n",
            "Iteration 729, loss = 0.16021953\n",
            "Iteration 730, loss = 0.16002734\n",
            "Iteration 731, loss = 0.15985259\n",
            "Iteration 732, loss = 0.15970814\n",
            "Iteration 733, loss = 0.15952407\n",
            "Iteration 734, loss = 0.15932109\n",
            "Iteration 735, loss = 0.15913161\n",
            "Iteration 736, loss = 0.15899421\n",
            "Iteration 737, loss = 0.15880563\n",
            "Iteration 738, loss = 0.15865131\n",
            "Iteration 739, loss = 0.15843132\n",
            "Iteration 740, loss = 0.15827616\n",
            "Iteration 741, loss = 0.15811068\n",
            "Iteration 742, loss = 0.15794904\n",
            "Iteration 743, loss = 0.15777365\n",
            "Iteration 744, loss = 0.15760626\n",
            "Iteration 745, loss = 0.15745914\n",
            "Iteration 746, loss = 0.15727264\n",
            "Iteration 747, loss = 0.15710666\n",
            "Iteration 748, loss = 0.15695562\n",
            "Iteration 749, loss = 0.15675557\n",
            "Iteration 750, loss = 0.15659835\n",
            "Iteration 751, loss = 0.15646295\n",
            "Iteration 752, loss = 0.15630797\n",
            "Iteration 753, loss = 0.15615614\n",
            "Iteration 754, loss = 0.15598755\n",
            "Iteration 755, loss = 0.15586731\n",
            "Iteration 756, loss = 0.15565379\n",
            "Iteration 757, loss = 0.15548362\n",
            "Iteration 758, loss = 0.15536356\n",
            "Iteration 759, loss = 0.15519663\n",
            "Iteration 760, loss = 0.15502518\n",
            "Iteration 761, loss = 0.15484594\n",
            "Iteration 762, loss = 0.15470375\n",
            "Iteration 763, loss = 0.15456755\n",
            "Iteration 764, loss = 0.15442849\n",
            "Iteration 765, loss = 0.15424144\n",
            "Iteration 766, loss = 0.15408669\n",
            "Iteration 767, loss = 0.15394203\n",
            "Iteration 768, loss = 0.15380362\n",
            "Iteration 769, loss = 0.15366721\n",
            "Iteration 770, loss = 0.15347403\n",
            "Iteration 771, loss = 0.15334112\n",
            "Iteration 772, loss = 0.15315158\n",
            "Iteration 773, loss = 0.15301812\n",
            "Iteration 774, loss = 0.15288857\n",
            "Iteration 775, loss = 0.15278482\n",
            "Iteration 776, loss = 0.15265145\n",
            "Iteration 777, loss = 0.15254096\n",
            "Iteration 778, loss = 0.15232332\n",
            "Iteration 779, loss = 0.15215895\n",
            "Iteration 780, loss = 0.15207629\n",
            "Iteration 781, loss = 0.15186301\n",
            "Iteration 782, loss = 0.15177295\n",
            "Iteration 783, loss = 0.15158906\n",
            "Iteration 784, loss = 0.15147589\n",
            "Iteration 785, loss = 0.15131943\n",
            "Iteration 786, loss = 0.15116260\n",
            "Iteration 787, loss = 0.15104072\n",
            "Iteration 788, loss = 0.15091192\n",
            "Iteration 789, loss = 0.15073946\n",
            "Iteration 790, loss = 0.15064747\n",
            "Iteration 791, loss = 0.15047939\n",
            "Iteration 792, loss = 0.15037153\n",
            "Iteration 793, loss = 0.15021784\n",
            "Iteration 794, loss = 0.15008154\n",
            "Iteration 795, loss = 0.15000773\n",
            "Iteration 796, loss = 0.14980328\n",
            "Iteration 797, loss = 0.14966000\n",
            "Iteration 798, loss = 0.14952550\n",
            "Iteration 799, loss = 0.14939922\n",
            "Iteration 800, loss = 0.14929670\n",
            "Iteration 801, loss = 0.14913375\n",
            "Iteration 802, loss = 0.14900523\n",
            "Iteration 803, loss = 0.14889227\n",
            "Iteration 804, loss = 0.14876474\n",
            "Iteration 805, loss = 0.14860668\n",
            "Iteration 806, loss = 0.14850797\n",
            "Iteration 807, loss = 0.14837888\n",
            "Iteration 808, loss = 0.14824004\n",
            "Iteration 809, loss = 0.14814832\n",
            "Iteration 810, loss = 0.14796795\n",
            "Iteration 811, loss = 0.14784751\n",
            "Iteration 812, loss = 0.14772923\n",
            "Iteration 813, loss = 0.14760833\n",
            "Iteration 814, loss = 0.14752642\n",
            "Iteration 815, loss = 0.14738371\n",
            "Iteration 816, loss = 0.14723699\n",
            "Iteration 817, loss = 0.14711844\n",
            "Iteration 818, loss = 0.14699582\n",
            "Iteration 819, loss = 0.14687924\n",
            "Iteration 820, loss = 0.14676158\n",
            "Iteration 821, loss = 0.14664344\n",
            "Iteration 822, loss = 0.14649932\n",
            "Iteration 823, loss = 0.14638731\n",
            "Iteration 824, loss = 0.14624468\n",
            "Iteration 825, loss = 0.14617063\n",
            "Iteration 826, loss = 0.14607403\n",
            "Iteration 827, loss = 0.14595714\n",
            "Iteration 828, loss = 0.14577923\n",
            "Iteration 829, loss = 0.14568245\n",
            "Iteration 830, loss = 0.14557801\n",
            "Iteration 831, loss = 0.14547278\n",
            "Iteration 832, loss = 0.14532250\n",
            "Iteration 833, loss = 0.14521065\n",
            "Iteration 834, loss = 0.14509357\n",
            "Iteration 835, loss = 0.14498796\n",
            "Iteration 836, loss = 0.14487377\n",
            "Iteration 837, loss = 0.14473861\n",
            "Iteration 838, loss = 0.14464225\n",
            "Iteration 839, loss = 0.14452316\n",
            "Iteration 840, loss = 0.14442538\n",
            "Iteration 841, loss = 0.14429809\n",
            "Iteration 842, loss = 0.14428653\n",
            "Iteration 843, loss = 0.14411115\n",
            "Iteration 844, loss = 0.14397332\n",
            "Iteration 845, loss = 0.14384945\n",
            "Iteration 846, loss = 0.14377495\n",
            "Iteration 847, loss = 0.14365806\n",
            "Iteration 848, loss = 0.14359446\n",
            "Iteration 849, loss = 0.14346675\n",
            "Iteration 850, loss = 0.14334368\n",
            "Iteration 851, loss = 0.14323586\n",
            "Iteration 852, loss = 0.14310819\n",
            "Iteration 853, loss = 0.14300871\n",
            "Iteration 854, loss = 0.14290386\n",
            "Iteration 855, loss = 0.14278726\n",
            "Iteration 856, loss = 0.14267976\n",
            "Iteration 857, loss = 0.14257741\n",
            "Iteration 858, loss = 0.14246912\n",
            "Iteration 859, loss = 0.14237779\n",
            "Iteration 860, loss = 0.14227002\n",
            "Iteration 861, loss = 0.14219433\n",
            "Iteration 862, loss = 0.14206667\n",
            "Iteration 863, loss = 0.14198863\n",
            "Iteration 864, loss = 0.14185000\n",
            "Iteration 865, loss = 0.14175674\n",
            "Iteration 866, loss = 0.14165617\n",
            "Iteration 867, loss = 0.14159335\n",
            "Iteration 868, loss = 0.14149560\n",
            "Iteration 869, loss = 0.14138959\n",
            "Iteration 870, loss = 0.14125373\n",
            "Iteration 871, loss = 0.14118143\n",
            "Iteration 872, loss = 0.14110859\n",
            "Iteration 873, loss = 0.14106155\n",
            "Iteration 874, loss = 0.14089969\n",
            "Iteration 875, loss = 0.14081513\n",
            "Iteration 876, loss = 0.14075213\n",
            "Iteration 877, loss = 0.14059476\n",
            "Iteration 878, loss = 0.14053775\n",
            "Iteration 879, loss = 0.14043676\n",
            "Iteration 880, loss = 0.14030971\n",
            "Iteration 881, loss = 0.14020698\n",
            "Iteration 882, loss = 0.14010654\n",
            "Iteration 883, loss = 0.14003772\n",
            "Iteration 884, loss = 0.13990918\n",
            "Iteration 885, loss = 0.13982655\n",
            "Iteration 886, loss = 0.13979247\n",
            "Iteration 887, loss = 0.13963677\n",
            "Iteration 888, loss = 0.13956109\n",
            "Iteration 889, loss = 0.13948067\n",
            "Iteration 890, loss = 0.13937574\n",
            "Iteration 891, loss = 0.13930060\n",
            "Iteration 892, loss = 0.13920836\n",
            "Iteration 893, loss = 0.13910886\n",
            "Iteration 894, loss = 0.13901186\n",
            "Iteration 895, loss = 0.13892824\n",
            "Iteration 896, loss = 0.13882036\n",
            "Iteration 897, loss = 0.13874008\n",
            "Iteration 898, loss = 0.13865016\n",
            "Iteration 899, loss = 0.13857839\n",
            "Iteration 900, loss = 0.13849383\n",
            "Iteration 901, loss = 0.13840207\n",
            "Iteration 902, loss = 0.13830815\n",
            "Iteration 903, loss = 0.13822583\n",
            "Iteration 904, loss = 0.13811634\n",
            "Iteration 905, loss = 0.13805243\n",
            "Iteration 906, loss = 0.13794863\n",
            "Iteration 907, loss = 0.13788986\n",
            "Iteration 908, loss = 0.13779307\n",
            "Iteration 909, loss = 0.13773230\n",
            "Iteration 910, loss = 0.13761919\n",
            "Iteration 911, loss = 0.13754714\n",
            "Iteration 912, loss = 0.13749197\n",
            "Iteration 913, loss = 0.13740572\n",
            "Iteration 914, loss = 0.13732580\n",
            "Iteration 915, loss = 0.13720303\n",
            "Iteration 916, loss = 0.13711776\n",
            "Iteration 917, loss = 0.13703182\n",
            "Iteration 918, loss = 0.13694626\n",
            "Iteration 919, loss = 0.13685233\n",
            "Iteration 920, loss = 0.13684414\n",
            "Iteration 921, loss = 0.13675264\n",
            "Iteration 922, loss = 0.13663549\n",
            "Iteration 923, loss = 0.13655302\n",
            "Iteration 924, loss = 0.13647108\n",
            "Iteration 925, loss = 0.13638906\n",
            "Iteration 926, loss = 0.13631717\n",
            "Iteration 927, loss = 0.13622715\n",
            "Iteration 928, loss = 0.13615743\n",
            "Iteration 929, loss = 0.13608266\n",
            "Iteration 930, loss = 0.13601237\n",
            "Iteration 931, loss = 0.13590223\n",
            "Iteration 932, loss = 0.13585454\n",
            "Iteration 933, loss = 0.13576001\n",
            "Iteration 934, loss = 0.13570755\n",
            "Iteration 935, loss = 0.13563479\n",
            "Iteration 936, loss = 0.13553988\n",
            "Iteration 937, loss = 0.13546230\n",
            "Iteration 938, loss = 0.13539342\n",
            "Iteration 939, loss = 0.13539242\n",
            "Iteration 940, loss = 0.13527501\n",
            "Iteration 941, loss = 0.13524881\n",
            "Iteration 942, loss = 0.13511067\n",
            "Iteration 943, loss = 0.13499716\n",
            "Iteration 944, loss = 0.13500466\n",
            "Iteration 945, loss = 0.13491851\n",
            "Iteration 946, loss = 0.13483524\n",
            "Iteration 947, loss = 0.13470636\n",
            "Iteration 948, loss = 0.13473317\n",
            "Iteration 949, loss = 0.13457359\n",
            "Iteration 950, loss = 0.13448070\n",
            "Iteration 951, loss = 0.13442990\n",
            "Iteration 952, loss = 0.13433376\n",
            "Iteration 953, loss = 0.13425529\n",
            "Iteration 954, loss = 0.13418199\n",
            "Iteration 955, loss = 0.13413004\n",
            "Iteration 956, loss = 0.13409746\n",
            "Iteration 957, loss = 0.13399013\n",
            "Iteration 958, loss = 0.13388956\n",
            "Iteration 959, loss = 0.13384489\n",
            "Iteration 960, loss = 0.13373198\n",
            "Iteration 961, loss = 0.13367909\n",
            "Iteration 962, loss = 0.13360276\n",
            "Iteration 963, loss = 0.13352968\n",
            "Iteration 964, loss = 0.13345674\n",
            "Iteration 965, loss = 0.13344025\n",
            "Iteration 966, loss = 0.13340867\n",
            "Iteration 967, loss = 0.13327721\n",
            "Iteration 968, loss = 0.13319730\n",
            "Iteration 969, loss = 0.13315410\n",
            "Iteration 970, loss = 0.13302927\n",
            "Iteration 971, loss = 0.13306437\n",
            "Iteration 972, loss = 0.13293122\n",
            "Iteration 973, loss = 0.13283398\n",
            "Iteration 974, loss = 0.13280762\n",
            "Iteration 975, loss = 0.13274556\n",
            "Iteration 976, loss = 0.13267261\n",
            "Iteration 977, loss = 0.13257030\n",
            "Iteration 978, loss = 0.13250234\n",
            "Iteration 979, loss = 0.13243628\n",
            "Iteration 980, loss = 0.13236797\n",
            "Iteration 981, loss = 0.13233068\n",
            "Iteration 982, loss = 0.13221932\n",
            "Iteration 983, loss = 0.13220040\n",
            "Iteration 984, loss = 0.13213322\n",
            "Iteration 985, loss = 0.13207079\n",
            "Iteration 986, loss = 0.13200184\n",
            "Iteration 987, loss = 0.13194896\n",
            "Iteration 988, loss = 0.13184553\n",
            "Iteration 989, loss = 0.13178135\n",
            "Iteration 990, loss = 0.13171764\n",
            "Iteration 991, loss = 0.13168878\n",
            "Iteration 992, loss = 0.13161941\n",
            "Iteration 993, loss = 0.13153134\n",
            "Iteration 994, loss = 0.13147467\n",
            "Iteration 995, loss = 0.13140728\n",
            "Iteration 996, loss = 0.13133043\n",
            "Iteration 997, loss = 0.13130552\n",
            "Iteration 998, loss = 0.13123173\n",
            "Iteration 999, loss = 0.13114277\n",
            "Iteration 1000, loss = 0.13110380\n",
            "Iteration 1001, loss = 0.13102341\n",
            "Iteration 1002, loss = 0.13099683\n",
            "Iteration 1003, loss = 0.13091047\n",
            "Iteration 1004, loss = 0.13091982\n",
            "Iteration 1005, loss = 0.13083058\n",
            "Iteration 1006, loss = 0.13074897\n",
            "Iteration 1007, loss = 0.13068979\n",
            "Iteration 1008, loss = 0.13069596\n",
            "Iteration 1009, loss = 0.13060267\n",
            "Iteration 1010, loss = 0.13051521\n",
            "Iteration 1011, loss = 0.13042323\n",
            "Iteration 1012, loss = 0.13036155\n",
            "Iteration 1013, loss = 0.13040900\n",
            "Iteration 1014, loss = 0.13025691\n",
            "Iteration 1015, loss = 0.13021172\n",
            "Iteration 1016, loss = 0.13019085\n",
            "Iteration 1017, loss = 0.13010669\n",
            "Iteration 1018, loss = 0.13001134\n",
            "Iteration 1019, loss = 0.12996656\n",
            "Iteration 1020, loss = 0.12993421\n",
            "Iteration 1021, loss = 0.12985016\n",
            "Iteration 1022, loss = 0.12980604\n",
            "Iteration 1023, loss = 0.12974285\n",
            "Iteration 1024, loss = 0.12979586\n",
            "Iteration 1025, loss = 0.12966356\n",
            "Iteration 1026, loss = 0.12957511\n",
            "Iteration 1027, loss = 0.12955934\n",
            "Iteration 1028, loss = 0.12947235\n",
            "Iteration 1029, loss = 0.12946179\n",
            "Iteration 1030, loss = 0.12938039\n",
            "Iteration 1031, loss = 0.12948467\n",
            "Iteration 1032, loss = 0.12933134\n",
            "Iteration 1033, loss = 0.12920432\n",
            "Iteration 1034, loss = 0.12915151\n",
            "Iteration 1035, loss = 0.12914958\n",
            "Iteration 1036, loss = 0.12905052\n",
            "Iteration 1037, loss = 0.12897423\n",
            "Iteration 1038, loss = 0.12893105\n",
            "Iteration 1039, loss = 0.12888257\n",
            "Iteration 1040, loss = 0.12888521\n",
            "Iteration 1041, loss = 0.12878139\n",
            "Iteration 1042, loss = 0.12880792\n",
            "Iteration 1043, loss = 0.12870487\n",
            "Iteration 1044, loss = 0.12866580\n",
            "Iteration 1045, loss = 0.12859316\n",
            "Iteration 1046, loss = 0.12854198\n",
            "Iteration 1047, loss = 0.12848645\n",
            "Iteration 1048, loss = 0.12843954\n",
            "Iteration 1049, loss = 0.12839652\n",
            "Iteration 1050, loss = 0.12833821\n",
            "Iteration 1051, loss = 0.12828242\n",
            "Iteration 1052, loss = 0.12823807\n",
            "Iteration 1053, loss = 0.12816611\n",
            "Iteration 1054, loss = 0.12814861\n",
            "Iteration 1055, loss = 0.12806417\n",
            "Iteration 1056, loss = 0.12803740\n",
            "Iteration 1057, loss = 0.12799274\n",
            "Iteration 1058, loss = 0.12794198\n",
            "Iteration 1059, loss = 0.12789010\n",
            "Iteration 1060, loss = 0.12783538\n",
            "Iteration 1061, loss = 0.12778191\n",
            "Iteration 1062, loss = 0.12773792\n",
            "Iteration 1063, loss = 0.12770772\n",
            "Iteration 1064, loss = 0.12765256\n",
            "Iteration 1065, loss = 0.12759761\n",
            "Iteration 1066, loss = 0.12756230\n",
            "Iteration 1067, loss = 0.12750063\n",
            "Iteration 1068, loss = 0.12747627\n",
            "Iteration 1069, loss = 0.12743312\n",
            "Iteration 1070, loss = 0.12741890\n",
            "Iteration 1071, loss = 0.12734171\n",
            "Iteration 1072, loss = 0.12731701\n",
            "Iteration 1073, loss = 0.12721990\n",
            "Iteration 1074, loss = 0.12720395\n",
            "Iteration 1075, loss = 0.12715192\n",
            "Iteration 1076, loss = 0.12709092\n",
            "Iteration 1077, loss = 0.12704818\n",
            "Iteration 1078, loss = 0.12698360\n",
            "Iteration 1079, loss = 0.12696555\n",
            "Iteration 1080, loss = 0.12698623\n",
            "Iteration 1081, loss = 0.12686360\n",
            "Iteration 1082, loss = 0.12682624\n",
            "Iteration 1083, loss = 0.12680708\n",
            "Iteration 1084, loss = 0.12672631\n",
            "Iteration 1085, loss = 0.12670864\n",
            "Iteration 1086, loss = 0.12666042\n",
            "Iteration 1087, loss = 0.12658580\n",
            "Iteration 1088, loss = 0.12660285\n",
            "Iteration 1089, loss = 0.12654228\n",
            "Iteration 1090, loss = 0.12653076\n",
            "Iteration 1091, loss = 0.12643729\n",
            "Iteration 1092, loss = 0.12638787\n",
            "Iteration 1093, loss = 0.12635628\n",
            "Iteration 1094, loss = 0.12628988\n",
            "Iteration 1095, loss = 0.12625549\n",
            "Iteration 1096, loss = 0.12621226\n",
            "Iteration 1097, loss = 0.12618702\n",
            "Iteration 1098, loss = 0.12616100\n",
            "Iteration 1099, loss = 0.12608430\n",
            "Iteration 1100, loss = 0.12604859\n",
            "Iteration 1101, loss = 0.12603128\n",
            "Iteration 1102, loss = 0.12596562\n",
            "Iteration 1103, loss = 0.12591174\n",
            "Iteration 1104, loss = 0.12589605\n",
            "Iteration 1105, loss = 0.12586730\n",
            "Iteration 1106, loss = 0.12582505\n",
            "Iteration 1107, loss = 0.12578790\n",
            "Iteration 1108, loss = 0.12574576\n",
            "Iteration 1109, loss = 0.12568124\n",
            "Iteration 1110, loss = 0.12565514\n",
            "Iteration 1111, loss = 0.12561557\n",
            "Iteration 1112, loss = 0.12558618\n",
            "Iteration 1113, loss = 0.12552601\n",
            "Iteration 1114, loss = 0.12561290\n",
            "Iteration 1115, loss = 0.12546839\n",
            "Iteration 1116, loss = 0.12550653\n",
            "Iteration 1117, loss = 0.12540334\n",
            "Iteration 1118, loss = 0.12532619\n",
            "Iteration 1119, loss = 0.12528272\n",
            "Iteration 1120, loss = 0.12529698\n",
            "Iteration 1121, loss = 0.12523397\n",
            "Iteration 1122, loss = 0.12517576\n",
            "Iteration 1123, loss = 0.12514247\n",
            "Iteration 1124, loss = 0.12509914\n",
            "Iteration 1125, loss = 0.12507333\n",
            "Iteration 1126, loss = 0.12503894\n",
            "Iteration 1127, loss = 0.12499587\n",
            "Iteration 1128, loss = 0.12492086\n",
            "Iteration 1129, loss = 0.12488842\n",
            "Iteration 1130, loss = 0.12489136\n",
            "Iteration 1131, loss = 0.12499647\n",
            "Iteration 1132, loss = 0.12487285\n",
            "Iteration 1133, loss = 0.12481484\n",
            "Iteration 1134, loss = 0.12474975\n",
            "Iteration 1135, loss = 0.12475287\n",
            "Iteration 1136, loss = 0.12471181\n",
            "Iteration 1137, loss = 0.12463624\n",
            "Iteration 1138, loss = 0.12458142\n",
            "Iteration 1139, loss = 0.12461760\n",
            "Iteration 1140, loss = 0.12458514\n",
            "Iteration 1141, loss = 0.12458369\n",
            "Iteration 1142, loss = 0.12452343\n",
            "Iteration 1143, loss = 0.12440525\n",
            "Iteration 1144, loss = 0.12437204\n",
            "Iteration 1145, loss = 0.12433413\n",
            "Iteration 1146, loss = 0.12432032\n",
            "Iteration 1147, loss = 0.12426733\n",
            "Iteration 1148, loss = 0.12427863\n",
            "Iteration 1149, loss = 0.12420461\n",
            "Iteration 1150, loss = 0.12419542\n",
            "Iteration 1151, loss = 0.12413575\n",
            "Iteration 1152, loss = 0.12411572\n",
            "Iteration 1153, loss = 0.12406406\n",
            "Iteration 1154, loss = 0.12402263\n",
            "Iteration 1155, loss = 0.12402752\n",
            "Iteration 1156, loss = 0.12398618\n",
            "Iteration 1157, loss = 0.12396397\n",
            "Iteration 1158, loss = 0.12390950\n",
            "Iteration 1159, loss = 0.12386647\n",
            "Iteration 1160, loss = 0.12384954\n",
            "Iteration 1161, loss = 0.12380507\n",
            "Iteration 1162, loss = 0.12379863\n",
            "Iteration 1163, loss = 0.12382695\n",
            "Iteration 1164, loss = 0.12370759\n",
            "Iteration 1165, loss = 0.12371698\n",
            "Iteration 1166, loss = 0.12366456\n",
            "Iteration 1167, loss = 0.12361932\n",
            "Iteration 1168, loss = 0.12359954\n",
            "Iteration 1169, loss = 0.12354898\n",
            "Iteration 1170, loss = 0.12353596\n",
            "Iteration 1171, loss = 0.12348698\n",
            "Iteration 1172, loss = 0.12342994\n",
            "Iteration 1173, loss = 0.12340303\n",
            "Iteration 1174, loss = 0.12339924\n",
            "Iteration 1175, loss = 0.12337100\n",
            "Iteration 1176, loss = 0.12335002\n",
            "Iteration 1177, loss = 0.12331381\n",
            "Iteration 1178, loss = 0.12334936\n",
            "Iteration 1179, loss = 0.12322897\n",
            "Iteration 1180, loss = 0.12317889\n",
            "Iteration 1181, loss = 0.12317717\n",
            "Iteration 1182, loss = 0.12314410\n",
            "Iteration 1183, loss = 0.12311123\n",
            "Iteration 1184, loss = 0.12310427\n",
            "Iteration 1185, loss = 0.12306248\n",
            "Iteration 1186, loss = 0.12300076\n",
            "Iteration 1187, loss = 0.12298977\n",
            "Iteration 1188, loss = 0.12297069\n",
            "Iteration 1189, loss = 0.12294604\n",
            "Iteration 1190, loss = 0.12289908\n",
            "Iteration 1191, loss = 0.12293598\n",
            "Iteration 1192, loss = 0.12285520\n",
            "Iteration 1193, loss = 0.12283091\n",
            "Iteration 1194, loss = 0.12283816\n",
            "Iteration 1195, loss = 0.12278170\n",
            "Iteration 1196, loss = 0.12275645\n",
            "Iteration 1197, loss = 0.12272527\n",
            "Iteration 1198, loss = 0.12269733\n",
            "Iteration 1199, loss = 0.12266072\n",
            "Iteration 1200, loss = 0.12265095\n",
            "Iteration 1201, loss = 0.12260478\n",
            "Iteration 1202, loss = 0.12257051\n",
            "Iteration 1203, loss = 0.12259506\n",
            "Iteration 1204, loss = 0.12250928\n",
            "Iteration 1205, loss = 0.12249566\n",
            "Iteration 1206, loss = 0.12244095\n",
            "Iteration 1207, loss = 0.12246944\n",
            "Iteration 1208, loss = 0.12242066\n",
            "Iteration 1209, loss = 0.12237156\n",
            "Iteration 1210, loss = 0.12234819\n",
            "Iteration 1211, loss = 0.12232800\n",
            "Iteration 1212, loss = 0.12231194\n",
            "Iteration 1213, loss = 0.12223978\n",
            "Iteration 1214, loss = 0.12223964\n",
            "Iteration 1215, loss = 0.12220770\n",
            "Iteration 1216, loss = 0.12225913\n",
            "Iteration 1217, loss = 0.12218294\n",
            "Iteration 1218, loss = 0.12215318\n",
            "Iteration 1219, loss = 0.12207641\n",
            "Iteration 1220, loss = 0.12207511\n",
            "Iteration 1221, loss = 0.12208015\n",
            "Iteration 1222, loss = 0.12214764\n",
            "Iteration 1223, loss = 0.12214659\n",
            "Iteration 1224, loss = 0.12198451\n",
            "Iteration 1225, loss = 0.12198963\n",
            "Iteration 1226, loss = 0.12203729\n",
            "Iteration 1227, loss = 0.12189773\n",
            "Iteration 1228, loss = 0.12186699\n",
            "Iteration 1229, loss = 0.12184890\n",
            "Iteration 1230, loss = 0.12183406\n",
            "Iteration 1231, loss = 0.12182044\n",
            "Iteration 1232, loss = 0.12180521\n",
            "Iteration 1233, loss = 0.12179858\n",
            "Iteration 1234, loss = 0.12174584\n",
            "Iteration 1235, loss = 0.12174891\n",
            "Iteration 1236, loss = 0.12167767\n",
            "Iteration 1237, loss = 0.12164264\n",
            "Iteration 1238, loss = 0.12163003\n",
            "Iteration 1239, loss = 0.12162864\n",
            "Iteration 1240, loss = 0.12157034\n",
            "Iteration 1241, loss = 0.12157916\n",
            "Iteration 1242, loss = 0.12150344\n",
            "Iteration 1243, loss = 0.12148637\n",
            "Iteration 1244, loss = 0.12152016\n",
            "Iteration 1245, loss = 0.12143587\n",
            "Iteration 1246, loss = 0.12151726\n",
            "Iteration 1247, loss = 0.12143421\n",
            "Iteration 1248, loss = 0.12141174\n",
            "Iteration 1249, loss = 0.12136213\n",
            "Iteration 1250, loss = 0.12134194\n",
            "Iteration 1251, loss = 0.12128119\n",
            "Iteration 1252, loss = 0.12120948\n",
            "Iteration 1253, loss = 0.12117302\n",
            "Iteration 1254, loss = 0.12113589\n",
            "Iteration 1255, loss = 0.12114571\n",
            "Iteration 1256, loss = 0.12110838\n",
            "Iteration 1257, loss = 0.12107193\n",
            "Iteration 1258, loss = 0.12101222\n",
            "Iteration 1259, loss = 0.12102375\n",
            "Iteration 1260, loss = 0.12096838\n",
            "Iteration 1261, loss = 0.12097134\n",
            "Iteration 1262, loss = 0.12089733\n",
            "Iteration 1263, loss = 0.12085265\n",
            "Iteration 1264, loss = 0.12088789\n",
            "Iteration 1265, loss = 0.12078890\n",
            "Iteration 1266, loss = 0.12076201\n",
            "Iteration 1267, loss = 0.12074449\n",
            "Iteration 1268, loss = 0.12073058\n",
            "Iteration 1269, loss = 0.12077205\n",
            "Iteration 1270, loss = 0.12066023\n",
            "Iteration 1271, loss = 0.12064756\n",
            "Iteration 1272, loss = 0.12061442\n",
            "Iteration 1273, loss = 0.12060395\n",
            "Iteration 1274, loss = 0.12055672\n",
            "Iteration 1275, loss = 0.12061263\n",
            "Iteration 1276, loss = 0.12062205\n",
            "Iteration 1277, loss = 0.12049710\n",
            "Iteration 1278, loss = 0.12046305\n",
            "Iteration 1279, loss = 0.12042331\n",
            "Iteration 1280, loss = 0.12043468\n",
            "Iteration 1281, loss = 0.12038567\n",
            "Iteration 1282, loss = 0.12034741\n",
            "Iteration 1283, loss = 0.12032229\n",
            "Iteration 1284, loss = 0.12032007\n",
            "Iteration 1285, loss = 0.12033655\n",
            "Iteration 1286, loss = 0.12025434\n",
            "Iteration 1287, loss = 0.12029218\n",
            "Iteration 1288, loss = 0.12023175\n",
            "Iteration 1289, loss = 0.12023699\n",
            "Iteration 1290, loss = 0.12017826\n",
            "Iteration 1291, loss = 0.12020094\n",
            "Iteration 1292, loss = 0.12013254\n",
            "Iteration 1293, loss = 0.12011165\n",
            "Iteration 1294, loss = 0.12013870\n",
            "Iteration 1295, loss = 0.12002930\n",
            "Iteration 1296, loss = 0.12007312\n",
            "Iteration 1297, loss = 0.12000544\n",
            "Iteration 1298, loss = 0.12000408\n",
            "Iteration 1299, loss = 0.11994005\n",
            "Iteration 1300, loss = 0.11994334\n",
            "Iteration 1301, loss = 0.11990683\n",
            "Iteration 1302, loss = 0.11988967\n",
            "Iteration 1303, loss = 0.11990291\n",
            "Iteration 1304, loss = 0.11987653\n",
            "Iteration 1305, loss = 0.11983556\n",
            "Iteration 1306, loss = 0.11979989\n",
            "Iteration 1307, loss = 0.11984025\n",
            "Iteration 1308, loss = 0.11989357\n",
            "Iteration 1309, loss = 0.11977163\n",
            "Iteration 1310, loss = 0.11969348\n",
            "Iteration 1311, loss = 0.11977805\n",
            "Iteration 1312, loss = 0.11976089\n",
            "Iteration 1313, loss = 0.11963948\n",
            "Iteration 1314, loss = 0.11962665\n",
            "Iteration 1315, loss = 0.11960136\n",
            "Iteration 1316, loss = 0.11959463\n",
            "Iteration 1317, loss = 0.11956204\n",
            "Iteration 1318, loss = 0.11955188\n",
            "Iteration 1319, loss = 0.11953255\n",
            "Iteration 1320, loss = 0.11948127\n",
            "Iteration 1321, loss = 0.11946756\n",
            "Iteration 1322, loss = 0.11945863\n",
            "Iteration 1323, loss = 0.11945066\n",
            "Iteration 1324, loss = 0.11944571\n",
            "Iteration 1325, loss = 0.11939157\n",
            "Iteration 1326, loss = 0.11937560\n",
            "Iteration 1327, loss = 0.11938613\n",
            "Iteration 1328, loss = 0.11935650\n",
            "Iteration 1329, loss = 0.11931975\n",
            "Iteration 1330, loss = 0.11927928\n",
            "Iteration 1331, loss = 0.11928905\n",
            "Iteration 1332, loss = 0.11924440\n",
            "Iteration 1333, loss = 0.11926866\n",
            "Iteration 1334, loss = 0.11922011\n",
            "Iteration 1335, loss = 0.11918666\n",
            "Iteration 1336, loss = 0.11920873\n",
            "Iteration 1337, loss = 0.11918987\n",
            "Iteration 1338, loss = 0.11911297\n",
            "Iteration 1339, loss = 0.11910952\n",
            "Iteration 1340, loss = 0.11911164\n",
            "Iteration 1341, loss = 0.11909181\n",
            "Iteration 1342, loss = 0.11905177\n",
            "Iteration 1343, loss = 0.11905985\n",
            "Iteration 1344, loss = 0.11899368\n",
            "Iteration 1345, loss = 0.11899426\n",
            "Iteration 1346, loss = 0.11899081\n",
            "Iteration 1347, loss = 0.11899744\n",
            "Iteration 1348, loss = 0.11893405\n",
            "Iteration 1349, loss = 0.11892496\n",
            "Iteration 1350, loss = 0.11889690\n",
            "Iteration 1351, loss = 0.11890347\n",
            "Iteration 1352, loss = 0.11888182\n",
            "Iteration 1353, loss = 0.11885245\n",
            "Iteration 1354, loss = 0.11889142\n",
            "Iteration 1355, loss = 0.11883240\n",
            "Iteration 1356, loss = 0.11882789\n",
            "Iteration 1357, loss = 0.11876885\n",
            "Iteration 1358, loss = 0.11873433\n",
            "Iteration 1359, loss = 0.11873381\n",
            "Iteration 1360, loss = 0.11872909\n",
            "Iteration 1361, loss = 0.11876082\n",
            "Iteration 1362, loss = 0.11868192\n",
            "Iteration 1363, loss = 0.11867147\n",
            "Iteration 1364, loss = 0.11869500\n",
            "Iteration 1365, loss = 0.11862602\n",
            "Iteration 1366, loss = 0.11861527\n",
            "Iteration 1367, loss = 0.11866940\n",
            "Iteration 1368, loss = 0.11857823\n",
            "Iteration 1369, loss = 0.11858022\n",
            "Iteration 1370, loss = 0.11853951\n",
            "Iteration 1371, loss = 0.11852297\n",
            "Iteration 1372, loss = 0.11851596\n",
            "Iteration 1373, loss = 0.11854645\n",
            "Iteration 1374, loss = 0.11846632\n",
            "Iteration 1375, loss = 0.11850623\n",
            "Iteration 1376, loss = 0.11844293\n",
            "Iteration 1377, loss = 0.11844060\n",
            "Iteration 1378, loss = 0.11841370\n",
            "Iteration 1379, loss = 0.11839114\n",
            "Iteration 1380, loss = 0.11838249\n",
            "Iteration 1381, loss = 0.11838281\n",
            "Iteration 1382, loss = 0.11839814\n",
            "Iteration 1383, loss = 0.11837976\n",
            "Iteration 1384, loss = 0.11834544\n",
            "Iteration 1385, loss = 0.11832900\n",
            "Iteration 1386, loss = 0.11828513\n",
            "Iteration 1387, loss = 0.11825390\n",
            "Iteration 1388, loss = 0.11829118\n",
            "Iteration 1389, loss = 0.11822161\n",
            "Iteration 1390, loss = 0.11821035\n",
            "Iteration 1391, loss = 0.11823030\n",
            "Iteration 1392, loss = 0.11820183\n",
            "Iteration 1393, loss = 0.11814247\n",
            "Iteration 1394, loss = 0.11819888\n",
            "Iteration 1395, loss = 0.11821087\n",
            "Iteration 1396, loss = 0.11815080\n",
            "Iteration 1397, loss = 0.11810595\n",
            "Iteration 1398, loss = 0.11820042\n",
            "Iteration 1399, loss = 0.11807571\n",
            "Iteration 1400, loss = 0.11814484\n",
            "Iteration 1401, loss = 0.11806314\n",
            "Iteration 1402, loss = 0.11804089\n",
            "Iteration 1403, loss = 0.11804036\n",
            "Iteration 1404, loss = 0.11799004\n",
            "Iteration 1405, loss = 0.11807888\n",
            "Iteration 1406, loss = 0.11797409\n",
            "Iteration 1407, loss = 0.11799343\n",
            "Iteration 1408, loss = 0.11795272\n",
            "Iteration 1409, loss = 0.11793529\n",
            "Iteration 1410, loss = 0.11795642\n",
            "Iteration 1411, loss = 0.11790947\n",
            "Iteration 1412, loss = 0.11786671\n",
            "Iteration 1413, loss = 0.11789940\n",
            "Iteration 1414, loss = 0.11786275\n",
            "Iteration 1415, loss = 0.11786687\n",
            "Iteration 1416, loss = 0.11781929\n",
            "Iteration 1417, loss = 0.11780666\n",
            "Iteration 1418, loss = 0.11779791\n",
            "Iteration 1419, loss = 0.11776831\n",
            "Iteration 1420, loss = 0.11780978\n",
            "Iteration 1421, loss = 0.11776159\n",
            "Iteration 1422, loss = 0.11778772\n",
            "Iteration 1423, loss = 0.11775445\n",
            "Iteration 1424, loss = 0.11778653\n",
            "Iteration 1425, loss = 0.11786126\n",
            "Iteration 1426, loss = 0.11769996\n",
            "Iteration 1427, loss = 0.11765244\n",
            "Iteration 1428, loss = 0.11765945\n",
            "Iteration 1429, loss = 0.11769033\n",
            "Iteration 1430, loss = 0.11763733\n",
            "Iteration 1431, loss = 0.11763961\n",
            "Iteration 1432, loss = 0.11761383\n",
            "Iteration 1433, loss = 0.11758980\n",
            "Iteration 1434, loss = 0.11757573\n",
            "Iteration 1435, loss = 0.11762839\n",
            "Iteration 1436, loss = 0.11759803\n",
            "Iteration 1437, loss = 0.11754002\n",
            "Iteration 1438, loss = 0.11751105\n",
            "Iteration 1439, loss = 0.11757010\n",
            "Iteration 1440, loss = 0.11757222\n",
            "Iteration 1441, loss = 0.11748111\n",
            "Iteration 1442, loss = 0.11746437\n",
            "Iteration 1443, loss = 0.11745684\n",
            "Iteration 1444, loss = 0.11747196\n",
            "Iteration 1445, loss = 0.11744160\n",
            "Iteration 1446, loss = 0.11743520\n",
            "Iteration 1447, loss = 0.11739830\n",
            "Iteration 1448, loss = 0.11745096\n",
            "Iteration 1449, loss = 0.11737223\n",
            "Iteration 1450, loss = 0.11739354\n",
            "Iteration 1451, loss = 0.11735619\n",
            "Iteration 1452, loss = 0.11734953\n",
            "Iteration 1453, loss = 0.11732723\n",
            "Iteration 1454, loss = 0.11733005\n",
            "Iteration 1455, loss = 0.11740385\n",
            "Iteration 1456, loss = 0.11730076\n",
            "Iteration 1457, loss = 0.11731207\n",
            "Iteration 1458, loss = 0.11741083\n",
            "Iteration 1459, loss = 0.11738614\n",
            "Iteration 1460, loss = 0.11732227\n",
            "Iteration 1461, loss = 0.11721965\n",
            "Iteration 1462, loss = 0.11723682\n",
            "Iteration 1463, loss = 0.11721684\n",
            "Iteration 1464, loss = 0.11724025\n",
            "Iteration 1465, loss = 0.11720063\n",
            "Iteration 1466, loss = 0.11718861\n",
            "Iteration 1467, loss = 0.11716984\n",
            "Iteration 1468, loss = 0.11715428\n",
            "Iteration 1469, loss = 0.11713440\n",
            "Iteration 1470, loss = 0.11711430\n",
            "Iteration 1471, loss = 0.11715606\n",
            "Iteration 1472, loss = 0.11711156\n",
            "Iteration 1473, loss = 0.11709874\n",
            "Iteration 1474, loss = 0.11708157\n",
            "Iteration 1475, loss = 0.11709510\n",
            "Iteration 1476, loss = 0.11706716\n",
            "Iteration 1477, loss = 0.11712934\n",
            "Iteration 1478, loss = 0.11706419\n",
            "Iteration 1479, loss = 0.11704694\n",
            "Iteration 1480, loss = 0.11699685\n",
            "Iteration 1481, loss = 0.11701428\n",
            "Iteration 1482, loss = 0.11703619\n",
            "Iteration 1483, loss = 0.11698587\n",
            "Iteration 1484, loss = 0.11699337\n",
            "Iteration 1485, loss = 0.11693528\n",
            "Iteration 1486, loss = 0.11705262\n",
            "Iteration 1487, loss = 0.11693308\n",
            "Iteration 1488, loss = 0.11698244\n",
            "Iteration 1489, loss = 0.11692743\n",
            "Iteration 1490, loss = 0.11692077\n",
            "Iteration 1491, loss = 0.11693153\n",
            "Iteration 1492, loss = 0.11686383\n",
            "Iteration 1493, loss = 0.11691750\n",
            "Iteration 1494, loss = 0.11701800\n",
            "Iteration 1495, loss = 0.11683059\n",
            "Iteration 1496, loss = 0.11684085\n",
            "Iteration 1497, loss = 0.11692035\n",
            "Iteration 1498, loss = 0.11682402\n",
            "Iteration 1499, loss = 0.11680313\n",
            "Iteration 1500, loss = 0.11698263\n",
            "Iteration 1501, loss = 0.11682073\n",
            "Iteration 1502, loss = 0.11678980\n",
            "Iteration 1503, loss = 0.11680345\n",
            "Iteration 1504, loss = 0.11672863\n",
            "Iteration 1505, loss = 0.11684967\n",
            "Iteration 1506, loss = 0.11677057\n",
            "Iteration 1507, loss = 0.11674579\n",
            "Iteration 1508, loss = 0.11672769\n",
            "Iteration 1509, loss = 0.11684490\n",
            "Iteration 1510, loss = 0.11669076\n",
            "Iteration 1511, loss = 0.11668169\n",
            "Iteration 1512, loss = 0.11669263\n",
            "Iteration 1513, loss = 0.11667999\n",
            "Iteration 1514, loss = 0.11678902\n",
            "Iteration 1515, loss = 0.11666960\n",
            "Iteration 1516, loss = 0.11663930\n",
            "Iteration 1517, loss = 0.11662228\n",
            "Iteration 1518, loss = 0.11662289\n",
            "Iteration 1519, loss = 0.11660207\n",
            "Iteration 1520, loss = 0.11658190\n",
            "Iteration 1521, loss = 0.11660128\n",
            "Iteration 1522, loss = 0.11656970\n",
            "Iteration 1523, loss = 0.11656656\n",
            "Iteration 1524, loss = 0.11661137\n",
            "Iteration 1525, loss = 0.11659949\n",
            "Iteration 1526, loss = 0.11656074\n",
            "Iteration 1527, loss = 0.11658736\n",
            "Iteration 1528, loss = 0.11654946\n",
            "Iteration 1529, loss = 0.11652564\n",
            "Iteration 1530, loss = 0.11654632\n",
            "Iteration 1531, loss = 0.11650012\n",
            "Iteration 1532, loss = 0.11649062\n",
            "Iteration 1533, loss = 0.11647974\n",
            "Iteration 1534, loss = 0.11647849\n",
            "Iteration 1535, loss = 0.11643194\n",
            "Iteration 1536, loss = 0.11643611\n",
            "Iteration 1537, loss = 0.11643552\n",
            "Iteration 1538, loss = 0.11640195\n",
            "Iteration 1539, loss = 0.11659925\n",
            "Iteration 1540, loss = 0.11639815\n",
            "Iteration 1541, loss = 0.11648299\n",
            "Iteration 1542, loss = 0.11638660\n",
            "Iteration 1543, loss = 0.11642046\n",
            "Iteration 1544, loss = 0.11645404\n",
            "Iteration 1545, loss = 0.11644322\n",
            "Iteration 1546, loss = 0.11640097\n",
            "Iteration 1547, loss = 0.11635811\n",
            "Iteration 1548, loss = 0.11631591\n",
            "Iteration 1549, loss = 0.11633299\n",
            "Iteration 1550, loss = 0.11629854\n",
            "Iteration 1551, loss = 0.11632625\n",
            "Iteration 1552, loss = 0.11633507\n",
            "Iteration 1553, loss = 0.11628229\n",
            "Iteration 1554, loss = 0.11626918\n",
            "Iteration 1555, loss = 0.11631055\n",
            "Iteration 1556, loss = 0.11627033\n",
            "Iteration 1557, loss = 0.11624391\n",
            "Iteration 1558, loss = 0.11623599\n",
            "Iteration 1559, loss = 0.11624304\n",
            "Iteration 1560, loss = 0.11619288\n",
            "Iteration 1561, loss = 0.11622533\n",
            "Iteration 1562, loss = 0.11619337\n",
            "Iteration 1563, loss = 0.11620818\n",
            "Iteration 1564, loss = 0.11620067\n",
            "Iteration 1565, loss = 0.11615811\n",
            "Iteration 1566, loss = 0.11630486\n",
            "Iteration 1567, loss = 0.11618235\n",
            "Iteration 1568, loss = 0.11614154\n",
            "Iteration 1569, loss = 0.11627929\n",
            "Iteration 1570, loss = 0.11614905\n",
            "Iteration 1571, loss = 0.11619825\n",
            "Iteration 1572, loss = 0.11613727\n",
            "Iteration 1573, loss = 0.11615060\n",
            "Iteration 1574, loss = 0.11612678\n",
            "Iteration 1575, loss = 0.11616602\n",
            "Iteration 1576, loss = 0.11611476\n",
            "Iteration 1577, loss = 0.11614415\n",
            "Iteration 1578, loss = 0.11608000\n",
            "Iteration 1579, loss = 0.11608469\n",
            "Iteration 1580, loss = 0.11606588\n",
            "Iteration 1581, loss = 0.11603767\n",
            "Iteration 1582, loss = 0.11603181\n",
            "Iteration 1583, loss = 0.11604398\n",
            "Iteration 1584, loss = 0.11603850\n",
            "Iteration 1585, loss = 0.11611766\n",
            "Iteration 1586, loss = 0.11604108\n",
            "Iteration 1587, loss = 0.11599340\n",
            "Iteration 1588, loss = 0.11601594\n",
            "Iteration 1589, loss = 0.11600201\n",
            "Iteration 1590, loss = 0.11600434\n",
            "Iteration 1591, loss = 0.11603923\n",
            "Iteration 1592, loss = 0.11595619\n",
            "Iteration 1593, loss = 0.11595647\n",
            "Iteration 1594, loss = 0.11595410\n",
            "Iteration 1595, loss = 0.11591621\n",
            "Iteration 1596, loss = 0.11593503\n",
            "Iteration 1597, loss = 0.11595752\n",
            "Iteration 1598, loss = 0.11591819\n",
            "Iteration 1599, loss = 0.11590650\n",
            "Iteration 1600, loss = 0.11595611\n",
            "Iteration 1601, loss = 0.11589654\n",
            "Iteration 1602, loss = 0.11590591\n",
            "Iteration 1603, loss = 0.11585640\n",
            "Iteration 1604, loss = 0.11586288\n",
            "Iteration 1605, loss = 0.11583921\n",
            "Iteration 1606, loss = 0.11588876\n",
            "Iteration 1607, loss = 0.11591800\n",
            "Iteration 1608, loss = 0.11591684\n",
            "Iteration 1609, loss = 0.11583141\n",
            "Iteration 1610, loss = 0.11582585\n",
            "Iteration 1611, loss = 0.11585569\n",
            "Iteration 1612, loss = 0.11582438\n",
            "Iteration 1613, loss = 0.11588398\n",
            "Iteration 1614, loss = 0.11577595\n",
            "Iteration 1615, loss = 0.11578970\n",
            "Iteration 1616, loss = 0.11578793\n",
            "Iteration 1617, loss = 0.11575586\n",
            "Iteration 1618, loss = 0.11576319\n",
            "Iteration 1619, loss = 0.11574216\n",
            "Iteration 1620, loss = 0.11575793\n",
            "Iteration 1621, loss = 0.11573169\n",
            "Iteration 1622, loss = 0.11571041\n",
            "Iteration 1623, loss = 0.11573536\n",
            "Iteration 1624, loss = 0.11569991\n",
            "Iteration 1625, loss = 0.11575536\n",
            "Iteration 1626, loss = 0.11570879\n",
            "Iteration 1627, loss = 0.11568290\n",
            "Iteration 1628, loss = 0.11573569\n",
            "Iteration 1629, loss = 0.11569206\n",
            "Iteration 1630, loss = 0.11571025\n",
            "Iteration 1631, loss = 0.11567669\n",
            "Iteration 1632, loss = 0.11566386\n",
            "Iteration 1633, loss = 0.11567579\n",
            "Iteration 1634, loss = 0.11566295\n",
            "Iteration 1635, loss = 0.11571436\n",
            "Iteration 1636, loss = 0.11566925\n",
            "Iteration 1637, loss = 0.11561520\n",
            "Iteration 1638, loss = 0.11560673\n",
            "Iteration 1639, loss = 0.11561368\n",
            "Iteration 1640, loss = 0.11560858\n",
            "Iteration 1641, loss = 0.11558139\n",
            "Iteration 1642, loss = 0.11562743\n",
            "Iteration 1643, loss = 0.11556215\n",
            "Iteration 1644, loss = 0.11557752\n",
            "Iteration 1645, loss = 0.11560069\n",
            "Iteration 1646, loss = 0.11555289\n",
            "Iteration 1647, loss = 0.11563322\n",
            "Iteration 1648, loss = 0.11557334\n",
            "Iteration 1649, loss = 0.11552677\n",
            "Iteration 1650, loss = 0.11557334\n",
            "Iteration 1651, loss = 0.11554499\n",
            "Iteration 1652, loss = 0.11551268\n",
            "Iteration 1653, loss = 0.11552270\n",
            "Iteration 1654, loss = 0.11548429\n",
            "Iteration 1655, loss = 0.11547476\n",
            "Iteration 1656, loss = 0.11548353\n",
            "Iteration 1657, loss = 0.11546284\n",
            "Iteration 1658, loss = 0.11551287\n",
            "Iteration 1659, loss = 0.11552990\n",
            "Iteration 1660, loss = 0.11547428\n",
            "Iteration 1661, loss = 0.11544332\n",
            "Iteration 1662, loss = 0.11547492\n",
            "Iteration 1663, loss = 0.11552873\n",
            "Iteration 1664, loss = 0.11541734\n",
            "Iteration 1665, loss = 0.11545964\n",
            "Iteration 1666, loss = 0.11543109\n",
            "Iteration 1667, loss = 0.11546464\n",
            "Iteration 1668, loss = 0.11540265\n",
            "Iteration 1669, loss = 0.11540428\n",
            "Iteration 1670, loss = 0.11546906\n",
            "Iteration 1671, loss = 0.11545112\n",
            "Iteration 1672, loss = 0.11541483\n",
            "Iteration 1673, loss = 0.11539366\n",
            "Iteration 1674, loss = 0.11539404\n",
            "Iteration 1675, loss = 0.11542321\n",
            "Iteration 1676, loss = 0.11547904\n",
            "Iteration 1677, loss = 0.11533867\n",
            "Iteration 1678, loss = 0.11535397\n",
            "Iteration 1679, loss = 0.11538304\n",
            "Iteration 1680, loss = 0.11535936\n",
            "Iteration 1681, loss = 0.11546654\n",
            "Iteration 1682, loss = 0.11531192\n",
            "Iteration 1683, loss = 0.11537985\n",
            "Iteration 1684, loss = 0.11539869\n",
            "Iteration 1685, loss = 0.11534545\n",
            "Iteration 1686, loss = 0.11529302\n",
            "Iteration 1687, loss = 0.11527949\n",
            "Iteration 1688, loss = 0.11528210\n",
            "Iteration 1689, loss = 0.11529519\n",
            "Iteration 1690, loss = 0.11526848\n",
            "Iteration 1691, loss = 0.11525103\n",
            "Iteration 1692, loss = 0.11525927\n",
            "Iteration 1693, loss = 0.11523738\n",
            "Iteration 1694, loss = 0.11523946\n",
            "Iteration 1695, loss = 0.11525772\n",
            "Iteration 1696, loss = 0.11529315\n",
            "Iteration 1697, loss = 0.11524562\n",
            "Iteration 1698, loss = 0.11522915\n",
            "Iteration 1699, loss = 0.11520064\n",
            "Iteration 1700, loss = 0.11522820\n",
            "Iteration 1701, loss = 0.11519199\n",
            "Iteration 1702, loss = 0.11523399\n",
            "Iteration 1703, loss = 0.11530546\n",
            "Iteration 1704, loss = 0.11514126\n",
            "Iteration 1705, loss = 0.11521707\n",
            "Iteration 1706, loss = 0.11525637\n",
            "Iteration 1707, loss = 0.11521567\n",
            "Iteration 1708, loss = 0.11518878\n",
            "Iteration 1709, loss = 0.11518462\n",
            "Iteration 1710, loss = 0.11514335\n",
            "Iteration 1711, loss = 0.11515931\n",
            "Iteration 1712, loss = 0.11515097\n",
            "Iteration 1713, loss = 0.11522939\n",
            "Iteration 1714, loss = 0.11521537\n",
            "Iteration 1715, loss = 0.11515557\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(2, 2), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=5000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKZVL6k2i5uT"
      },
      "source": [
        "previsoes = neural_credit.predict(x_credit_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjHEoX1Xirp0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcaADHZoi133",
        "outputId": "0dcaf48d-f3e3-41db-8311-83a60c1bbf30"
      },
      "source": [
        "print(accuracy_score(y_credit_teste,previsoes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLXz3SZLjDNP"
      },
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0CQRYxuMjKNP",
        "outputId": "9783168d-3dc0-4b8b-8659-cb0e19588d31"
      },
      "source": [
        "cn = ConfusionMatrix(neural_credit)\n",
        "cn.fit(x_credit_treinamento,y_credit_treinamento)\n",
        "cn.score(x_credit_teste,y_credit_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.93216498\n",
            "Iteration 2, loss = 0.91349825\n",
            "Iteration 3, loss = 0.89381341\n",
            "Iteration 4, loss = 0.87400613\n",
            "Iteration 5, loss = 0.85460885\n",
            "Iteration 6, loss = 0.83540014\n",
            "Iteration 7, loss = 0.81760257\n",
            "Iteration 8, loss = 0.80011585\n",
            "Iteration 9, loss = 0.78377137\n",
            "Iteration 10, loss = 0.76778298\n",
            "Iteration 11, loss = 0.75266810\n",
            "Iteration 12, loss = 0.73811051\n",
            "Iteration 13, loss = 0.72399956\n",
            "Iteration 14, loss = 0.71057796\n",
            "Iteration 15, loss = 0.69754282\n",
            "Iteration 16, loss = 0.68502620\n",
            "Iteration 17, loss = 0.67310720\n",
            "Iteration 18, loss = 0.66149105\n",
            "Iteration 19, loss = 0.65038191\n",
            "Iteration 20, loss = 0.63956487\n",
            "Iteration 21, loss = 0.62929409\n",
            "Iteration 22, loss = 0.61923124\n",
            "Iteration 23, loss = 0.60967983\n",
            "Iteration 24, loss = 0.60036697\n",
            "Iteration 25, loss = 0.59152876\n",
            "Iteration 26, loss = 0.58300497\n",
            "Iteration 27, loss = 0.57455054\n",
            "Iteration 28, loss = 0.56670848\n",
            "Iteration 29, loss = 0.55889212\n",
            "Iteration 30, loss = 0.55160336\n",
            "Iteration 31, loss = 0.54448312\n",
            "Iteration 32, loss = 0.53755789\n",
            "Iteration 33, loss = 0.53089006\n",
            "Iteration 34, loss = 0.52453430\n",
            "Iteration 35, loss = 0.51845951\n",
            "Iteration 36, loss = 0.51257690\n",
            "Iteration 37, loss = 0.50676942\n",
            "Iteration 38, loss = 0.50125741\n",
            "Iteration 39, loss = 0.49588085\n",
            "Iteration 40, loss = 0.49064976\n",
            "Iteration 41, loss = 0.48550455\n",
            "Iteration 42, loss = 0.48053270\n",
            "Iteration 43, loss = 0.47576038\n",
            "Iteration 44, loss = 0.47109245\n",
            "Iteration 45, loss = 0.46643495\n",
            "Iteration 46, loss = 0.46201252\n",
            "Iteration 47, loss = 0.45761738\n",
            "Iteration 48, loss = 0.45328169\n",
            "Iteration 49, loss = 0.44915373\n",
            "Iteration 50, loss = 0.44507425\n",
            "Iteration 51, loss = 0.44111473\n",
            "Iteration 52, loss = 0.43721921\n",
            "Iteration 53, loss = 0.43340944\n",
            "Iteration 54, loss = 0.42971203\n",
            "Iteration 55, loss = 0.42615748\n",
            "Iteration 56, loss = 0.42259970\n",
            "Iteration 57, loss = 0.41923297\n",
            "Iteration 58, loss = 0.41579000\n",
            "Iteration 59, loss = 0.41258866\n",
            "Iteration 60, loss = 0.40932201\n",
            "Iteration 61, loss = 0.40631675\n",
            "Iteration 62, loss = 0.40323928\n",
            "Iteration 63, loss = 0.40032026\n",
            "Iteration 64, loss = 0.39741582\n",
            "Iteration 65, loss = 0.39458281\n",
            "Iteration 66, loss = 0.39182643\n",
            "Iteration 67, loss = 0.38910547\n",
            "Iteration 68, loss = 0.38645322\n",
            "Iteration 69, loss = 0.38381819\n",
            "Iteration 70, loss = 0.38129459\n",
            "Iteration 71, loss = 0.37874191\n",
            "Iteration 72, loss = 0.37629509\n",
            "Iteration 73, loss = 0.37382236\n",
            "Iteration 74, loss = 0.37139676\n",
            "Iteration 75, loss = 0.36901376\n",
            "Iteration 76, loss = 0.36665215\n",
            "Iteration 77, loss = 0.36434072\n",
            "Iteration 78, loss = 0.36207725\n",
            "Iteration 79, loss = 0.35979109\n",
            "Iteration 80, loss = 0.35753741\n",
            "Iteration 81, loss = 0.35529797\n",
            "Iteration 82, loss = 0.35305689\n",
            "Iteration 83, loss = 0.35084959\n",
            "Iteration 84, loss = 0.34866921\n",
            "Iteration 85, loss = 0.34645942\n",
            "Iteration 86, loss = 0.34426536\n",
            "Iteration 87, loss = 0.34211108\n",
            "Iteration 88, loss = 0.33990386\n",
            "Iteration 89, loss = 0.33766942\n",
            "Iteration 90, loss = 0.33546539\n",
            "Iteration 91, loss = 0.33319429\n",
            "Iteration 92, loss = 0.33099892\n",
            "Iteration 93, loss = 0.32873941\n",
            "Iteration 94, loss = 0.32654425\n",
            "Iteration 95, loss = 0.32427602\n",
            "Iteration 96, loss = 0.32202805\n",
            "Iteration 97, loss = 0.31973108\n",
            "Iteration 98, loss = 0.31747622\n",
            "Iteration 99, loss = 0.31516921\n",
            "Iteration 100, loss = 0.31284521\n",
            "Iteration 101, loss = 0.31058988\n",
            "Iteration 102, loss = 0.30820524\n",
            "Iteration 103, loss = 0.30587808\n",
            "Iteration 104, loss = 0.30352317\n",
            "Iteration 105, loss = 0.30114410\n",
            "Iteration 106, loss = 0.29877620\n",
            "Iteration 107, loss = 0.29640654\n",
            "Iteration 108, loss = 0.29401326\n",
            "Iteration 109, loss = 0.29168728\n",
            "Iteration 110, loss = 0.28926722\n",
            "Iteration 111, loss = 0.28693779\n",
            "Iteration 112, loss = 0.28455655\n",
            "Iteration 113, loss = 0.28224048\n",
            "Iteration 114, loss = 0.27987871\n",
            "Iteration 115, loss = 0.27747627\n",
            "Iteration 116, loss = 0.27515104\n",
            "Iteration 117, loss = 0.27282995\n",
            "Iteration 118, loss = 0.27037765\n",
            "Iteration 119, loss = 0.26810144\n",
            "Iteration 120, loss = 0.26568264\n",
            "Iteration 121, loss = 0.26337149\n",
            "Iteration 122, loss = 0.26099963\n",
            "Iteration 123, loss = 0.25863753\n",
            "Iteration 124, loss = 0.25628372\n",
            "Iteration 125, loss = 0.25396753\n",
            "Iteration 126, loss = 0.25169620\n",
            "Iteration 127, loss = 0.24939545\n",
            "Iteration 128, loss = 0.24706990\n",
            "Iteration 129, loss = 0.24485388\n",
            "Iteration 130, loss = 0.24260067\n",
            "Iteration 131, loss = 0.24035675\n",
            "Iteration 132, loss = 0.23816206\n",
            "Iteration 133, loss = 0.23598051\n",
            "Iteration 134, loss = 0.23384770\n",
            "Iteration 135, loss = 0.23169822\n",
            "Iteration 136, loss = 0.22963815\n",
            "Iteration 137, loss = 0.22748786\n",
            "Iteration 138, loss = 0.22553761\n",
            "Iteration 139, loss = 0.22347136\n",
            "Iteration 140, loss = 0.22149412\n",
            "Iteration 141, loss = 0.21957821\n",
            "Iteration 142, loss = 0.21761146\n",
            "Iteration 143, loss = 0.21577899\n",
            "Iteration 144, loss = 0.21381782\n",
            "Iteration 145, loss = 0.21203193\n",
            "Iteration 146, loss = 0.21022825\n",
            "Iteration 147, loss = 0.20843268\n",
            "Iteration 148, loss = 0.20667859\n",
            "Iteration 149, loss = 0.20494983\n",
            "Iteration 150, loss = 0.20325436\n",
            "Iteration 151, loss = 0.20161463\n",
            "Iteration 152, loss = 0.20002965\n",
            "Iteration 153, loss = 0.19844619\n",
            "Iteration 154, loss = 0.19680641\n",
            "Iteration 155, loss = 0.19522619\n",
            "Iteration 156, loss = 0.19371670\n",
            "Iteration 157, loss = 0.19223999\n",
            "Iteration 158, loss = 0.19087980\n",
            "Iteration 159, loss = 0.18943059\n",
            "Iteration 160, loss = 0.18805566\n",
            "Iteration 161, loss = 0.18666009\n",
            "Iteration 162, loss = 0.18536679\n",
            "Iteration 163, loss = 0.18405866\n",
            "Iteration 164, loss = 0.18279265\n",
            "Iteration 165, loss = 0.18155411\n",
            "Iteration 166, loss = 0.18038492\n",
            "Iteration 167, loss = 0.17913130\n",
            "Iteration 168, loss = 0.17797392\n",
            "Iteration 169, loss = 0.17682331\n",
            "Iteration 170, loss = 0.17571780\n",
            "Iteration 171, loss = 0.17462855\n",
            "Iteration 172, loss = 0.17360375\n",
            "Iteration 173, loss = 0.17251455\n",
            "Iteration 174, loss = 0.17156355\n",
            "Iteration 175, loss = 0.17053871\n",
            "Iteration 176, loss = 0.16954992\n",
            "Iteration 177, loss = 0.16858097\n",
            "Iteration 178, loss = 0.16762596\n",
            "Iteration 179, loss = 0.16668261\n",
            "Iteration 180, loss = 0.16579299\n",
            "Iteration 181, loss = 0.16486990\n",
            "Iteration 182, loss = 0.16399334\n",
            "Iteration 183, loss = 0.16313674\n",
            "Iteration 184, loss = 0.16230255\n",
            "Iteration 185, loss = 0.16147063\n",
            "Iteration 186, loss = 0.16066459\n",
            "Iteration 187, loss = 0.15988181\n",
            "Iteration 188, loss = 0.15911238\n",
            "Iteration 189, loss = 0.15833126\n",
            "Iteration 190, loss = 0.15760013\n",
            "Iteration 191, loss = 0.15682278\n",
            "Iteration 192, loss = 0.15608361\n",
            "Iteration 193, loss = 0.15533980\n",
            "Iteration 194, loss = 0.15468164\n",
            "Iteration 195, loss = 0.15399771\n",
            "Iteration 196, loss = 0.15339753\n",
            "Iteration 197, loss = 0.15271565\n",
            "Iteration 198, loss = 0.15209117\n",
            "Iteration 199, loss = 0.15151417\n",
            "Iteration 200, loss = 0.15091398\n",
            "Iteration 201, loss = 0.15030010\n",
            "Iteration 202, loss = 0.14971000\n",
            "Iteration 203, loss = 0.14916639\n",
            "Iteration 204, loss = 0.14856768\n",
            "Iteration 205, loss = 0.14806106\n",
            "Iteration 206, loss = 0.14750067\n",
            "Iteration 207, loss = 0.14701041\n",
            "Iteration 208, loss = 0.14645934\n",
            "Iteration 209, loss = 0.14595332\n",
            "Iteration 210, loss = 0.14544834\n",
            "Iteration 211, loss = 0.14514645\n",
            "Iteration 212, loss = 0.14456278\n",
            "Iteration 213, loss = 0.14406767\n",
            "Iteration 214, loss = 0.14359797\n",
            "Iteration 215, loss = 0.14317931\n",
            "Iteration 216, loss = 0.14277235\n",
            "Iteration 217, loss = 0.14233446\n",
            "Iteration 218, loss = 0.14194535\n",
            "Iteration 219, loss = 0.14156368\n",
            "Iteration 220, loss = 0.14117464\n",
            "Iteration 221, loss = 0.14076612\n",
            "Iteration 222, loss = 0.14031333\n",
            "Iteration 223, loss = 0.14000301\n",
            "Iteration 224, loss = 0.13959684\n",
            "Iteration 225, loss = 0.13928879\n",
            "Iteration 226, loss = 0.13894435\n",
            "Iteration 227, loss = 0.13855940\n",
            "Iteration 228, loss = 0.13821942\n",
            "Iteration 229, loss = 0.13787906\n",
            "Iteration 230, loss = 0.13754783\n",
            "Iteration 231, loss = 0.13725631\n",
            "Iteration 232, loss = 0.13692621\n",
            "Iteration 233, loss = 0.13661687\n",
            "Iteration 234, loss = 0.13627879\n",
            "Iteration 235, loss = 0.13598186\n",
            "Iteration 236, loss = 0.13574268\n",
            "Iteration 237, loss = 0.13542022\n",
            "Iteration 238, loss = 0.13511503\n",
            "Iteration 239, loss = 0.13481720\n",
            "Iteration 240, loss = 0.13456721\n",
            "Iteration 241, loss = 0.13436161\n",
            "Iteration 242, loss = 0.13402930\n",
            "Iteration 243, loss = 0.13377930\n",
            "Iteration 244, loss = 0.13356713\n",
            "Iteration 245, loss = 0.13329311\n",
            "Iteration 246, loss = 0.13313193\n",
            "Iteration 247, loss = 0.13286921\n",
            "Iteration 248, loss = 0.13259441\n",
            "Iteration 249, loss = 0.13242282\n",
            "Iteration 250, loss = 0.13214488\n",
            "Iteration 251, loss = 0.13188075\n",
            "Iteration 252, loss = 0.13166268\n",
            "Iteration 253, loss = 0.13144524\n",
            "Iteration 254, loss = 0.13125729\n",
            "Iteration 255, loss = 0.13104438\n",
            "Iteration 256, loss = 0.13085242\n",
            "Iteration 257, loss = 0.13066531\n",
            "Iteration 258, loss = 0.13044549\n",
            "Iteration 259, loss = 0.13023435\n",
            "Iteration 260, loss = 0.13001930\n",
            "Iteration 261, loss = 0.12993703\n",
            "Iteration 262, loss = 0.12973747\n",
            "Iteration 263, loss = 0.12948613\n",
            "Iteration 264, loss = 0.12926945\n",
            "Iteration 265, loss = 0.12905973\n",
            "Iteration 266, loss = 0.12886983\n",
            "Iteration 267, loss = 0.12874834\n",
            "Iteration 268, loss = 0.12853141\n",
            "Iteration 269, loss = 0.12830695\n",
            "Iteration 270, loss = 0.12819879\n",
            "Iteration 271, loss = 0.12807959\n",
            "Iteration 272, loss = 0.12789052\n",
            "Iteration 273, loss = 0.12768415\n",
            "Iteration 274, loss = 0.12757329\n",
            "Iteration 275, loss = 0.12735282\n",
            "Iteration 276, loss = 0.12724747\n",
            "Iteration 277, loss = 0.12708367\n",
            "Iteration 278, loss = 0.12691581\n",
            "Iteration 279, loss = 0.12674660\n",
            "Iteration 280, loss = 0.12656390\n",
            "Iteration 281, loss = 0.12642607\n",
            "Iteration 282, loss = 0.12621524\n",
            "Iteration 283, loss = 0.12608551\n",
            "Iteration 284, loss = 0.12593466\n",
            "Iteration 285, loss = 0.12580657\n",
            "Iteration 286, loss = 0.12567276\n",
            "Iteration 287, loss = 0.12550944\n",
            "Iteration 288, loss = 0.12532436\n",
            "Iteration 289, loss = 0.12518941\n",
            "Iteration 290, loss = 0.12505669\n",
            "Iteration 291, loss = 0.12493766\n",
            "Iteration 292, loss = 0.12482892\n",
            "Iteration 293, loss = 0.12470354\n",
            "Iteration 294, loss = 0.12453155\n",
            "Iteration 295, loss = 0.12444341\n",
            "Iteration 296, loss = 0.12429390\n",
            "Iteration 297, loss = 0.12418296\n",
            "Iteration 298, loss = 0.12414063\n",
            "Iteration 299, loss = 0.12397220\n",
            "Iteration 300, loss = 0.12386517\n",
            "Iteration 301, loss = 0.12374705\n",
            "Iteration 302, loss = 0.12359511\n",
            "Iteration 303, loss = 0.12349780\n",
            "Iteration 304, loss = 0.12337137\n",
            "Iteration 305, loss = 0.12327251\n",
            "Iteration 306, loss = 0.12315747\n",
            "Iteration 307, loss = 0.12309703\n",
            "Iteration 308, loss = 0.12300232\n",
            "Iteration 309, loss = 0.12284462\n",
            "Iteration 310, loss = 0.12276221\n",
            "Iteration 311, loss = 0.12274139\n",
            "Iteration 312, loss = 0.12261893\n",
            "Iteration 313, loss = 0.12248736\n",
            "Iteration 314, loss = 0.12236534\n",
            "Iteration 315, loss = 0.12232818\n",
            "Iteration 316, loss = 0.12221990\n",
            "Iteration 317, loss = 0.12212684\n",
            "Iteration 318, loss = 0.12217444\n",
            "Iteration 319, loss = 0.12195793\n",
            "Iteration 320, loss = 0.12186948\n",
            "Iteration 321, loss = 0.12183580\n",
            "Iteration 322, loss = 0.12168620\n",
            "Iteration 323, loss = 0.12161683\n",
            "Iteration 324, loss = 0.12153592\n",
            "Iteration 325, loss = 0.12142188\n",
            "Iteration 326, loss = 0.12137468\n",
            "Iteration 327, loss = 0.12130442\n",
            "Iteration 328, loss = 0.12122452\n",
            "Iteration 329, loss = 0.12112136\n",
            "Iteration 330, loss = 0.12104792\n",
            "Iteration 331, loss = 0.12095664\n",
            "Iteration 332, loss = 0.12084884\n",
            "Iteration 333, loss = 0.12087250\n",
            "Iteration 334, loss = 0.12072373\n",
            "Iteration 335, loss = 0.12064849\n",
            "Iteration 336, loss = 0.12058959\n",
            "Iteration 337, loss = 0.12049683\n",
            "Iteration 338, loss = 0.12053119\n",
            "Iteration 339, loss = 0.12039227\n",
            "Iteration 340, loss = 0.12030480\n",
            "Iteration 341, loss = 0.12021530\n",
            "Iteration 342, loss = 0.12014926\n",
            "Iteration 343, loss = 0.12011858\n",
            "Iteration 344, loss = 0.12011936\n",
            "Iteration 345, loss = 0.11995887\n",
            "Iteration 346, loss = 0.11988442\n",
            "Iteration 347, loss = 0.11981781\n",
            "Iteration 348, loss = 0.11974189\n",
            "Iteration 349, loss = 0.11973349\n",
            "Iteration 350, loss = 0.11970949\n",
            "Iteration 351, loss = 0.11964416\n",
            "Iteration 352, loss = 0.11954954\n",
            "Iteration 353, loss = 0.11944783\n",
            "Iteration 354, loss = 0.11940375\n",
            "Iteration 355, loss = 0.11934790\n",
            "Iteration 356, loss = 0.11928100\n",
            "Iteration 357, loss = 0.11924856\n",
            "Iteration 358, loss = 0.11922780\n",
            "Iteration 359, loss = 0.11917463\n",
            "Iteration 360, loss = 0.11904860\n",
            "Iteration 361, loss = 0.11899831\n",
            "Iteration 362, loss = 0.11894069\n",
            "Iteration 363, loss = 0.11889214\n",
            "Iteration 364, loss = 0.11884484\n",
            "Iteration 365, loss = 0.11880287\n",
            "Iteration 366, loss = 0.11882050\n",
            "Iteration 367, loss = 0.11877079\n",
            "Iteration 368, loss = 0.11866521\n",
            "Iteration 369, loss = 0.11862037\n",
            "Iteration 370, loss = 0.11864843\n",
            "Iteration 371, loss = 0.11850275\n",
            "Iteration 372, loss = 0.11841397\n",
            "Iteration 373, loss = 0.11837539\n",
            "Iteration 374, loss = 0.11832945\n",
            "Iteration 375, loss = 0.11833881\n",
            "Iteration 376, loss = 0.11825968\n",
            "Iteration 377, loss = 0.11822568\n",
            "Iteration 378, loss = 0.11817328\n",
            "Iteration 379, loss = 0.11810135\n",
            "Iteration 380, loss = 0.11813983\n",
            "Iteration 381, loss = 0.11801528\n",
            "Iteration 382, loss = 0.11797444\n",
            "Iteration 383, loss = 0.11805183\n",
            "Iteration 384, loss = 0.11792703\n",
            "Iteration 385, loss = 0.11791530\n",
            "Iteration 386, loss = 0.11776953\n",
            "Iteration 387, loss = 0.11776164\n",
            "Iteration 388, loss = 0.11768402\n",
            "Iteration 389, loss = 0.11763419\n",
            "Iteration 390, loss = 0.11760574\n",
            "Iteration 391, loss = 0.11753541\n",
            "Iteration 392, loss = 0.11751670\n",
            "Iteration 393, loss = 0.11749602\n",
            "Iteration 394, loss = 0.11739778\n",
            "Iteration 395, loss = 0.11738453\n",
            "Iteration 396, loss = 0.11734560\n",
            "Iteration 397, loss = 0.11728657\n",
            "Iteration 398, loss = 0.11725534\n",
            "Iteration 399, loss = 0.11722995\n",
            "Iteration 400, loss = 0.11718749\n",
            "Iteration 401, loss = 0.11711529\n",
            "Iteration 402, loss = 0.11718988\n",
            "Iteration 403, loss = 0.11705573\n",
            "Iteration 404, loss = 0.11701092\n",
            "Iteration 405, loss = 0.11704495\n",
            "Iteration 406, loss = 0.11698692\n",
            "Iteration 407, loss = 0.11691128\n",
            "Iteration 408, loss = 0.11683794\n",
            "Iteration 409, loss = 0.11711842\n",
            "Iteration 410, loss = 0.11678343\n",
            "Iteration 411, loss = 0.11672339\n",
            "Iteration 412, loss = 0.11681697\n",
            "Iteration 413, loss = 0.11670664\n",
            "Iteration 414, loss = 0.11685039\n",
            "Iteration 415, loss = 0.11661964\n",
            "Iteration 416, loss = 0.11655768\n",
            "Iteration 417, loss = 0.11654634\n",
            "Iteration 418, loss = 0.11646677\n",
            "Iteration 419, loss = 0.11644791\n",
            "Iteration 420, loss = 0.11638686\n",
            "Iteration 421, loss = 0.11638384\n",
            "Iteration 422, loss = 0.11631283\n",
            "Iteration 423, loss = 0.11626937\n",
            "Iteration 424, loss = 0.11627467\n",
            "Iteration 425, loss = 0.11628291\n",
            "Iteration 426, loss = 0.11622176\n",
            "Iteration 427, loss = 0.11613444\n",
            "Iteration 428, loss = 0.11612316\n",
            "Iteration 429, loss = 0.11612487\n",
            "Iteration 430, loss = 0.11606129\n",
            "Iteration 431, loss = 0.11602773\n",
            "Iteration 432, loss = 0.11606680\n",
            "Iteration 433, loss = 0.11592385\n",
            "Iteration 434, loss = 0.11590609\n",
            "Iteration 435, loss = 0.11586154\n",
            "Iteration 436, loss = 0.11586949\n",
            "Iteration 437, loss = 0.11578048\n",
            "Iteration 438, loss = 0.11591825\n",
            "Iteration 439, loss = 0.11569219\n",
            "Iteration 440, loss = 0.11567834\n",
            "Iteration 441, loss = 0.11567622\n",
            "Iteration 442, loss = 0.11567371\n",
            "Iteration 443, loss = 0.11561319\n",
            "Iteration 444, loss = 0.11556937\n",
            "Iteration 445, loss = 0.11554687\n",
            "Iteration 446, loss = 0.11559831\n",
            "Iteration 447, loss = 0.11543256\n",
            "Iteration 448, loss = 0.11542640\n",
            "Iteration 449, loss = 0.11537180\n",
            "Iteration 450, loss = 0.11538356\n",
            "Iteration 451, loss = 0.11549346\n",
            "Iteration 452, loss = 0.11529545\n",
            "Iteration 453, loss = 0.11528669\n",
            "Iteration 454, loss = 0.11520808\n",
            "Iteration 455, loss = 0.11515275\n",
            "Iteration 456, loss = 0.11517339\n",
            "Iteration 457, loss = 0.11512041\n",
            "Iteration 458, loss = 0.11515179\n",
            "Iteration 459, loss = 0.11505804\n",
            "Iteration 460, loss = 0.11500574\n",
            "Iteration 461, loss = 0.11496313\n",
            "Iteration 462, loss = 0.11492227\n",
            "Iteration 463, loss = 0.11489104\n",
            "Iteration 464, loss = 0.11491588\n",
            "Iteration 465, loss = 0.11480146\n",
            "Iteration 466, loss = 0.11485408\n",
            "Iteration 467, loss = 0.11480848\n",
            "Iteration 468, loss = 0.11478284\n",
            "Iteration 469, loss = 0.11474745\n",
            "Iteration 470, loss = 0.11474471\n",
            "Iteration 471, loss = 0.11492061\n",
            "Iteration 472, loss = 0.11454696\n",
            "Iteration 473, loss = 0.11451203\n",
            "Iteration 474, loss = 0.11450049\n",
            "Iteration 475, loss = 0.11444951\n",
            "Iteration 476, loss = 0.11442005\n",
            "Iteration 477, loss = 0.11444956\n",
            "Iteration 478, loss = 0.11436219\n",
            "Iteration 479, loss = 0.11437046\n",
            "Iteration 480, loss = 0.11429364\n",
            "Iteration 481, loss = 0.11422906\n",
            "Iteration 482, loss = 0.11424047\n",
            "Iteration 483, loss = 0.11418763\n",
            "Iteration 484, loss = 0.11417134\n",
            "Iteration 485, loss = 0.11408065\n",
            "Iteration 486, loss = 0.11402741\n",
            "Iteration 487, loss = 0.11401863\n",
            "Iteration 488, loss = 0.11409113\n",
            "Iteration 489, loss = 0.11393674\n",
            "Iteration 490, loss = 0.11385538\n",
            "Iteration 491, loss = 0.11393047\n",
            "Iteration 492, loss = 0.11382059\n",
            "Iteration 493, loss = 0.11379292\n",
            "Iteration 494, loss = 0.11385893\n",
            "Iteration 495, loss = 0.11364402\n",
            "Iteration 496, loss = 0.11366197\n",
            "Iteration 497, loss = 0.11363314\n",
            "Iteration 498, loss = 0.11369886\n",
            "Iteration 499, loss = 0.11350479\n",
            "Iteration 500, loss = 0.11350971\n",
            "Iteration 501, loss = 0.11340700\n",
            "Iteration 502, loss = 0.11338852\n",
            "Iteration 503, loss = 0.11334968\n",
            "Iteration 504, loss = 0.11332083\n",
            "Iteration 505, loss = 0.11331025\n",
            "Iteration 506, loss = 0.11320459\n",
            "Iteration 507, loss = 0.11316364\n",
            "Iteration 508, loss = 0.11311550\n",
            "Iteration 509, loss = 0.11310354\n",
            "Iteration 510, loss = 0.11305003\n",
            "Iteration 511, loss = 0.11299619\n",
            "Iteration 512, loss = 0.11294966\n",
            "Iteration 513, loss = 0.11291786\n",
            "Iteration 514, loss = 0.11285416\n",
            "Iteration 515, loss = 0.11287344\n",
            "Iteration 516, loss = 0.11278084\n",
            "Iteration 517, loss = 0.11270767\n",
            "Iteration 518, loss = 0.11272254\n",
            "Iteration 519, loss = 0.11260740\n",
            "Iteration 520, loss = 0.11258408\n",
            "Iteration 521, loss = 0.11253155\n",
            "Iteration 522, loss = 0.11248023\n",
            "Iteration 523, loss = 0.11242160\n",
            "Iteration 524, loss = 0.11241093\n",
            "Iteration 525, loss = 0.11237661\n",
            "Iteration 526, loss = 0.11238493\n",
            "Iteration 527, loss = 0.11234406\n",
            "Iteration 528, loss = 0.11229455\n",
            "Iteration 529, loss = 0.11226399\n",
            "Iteration 530, loss = 0.11226514\n",
            "Iteration 531, loss = 0.11213315\n",
            "Iteration 532, loss = 0.11204541\n",
            "Iteration 533, loss = 0.11206092\n",
            "Iteration 534, loss = 0.11195352\n",
            "Iteration 535, loss = 0.11191867\n",
            "Iteration 536, loss = 0.11190890\n",
            "Iteration 537, loss = 0.11188485\n",
            "Iteration 538, loss = 0.11180181\n",
            "Iteration 539, loss = 0.11173260\n",
            "Iteration 540, loss = 0.11171498\n",
            "Iteration 541, loss = 0.11168032\n",
            "Iteration 542, loss = 0.11161404\n",
            "Iteration 543, loss = 0.11156130\n",
            "Iteration 544, loss = 0.11151915\n",
            "Iteration 545, loss = 0.11149022\n",
            "Iteration 546, loss = 0.11138908\n",
            "Iteration 547, loss = 0.11144471\n",
            "Iteration 548, loss = 0.11135400\n",
            "Iteration 549, loss = 0.11123510\n",
            "Iteration 550, loss = 0.11122711\n",
            "Iteration 551, loss = 0.11115354\n",
            "Iteration 552, loss = 0.11110766\n",
            "Iteration 553, loss = 0.11102476\n",
            "Iteration 554, loss = 0.11095782\n",
            "Iteration 555, loss = 0.11093953\n",
            "Iteration 556, loss = 0.11095709\n",
            "Iteration 557, loss = 0.11086292\n",
            "Iteration 558, loss = 0.11078453\n",
            "Iteration 559, loss = 0.11071837\n",
            "Iteration 560, loss = 0.11067324\n",
            "Iteration 561, loss = 0.11059534\n",
            "Iteration 562, loss = 0.11054188\n",
            "Iteration 563, loss = 0.11045895\n",
            "Iteration 564, loss = 0.11040985\n",
            "Iteration 565, loss = 0.11033198\n",
            "Iteration 566, loss = 0.11022882\n",
            "Iteration 567, loss = 0.11021913\n",
            "Iteration 568, loss = 0.11009392\n",
            "Iteration 569, loss = 0.11004694\n",
            "Iteration 570, loss = 0.10993381\n",
            "Iteration 571, loss = 0.10984509\n",
            "Iteration 572, loss = 0.10970884\n",
            "Iteration 573, loss = 0.10973667\n",
            "Iteration 574, loss = 0.10962561\n",
            "Iteration 575, loss = 0.10964956\n",
            "Iteration 576, loss = 0.10943801\n",
            "Iteration 577, loss = 0.10937618\n",
            "Iteration 578, loss = 0.10924877\n",
            "Iteration 579, loss = 0.10920299\n",
            "Iteration 580, loss = 0.10912651\n",
            "Iteration 581, loss = 0.10903568\n",
            "Iteration 582, loss = 0.10893648\n",
            "Iteration 583, loss = 0.10881242\n",
            "Iteration 584, loss = 0.10875473\n",
            "Iteration 585, loss = 0.10867710\n",
            "Iteration 586, loss = 0.10858416\n",
            "Iteration 587, loss = 0.10864772\n",
            "Iteration 588, loss = 0.10841823\n",
            "Iteration 589, loss = 0.10852642\n",
            "Iteration 590, loss = 0.10830624\n",
            "Iteration 591, loss = 0.10815401\n",
            "Iteration 592, loss = 0.10801748\n",
            "Iteration 593, loss = 0.10796210\n",
            "Iteration 594, loss = 0.10785055\n",
            "Iteration 595, loss = 0.10773999\n",
            "Iteration 596, loss = 0.10764466\n",
            "Iteration 597, loss = 0.10759321\n",
            "Iteration 598, loss = 0.10744005\n",
            "Iteration 599, loss = 0.10740249\n",
            "Iteration 600, loss = 0.10720651\n",
            "Iteration 601, loss = 0.10722030\n",
            "Iteration 602, loss = 0.10697715\n",
            "Iteration 603, loss = 0.10692342\n",
            "Iteration 604, loss = 0.10674389\n",
            "Iteration 605, loss = 0.10664310\n",
            "Iteration 606, loss = 0.10650427\n",
            "Iteration 607, loss = 0.10638876\n",
            "Iteration 608, loss = 0.10626864\n",
            "Iteration 609, loss = 0.10611609\n",
            "Iteration 610, loss = 0.10596123\n",
            "Iteration 611, loss = 0.10578333\n",
            "Iteration 612, loss = 0.10568493\n",
            "Iteration 613, loss = 0.10543362\n",
            "Iteration 614, loss = 0.10529947\n",
            "Iteration 615, loss = 0.10514020\n",
            "Iteration 616, loss = 0.10498839\n",
            "Iteration 617, loss = 0.10479909\n",
            "Iteration 618, loss = 0.10462324\n",
            "Iteration 619, loss = 0.10447786\n",
            "Iteration 620, loss = 0.10431746\n",
            "Iteration 621, loss = 0.10409625\n",
            "Iteration 622, loss = 0.10395117\n",
            "Iteration 623, loss = 0.10375307\n",
            "Iteration 624, loss = 0.10354552\n",
            "Iteration 625, loss = 0.10336250\n",
            "Iteration 626, loss = 0.10318181\n",
            "Iteration 627, loss = 0.10297632\n",
            "Iteration 628, loss = 0.10277651\n",
            "Iteration 629, loss = 0.10268941\n",
            "Iteration 630, loss = 0.10232521\n",
            "Iteration 631, loss = 0.10209984\n",
            "Iteration 632, loss = 0.10186620\n",
            "Iteration 633, loss = 0.10166778\n",
            "Iteration 634, loss = 0.10139087\n",
            "Iteration 635, loss = 0.10114866\n",
            "Iteration 636, loss = 0.10090716\n",
            "Iteration 637, loss = 0.10066050\n",
            "Iteration 638, loss = 0.10037854\n",
            "Iteration 639, loss = 0.10011303\n",
            "Iteration 640, loss = 0.09987686\n",
            "Iteration 641, loss = 0.09961201\n",
            "Iteration 642, loss = 0.09942044\n",
            "Iteration 643, loss = 0.09908169\n",
            "Iteration 644, loss = 0.09887973\n",
            "Iteration 645, loss = 0.09866993\n",
            "Iteration 646, loss = 0.09838514\n",
            "Iteration 647, loss = 0.09809390\n",
            "Iteration 648, loss = 0.09792249\n",
            "Iteration 649, loss = 0.09760634\n",
            "Iteration 650, loss = 0.09733757\n",
            "Iteration 651, loss = 0.09718467\n",
            "Iteration 652, loss = 0.09684070\n",
            "Iteration 653, loss = 0.09660250\n",
            "Iteration 654, loss = 0.09628820\n",
            "Iteration 655, loss = 0.09616866\n",
            "Iteration 656, loss = 0.09579044\n",
            "Iteration 657, loss = 0.09552424\n",
            "Iteration 658, loss = 0.09524869\n",
            "Iteration 659, loss = 0.09501411\n",
            "Iteration 660, loss = 0.09470480\n",
            "Iteration 661, loss = 0.09445528\n",
            "Iteration 662, loss = 0.09420357\n",
            "Iteration 663, loss = 0.09394410\n",
            "Iteration 664, loss = 0.09364698\n",
            "Iteration 665, loss = 0.09343534\n",
            "Iteration 666, loss = 0.09312233\n",
            "Iteration 667, loss = 0.09288892\n",
            "Iteration 668, loss = 0.09260382\n",
            "Iteration 669, loss = 0.09233314\n",
            "Iteration 670, loss = 0.09200217\n",
            "Iteration 671, loss = 0.09176401\n",
            "Iteration 672, loss = 0.09144972\n",
            "Iteration 673, loss = 0.09121292\n",
            "Iteration 674, loss = 0.09088819\n",
            "Iteration 675, loss = 0.09062651\n",
            "Iteration 676, loss = 0.09041721\n",
            "Iteration 677, loss = 0.09002150\n",
            "Iteration 678, loss = 0.08980153\n",
            "Iteration 679, loss = 0.08943261\n",
            "Iteration 680, loss = 0.08916371\n",
            "Iteration 681, loss = 0.08890732\n",
            "Iteration 682, loss = 0.08856097\n",
            "Iteration 683, loss = 0.08829598\n",
            "Iteration 684, loss = 0.08797479\n",
            "Iteration 685, loss = 0.08772363\n",
            "Iteration 686, loss = 0.08741437\n",
            "Iteration 687, loss = 0.08713187\n",
            "Iteration 688, loss = 0.08679572\n",
            "Iteration 689, loss = 0.08649263\n",
            "Iteration 690, loss = 0.08628093\n",
            "Iteration 691, loss = 0.08595057\n",
            "Iteration 692, loss = 0.08565028\n",
            "Iteration 693, loss = 0.08526250\n",
            "Iteration 694, loss = 0.08496230\n",
            "Iteration 695, loss = 0.08459875\n",
            "Iteration 696, loss = 0.08425083\n",
            "Iteration 697, loss = 0.08393427\n",
            "Iteration 698, loss = 0.08362609\n",
            "Iteration 699, loss = 0.08335388\n",
            "Iteration 700, loss = 0.08301905\n",
            "Iteration 701, loss = 0.08271479\n",
            "Iteration 702, loss = 0.08233910\n",
            "Iteration 703, loss = 0.08204045\n",
            "Iteration 704, loss = 0.08174915\n",
            "Iteration 705, loss = 0.08140719\n",
            "Iteration 706, loss = 0.08108150\n",
            "Iteration 707, loss = 0.08073932\n",
            "Iteration 708, loss = 0.08039926\n",
            "Iteration 709, loss = 0.08017255\n",
            "Iteration 710, loss = 0.07981886\n",
            "Iteration 711, loss = 0.07944290\n",
            "Iteration 712, loss = 0.07909580\n",
            "Iteration 713, loss = 0.07880967\n",
            "Iteration 714, loss = 0.07846911\n",
            "Iteration 715, loss = 0.07809997\n",
            "Iteration 716, loss = 0.07786510\n",
            "Iteration 717, loss = 0.07747351\n",
            "Iteration 718, loss = 0.07717248\n",
            "Iteration 719, loss = 0.07682874\n",
            "Iteration 720, loss = 0.07652088\n",
            "Iteration 721, loss = 0.07617726\n",
            "Iteration 722, loss = 0.07602162\n",
            "Iteration 723, loss = 0.07551961\n",
            "Iteration 724, loss = 0.07516995\n",
            "Iteration 725, loss = 0.07487110\n",
            "Iteration 726, loss = 0.07456811\n",
            "Iteration 727, loss = 0.07435377\n",
            "Iteration 728, loss = 0.07393167\n",
            "Iteration 729, loss = 0.07364726\n",
            "Iteration 730, loss = 0.07338060\n",
            "Iteration 731, loss = 0.07308987\n",
            "Iteration 732, loss = 0.07271876\n",
            "Iteration 733, loss = 0.07242725\n",
            "Iteration 734, loss = 0.07211670\n",
            "Iteration 735, loss = 0.07184201\n",
            "Iteration 736, loss = 0.07149854\n",
            "Iteration 737, loss = 0.07122231\n",
            "Iteration 738, loss = 0.07090472\n",
            "Iteration 739, loss = 0.07058375\n",
            "Iteration 740, loss = 0.07027869\n",
            "Iteration 741, loss = 0.06998375\n",
            "Iteration 742, loss = 0.06970686\n",
            "Iteration 743, loss = 0.06935103\n",
            "Iteration 744, loss = 0.06903724\n",
            "Iteration 745, loss = 0.06868695\n",
            "Iteration 746, loss = 0.06841075\n",
            "Iteration 747, loss = 0.06805736\n",
            "Iteration 748, loss = 0.06781168\n",
            "Iteration 749, loss = 0.06753061\n",
            "Iteration 750, loss = 0.06720330\n",
            "Iteration 751, loss = 0.06694011\n",
            "Iteration 752, loss = 0.06658004\n",
            "Iteration 753, loss = 0.06632525\n",
            "Iteration 754, loss = 0.06599103\n",
            "Iteration 755, loss = 0.06569595\n",
            "Iteration 756, loss = 0.06535383\n",
            "Iteration 757, loss = 0.06507621\n",
            "Iteration 758, loss = 0.06475971\n",
            "Iteration 759, loss = 0.06444638\n",
            "Iteration 760, loss = 0.06412594\n",
            "Iteration 761, loss = 0.06382322\n",
            "Iteration 762, loss = 0.06352215\n",
            "Iteration 763, loss = 0.06322878\n",
            "Iteration 764, loss = 0.06295735\n",
            "Iteration 765, loss = 0.06261347\n",
            "Iteration 766, loss = 0.06232372\n",
            "Iteration 767, loss = 0.06204052\n",
            "Iteration 768, loss = 0.06175618\n",
            "Iteration 769, loss = 0.06145761\n",
            "Iteration 770, loss = 0.06120496\n",
            "Iteration 771, loss = 0.06088155\n",
            "Iteration 772, loss = 0.06058914\n",
            "Iteration 773, loss = 0.06034614\n",
            "Iteration 774, loss = 0.06004361\n",
            "Iteration 775, loss = 0.05975154\n",
            "Iteration 776, loss = 0.05947697\n",
            "Iteration 777, loss = 0.05919212\n",
            "Iteration 778, loss = 0.05896209\n",
            "Iteration 779, loss = 0.05873759\n",
            "Iteration 780, loss = 0.05837897\n",
            "Iteration 781, loss = 0.05813524\n",
            "Iteration 782, loss = 0.05787998\n",
            "Iteration 783, loss = 0.05759463\n",
            "Iteration 784, loss = 0.05733207\n",
            "Iteration 785, loss = 0.05708204\n",
            "Iteration 786, loss = 0.05684978\n",
            "Iteration 787, loss = 0.05658712\n",
            "Iteration 788, loss = 0.05636249\n",
            "Iteration 789, loss = 0.05610633\n",
            "Iteration 790, loss = 0.05584866\n",
            "Iteration 791, loss = 0.05562393\n",
            "Iteration 792, loss = 0.05540653\n",
            "Iteration 793, loss = 0.05511728\n",
            "Iteration 794, loss = 0.05487557\n",
            "Iteration 795, loss = 0.05466445\n",
            "Iteration 796, loss = 0.05444567\n",
            "Iteration 797, loss = 0.05423763\n",
            "Iteration 798, loss = 0.05405575\n",
            "Iteration 799, loss = 0.05380196\n",
            "Iteration 800, loss = 0.05361367\n",
            "Iteration 801, loss = 0.05339621\n",
            "Iteration 802, loss = 0.05316526\n",
            "Iteration 803, loss = 0.05303223\n",
            "Iteration 804, loss = 0.05275589\n",
            "Iteration 805, loss = 0.05261648\n",
            "Iteration 806, loss = 0.05231810\n",
            "Iteration 807, loss = 0.05211944\n",
            "Iteration 808, loss = 0.05191262\n",
            "Iteration 809, loss = 0.05172523\n",
            "Iteration 810, loss = 0.05150676\n",
            "Iteration 811, loss = 0.05133126\n",
            "Iteration 812, loss = 0.05112532\n",
            "Iteration 813, loss = 0.05090993\n",
            "Iteration 814, loss = 0.05074609\n",
            "Iteration 815, loss = 0.05052281\n",
            "Iteration 816, loss = 0.05034210\n",
            "Iteration 817, loss = 0.05014202\n",
            "Iteration 818, loss = 0.04996042\n",
            "Iteration 819, loss = 0.04977723\n",
            "Iteration 820, loss = 0.04956966\n",
            "Iteration 821, loss = 0.04949718\n",
            "Iteration 822, loss = 0.04924113\n",
            "Iteration 823, loss = 0.04904181\n",
            "Iteration 824, loss = 0.04884864\n",
            "Iteration 825, loss = 0.04867612\n",
            "Iteration 826, loss = 0.04851401\n",
            "Iteration 827, loss = 0.04835005\n",
            "Iteration 828, loss = 0.04816098\n",
            "Iteration 829, loss = 0.04802785\n",
            "Iteration 830, loss = 0.04784947\n",
            "Iteration 831, loss = 0.04767982\n",
            "Iteration 832, loss = 0.04750046\n",
            "Iteration 833, loss = 0.04735733\n",
            "Iteration 834, loss = 0.04722707\n",
            "Iteration 835, loss = 0.04702318\n",
            "Iteration 836, loss = 0.04689658\n",
            "Iteration 837, loss = 0.04673503\n",
            "Iteration 838, loss = 0.04660680\n",
            "Iteration 839, loss = 0.04638670\n",
            "Iteration 840, loss = 0.04626894\n",
            "Iteration 841, loss = 0.04613162\n",
            "Iteration 842, loss = 0.04603231\n",
            "Iteration 843, loss = 0.04583987\n",
            "Iteration 844, loss = 0.04566439\n",
            "Iteration 845, loss = 0.04552170\n",
            "Iteration 846, loss = 0.04540570\n",
            "Iteration 847, loss = 0.04532011\n",
            "Iteration 848, loss = 0.04510127\n",
            "Iteration 849, loss = 0.04507053\n",
            "Iteration 850, loss = 0.04483544\n",
            "Iteration 851, loss = 0.04470644\n",
            "Iteration 852, loss = 0.04454557\n",
            "Iteration 853, loss = 0.04442683\n",
            "Iteration 854, loss = 0.04431420\n",
            "Iteration 855, loss = 0.04422306\n",
            "Iteration 856, loss = 0.04401821\n",
            "Iteration 857, loss = 0.04388488\n",
            "Iteration 858, loss = 0.04382450\n",
            "Iteration 859, loss = 0.04360689\n",
            "Iteration 860, loss = 0.04351449\n",
            "Iteration 861, loss = 0.04339755\n",
            "Iteration 862, loss = 0.04328146\n",
            "Iteration 863, loss = 0.04318868\n",
            "Iteration 864, loss = 0.04306119\n",
            "Iteration 865, loss = 0.04290181\n",
            "Iteration 866, loss = 0.04276858\n",
            "Iteration 867, loss = 0.04264978\n",
            "Iteration 868, loss = 0.04261678\n",
            "Iteration 869, loss = 0.04243142\n",
            "Iteration 870, loss = 0.04236129\n",
            "Iteration 871, loss = 0.04221363\n",
            "Iteration 872, loss = 0.04208861\n",
            "Iteration 873, loss = 0.04196585\n",
            "Iteration 874, loss = 0.04184129\n",
            "Iteration 875, loss = 0.04171241\n",
            "Iteration 876, loss = 0.04161065\n",
            "Iteration 877, loss = 0.04153699\n",
            "Iteration 878, loss = 0.04140749\n",
            "Iteration 879, loss = 0.04128224\n",
            "Iteration 880, loss = 0.04117306\n",
            "Iteration 881, loss = 0.04112370\n",
            "Iteration 882, loss = 0.04093975\n",
            "Iteration 883, loss = 0.04086206\n",
            "Iteration 884, loss = 0.04073708\n",
            "Iteration 885, loss = 0.04080153\n",
            "Iteration 886, loss = 0.04052278\n",
            "Iteration 887, loss = 0.04041975\n",
            "Iteration 888, loss = 0.04031842\n",
            "Iteration 889, loss = 0.04020322\n",
            "Iteration 890, loss = 0.04013265\n",
            "Iteration 891, loss = 0.04001527\n",
            "Iteration 892, loss = 0.03986590\n",
            "Iteration 893, loss = 0.03984755\n",
            "Iteration 894, loss = 0.03965459\n",
            "Iteration 895, loss = 0.03961682\n",
            "Iteration 896, loss = 0.03948426\n",
            "Iteration 897, loss = 0.03935823\n",
            "Iteration 898, loss = 0.03926845\n",
            "Iteration 899, loss = 0.03918666\n",
            "Iteration 900, loss = 0.03905854\n",
            "Iteration 901, loss = 0.03896397\n",
            "Iteration 902, loss = 0.03886462\n",
            "Iteration 903, loss = 0.03874353\n",
            "Iteration 904, loss = 0.03865219\n",
            "Iteration 905, loss = 0.03857160\n",
            "Iteration 906, loss = 0.03846253\n",
            "Iteration 907, loss = 0.03836373\n",
            "Iteration 908, loss = 0.03826725\n",
            "Iteration 909, loss = 0.03816601\n",
            "Iteration 910, loss = 0.03810526\n",
            "Iteration 911, loss = 0.03799993\n",
            "Iteration 912, loss = 0.03788736\n",
            "Iteration 913, loss = 0.03786042\n",
            "Iteration 914, loss = 0.03771385\n",
            "Iteration 915, loss = 0.03763082\n",
            "Iteration 916, loss = 0.03754595\n",
            "Iteration 917, loss = 0.03746064\n",
            "Iteration 918, loss = 0.03734427\n",
            "Iteration 919, loss = 0.03727962\n",
            "Iteration 920, loss = 0.03716014\n",
            "Iteration 921, loss = 0.03711336\n",
            "Iteration 922, loss = 0.03704886\n",
            "Iteration 923, loss = 0.03693799\n",
            "Iteration 924, loss = 0.03684576\n",
            "Iteration 925, loss = 0.03673904\n",
            "Iteration 926, loss = 0.03665622\n",
            "Iteration 927, loss = 0.03658121\n",
            "Iteration 928, loss = 0.03648631\n",
            "Iteration 929, loss = 0.03642396\n",
            "Iteration 930, loss = 0.03633984\n",
            "Iteration 931, loss = 0.03628281\n",
            "Iteration 932, loss = 0.03616937\n",
            "Iteration 933, loss = 0.03609576\n",
            "Iteration 934, loss = 0.03601765\n",
            "Iteration 935, loss = 0.03593526\n",
            "Iteration 936, loss = 0.03586614\n",
            "Iteration 937, loss = 0.03576156\n",
            "Iteration 938, loss = 0.03568983\n",
            "Iteration 939, loss = 0.03559432\n",
            "Iteration 940, loss = 0.03550568\n",
            "Iteration 941, loss = 0.03554357\n",
            "Iteration 942, loss = 0.03546147\n",
            "Iteration 943, loss = 0.03532802\n",
            "Iteration 944, loss = 0.03523315\n",
            "Iteration 945, loss = 0.03515174\n",
            "Iteration 946, loss = 0.03506549\n",
            "Iteration 947, loss = 0.03499220\n",
            "Iteration 948, loss = 0.03491568\n",
            "Iteration 949, loss = 0.03488985\n",
            "Iteration 950, loss = 0.03481869\n",
            "Iteration 951, loss = 0.03469744\n",
            "Iteration 952, loss = 0.03461845\n",
            "Iteration 953, loss = 0.03456128\n",
            "Iteration 954, loss = 0.03446942\n",
            "Iteration 955, loss = 0.03444052\n",
            "Iteration 956, loss = 0.03434256\n",
            "Iteration 957, loss = 0.03427695\n",
            "Iteration 958, loss = 0.03419472\n",
            "Iteration 959, loss = 0.03412721\n",
            "Iteration 960, loss = 0.03406126\n",
            "Iteration 961, loss = 0.03398842\n",
            "Iteration 962, loss = 0.03389586\n",
            "Iteration 963, loss = 0.03385430\n",
            "Iteration 964, loss = 0.03378101\n",
            "Iteration 965, loss = 0.03372261\n",
            "Iteration 966, loss = 0.03365384\n",
            "Iteration 967, loss = 0.03360897\n",
            "Iteration 968, loss = 0.03350354\n",
            "Iteration 969, loss = 0.03343031\n",
            "Iteration 970, loss = 0.03339697\n",
            "Iteration 971, loss = 0.03329454\n",
            "Iteration 972, loss = 0.03324213\n",
            "Iteration 973, loss = 0.03316670\n",
            "Iteration 974, loss = 0.03307779\n",
            "Iteration 975, loss = 0.03299664\n",
            "Iteration 976, loss = 0.03298136\n",
            "Iteration 977, loss = 0.03291096\n",
            "Iteration 978, loss = 0.03284686\n",
            "Iteration 979, loss = 0.03278782\n",
            "Iteration 980, loss = 0.03269903\n",
            "Iteration 981, loss = 0.03269903\n",
            "Iteration 982, loss = 0.03263093\n",
            "Iteration 983, loss = 0.03254092\n",
            "Iteration 984, loss = 0.03244734\n",
            "Iteration 985, loss = 0.03239985\n",
            "Iteration 986, loss = 0.03231473\n",
            "Iteration 987, loss = 0.03227194\n",
            "Iteration 988, loss = 0.03221201\n",
            "Iteration 989, loss = 0.03212915\n",
            "Iteration 990, loss = 0.03206749\n",
            "Iteration 991, loss = 0.03199950\n",
            "Iteration 992, loss = 0.03199607\n",
            "Iteration 993, loss = 0.03189653\n",
            "Iteration 994, loss = 0.03185860\n",
            "Iteration 995, loss = 0.03178629\n",
            "Iteration 996, loss = 0.03169090\n",
            "Iteration 997, loss = 0.03163386\n",
            "Iteration 998, loss = 0.03158339\n",
            "Iteration 999, loss = 0.03154079\n",
            "Iteration 1000, loss = 0.03143033\n",
            "Iteration 1001, loss = 0.03143523\n",
            "Iteration 1002, loss = 0.03136936\n",
            "Iteration 1003, loss = 0.03127334\n",
            "Iteration 1004, loss = 0.03120999\n",
            "Iteration 1005, loss = 0.03114749\n",
            "Iteration 1006, loss = 0.03110429\n",
            "Iteration 1007, loss = 0.03105119\n",
            "Iteration 1008, loss = 0.03100326\n",
            "Iteration 1009, loss = 0.03096117\n",
            "Iteration 1010, loss = 0.03086745\n",
            "Iteration 1011, loss = 0.03081145\n",
            "Iteration 1012, loss = 0.03077819\n",
            "Iteration 1013, loss = 0.03069275\n",
            "Iteration 1014, loss = 0.03064145\n",
            "Iteration 1015, loss = 0.03063072\n",
            "Iteration 1016, loss = 0.03054327\n",
            "Iteration 1017, loss = 0.03054631\n",
            "Iteration 1018, loss = 0.03045045\n",
            "Iteration 1019, loss = 0.03037647\n",
            "Iteration 1020, loss = 0.03030179\n",
            "Iteration 1021, loss = 0.03024310\n",
            "Iteration 1022, loss = 0.03025565\n",
            "Iteration 1023, loss = 0.03013823\n",
            "Iteration 1024, loss = 0.03008579\n",
            "Iteration 1025, loss = 0.03004202\n",
            "Iteration 1026, loss = 0.02999441\n",
            "Iteration 1027, loss = 0.02997415\n",
            "Iteration 1028, loss = 0.02988261\n",
            "Iteration 1029, loss = 0.02981628\n",
            "Iteration 1030, loss = 0.02977076\n",
            "Iteration 1031, loss = 0.02974006\n",
            "Iteration 1032, loss = 0.02966851\n",
            "Iteration 1033, loss = 0.02961072\n",
            "Iteration 1034, loss = 0.02955565\n",
            "Iteration 1035, loss = 0.02949677\n",
            "Iteration 1036, loss = 0.02947809\n",
            "Iteration 1037, loss = 0.02940104\n",
            "Iteration 1038, loss = 0.02937124\n",
            "Iteration 1039, loss = 0.02929010\n",
            "Iteration 1040, loss = 0.02925534\n",
            "Iteration 1041, loss = 0.02921765\n",
            "Iteration 1042, loss = 0.02916924\n",
            "Iteration 1043, loss = 0.02909694\n",
            "Iteration 1044, loss = 0.02904975\n",
            "Iteration 1045, loss = 0.02897545\n",
            "Iteration 1046, loss = 0.02895677\n",
            "Iteration 1047, loss = 0.02894036\n",
            "Iteration 1048, loss = 0.02884377\n",
            "Iteration 1049, loss = 0.02880811\n",
            "Iteration 1050, loss = 0.02878665\n",
            "Iteration 1051, loss = 0.02871498\n",
            "Iteration 1052, loss = 0.02862039\n",
            "Iteration 1053, loss = 0.02856611\n",
            "Iteration 1054, loss = 0.02860720\n",
            "Iteration 1055, loss = 0.02846601\n",
            "Iteration 1056, loss = 0.02844153\n",
            "Iteration 1057, loss = 0.02836617\n",
            "Iteration 1058, loss = 0.02835212\n",
            "Iteration 1059, loss = 0.02826746\n",
            "Iteration 1060, loss = 0.02825709\n",
            "Iteration 1061, loss = 0.02818200\n",
            "Iteration 1062, loss = 0.02814291\n",
            "Iteration 1063, loss = 0.02821318\n",
            "Iteration 1064, loss = 0.02810409\n",
            "Iteration 1065, loss = 0.02804298\n",
            "Iteration 1066, loss = 0.02797055\n",
            "Iteration 1067, loss = 0.02790884\n",
            "Iteration 1068, loss = 0.02785463\n",
            "Iteration 1069, loss = 0.02782535\n",
            "Iteration 1070, loss = 0.02776410\n",
            "Iteration 1071, loss = 0.02774537\n",
            "Iteration 1072, loss = 0.02765061\n",
            "Iteration 1073, loss = 0.02759390\n",
            "Iteration 1074, loss = 0.02756078\n",
            "Iteration 1075, loss = 0.02750091\n",
            "Iteration 1076, loss = 0.02746563\n",
            "Iteration 1077, loss = 0.02743875\n",
            "Iteration 1078, loss = 0.02740614\n",
            "Iteration 1079, loss = 0.02740036\n",
            "Iteration 1080, loss = 0.02734699\n",
            "Iteration 1081, loss = 0.02725373\n",
            "Iteration 1082, loss = 0.02720739\n",
            "Iteration 1083, loss = 0.02713095\n",
            "Iteration 1084, loss = 0.02710140\n",
            "Iteration 1085, loss = 0.02705420\n",
            "Iteration 1086, loss = 0.02700545\n",
            "Iteration 1087, loss = 0.02698221\n",
            "Iteration 1088, loss = 0.02694133\n",
            "Iteration 1089, loss = 0.02685914\n",
            "Iteration 1090, loss = 0.02684000\n",
            "Iteration 1091, loss = 0.02681315\n",
            "Iteration 1092, loss = 0.02675734\n",
            "Iteration 1093, loss = 0.02669006\n",
            "Iteration 1094, loss = 0.02665052\n",
            "Iteration 1095, loss = 0.02666572\n",
            "Iteration 1096, loss = 0.02658931\n",
            "Iteration 1097, loss = 0.02654913\n",
            "Iteration 1098, loss = 0.02648528\n",
            "Iteration 1099, loss = 0.02645394\n",
            "Iteration 1100, loss = 0.02640196\n",
            "Iteration 1101, loss = 0.02634992\n",
            "Iteration 1102, loss = 0.02632747\n",
            "Iteration 1103, loss = 0.02623347\n",
            "Iteration 1104, loss = 0.02622115\n",
            "Iteration 1105, loss = 0.02621659\n",
            "Iteration 1106, loss = 0.02618437\n",
            "Iteration 1107, loss = 0.02612007\n",
            "Iteration 1108, loss = 0.02609365\n",
            "Iteration 1109, loss = 0.02607261\n",
            "Iteration 1110, loss = 0.02608626\n",
            "Iteration 1111, loss = 0.02596283\n",
            "Iteration 1112, loss = 0.02589823\n",
            "Iteration 1113, loss = 0.02583489\n",
            "Iteration 1114, loss = 0.02579543\n",
            "Iteration 1115, loss = 0.02575651\n",
            "Iteration 1116, loss = 0.02574767\n",
            "Iteration 1117, loss = 0.02568927\n",
            "Iteration 1118, loss = 0.02565971\n",
            "Iteration 1119, loss = 0.02561733\n",
            "Iteration 1120, loss = 0.02558540\n",
            "Iteration 1121, loss = 0.02551933\n",
            "Iteration 1122, loss = 0.02571854\n",
            "Iteration 1123, loss = 0.02546028\n",
            "Iteration 1124, loss = 0.02546560\n",
            "Iteration 1125, loss = 0.02536440\n",
            "Iteration 1126, loss = 0.02533501\n",
            "Iteration 1127, loss = 0.02529356\n",
            "Iteration 1128, loss = 0.02526611\n",
            "Iteration 1129, loss = 0.02533671\n",
            "Iteration 1130, loss = 0.02516427\n",
            "Iteration 1131, loss = 0.02514388\n",
            "Iteration 1132, loss = 0.02511812\n",
            "Iteration 1133, loss = 0.02508365\n",
            "Iteration 1134, loss = 0.02501583\n",
            "Iteration 1135, loss = 0.02496289\n",
            "Iteration 1136, loss = 0.02494384\n",
            "Iteration 1137, loss = 0.02492969\n",
            "Iteration 1138, loss = 0.02486847\n",
            "Iteration 1139, loss = 0.02482093\n",
            "Iteration 1140, loss = 0.02477513\n",
            "Iteration 1141, loss = 0.02475468\n",
            "Iteration 1142, loss = 0.02476091\n",
            "Iteration 1143, loss = 0.02466890\n",
            "Iteration 1144, loss = 0.02464466\n",
            "Iteration 1145, loss = 0.02460894\n",
            "Iteration 1146, loss = 0.02457253\n",
            "Iteration 1147, loss = 0.02452599\n",
            "Iteration 1148, loss = 0.02454093\n",
            "Iteration 1149, loss = 0.02444833\n",
            "Iteration 1150, loss = 0.02442761\n",
            "Iteration 1151, loss = 0.02445630\n",
            "Iteration 1152, loss = 0.02444546\n",
            "Iteration 1153, loss = 0.02436320\n",
            "Iteration 1154, loss = 0.02433616\n",
            "Iteration 1155, loss = 0.02427593\n",
            "Iteration 1156, loss = 0.02421749\n",
            "Iteration 1157, loss = 0.02415830\n",
            "Iteration 1158, loss = 0.02414855\n",
            "Iteration 1159, loss = 0.02408012\n",
            "Iteration 1160, loss = 0.02405482\n",
            "Iteration 1161, loss = 0.02400903\n",
            "Iteration 1162, loss = 0.02401652\n",
            "Iteration 1163, loss = 0.02393298\n",
            "Iteration 1164, loss = 0.02392245\n",
            "Iteration 1165, loss = 0.02393821\n",
            "Iteration 1166, loss = 0.02385450\n",
            "Iteration 1167, loss = 0.02380269\n",
            "Iteration 1168, loss = 0.02379677\n",
            "Iteration 1169, loss = 0.02372983\n",
            "Iteration 1170, loss = 0.02378942\n",
            "Iteration 1171, loss = 0.02371877\n",
            "Iteration 1172, loss = 0.02361998\n",
            "Iteration 1173, loss = 0.02355747\n",
            "Iteration 1174, loss = 0.02372814\n",
            "Iteration 1175, loss = 0.02360371\n",
            "Iteration 1176, loss = 0.02346207\n",
            "Iteration 1177, loss = 0.02344029\n",
            "Iteration 1178, loss = 0.02346197\n",
            "Iteration 1179, loss = 0.02344859\n",
            "Iteration 1180, loss = 0.02337460\n",
            "Iteration 1181, loss = 0.02333966\n",
            "Iteration 1182, loss = 0.02338521\n",
            "Iteration 1183, loss = 0.02325054\n",
            "Iteration 1184, loss = 0.02327411\n",
            "Iteration 1185, loss = 0.02319342\n",
            "Iteration 1186, loss = 0.02314481\n",
            "Iteration 1187, loss = 0.02316153\n",
            "Iteration 1188, loss = 0.02311025\n",
            "Iteration 1189, loss = 0.02309666\n",
            "Iteration 1190, loss = 0.02302966\n",
            "Iteration 1191, loss = 0.02301450\n",
            "Iteration 1192, loss = 0.02297230\n",
            "Iteration 1193, loss = 0.02302855\n",
            "Iteration 1194, loss = 0.02292698\n",
            "Iteration 1195, loss = 0.02288551\n",
            "Iteration 1196, loss = 0.02281328\n",
            "Iteration 1197, loss = 0.02280597\n",
            "Iteration 1198, loss = 0.02276210\n",
            "Iteration 1199, loss = 0.02276505\n",
            "Iteration 1200, loss = 0.02275584\n",
            "Iteration 1201, loss = 0.02269043\n",
            "Iteration 1202, loss = 0.02264383\n",
            "Iteration 1203, loss = 0.02263519\n",
            "Iteration 1204, loss = 0.02262508\n",
            "Iteration 1205, loss = 0.02253262\n",
            "Iteration 1206, loss = 0.02252527\n",
            "Iteration 1207, loss = 0.02251625\n",
            "Iteration 1208, loss = 0.02244197\n",
            "Iteration 1209, loss = 0.02242311\n",
            "Iteration 1210, loss = 0.02240874\n",
            "Iteration 1211, loss = 0.02239153\n",
            "Iteration 1212, loss = 0.02246617\n",
            "Iteration 1213, loss = 0.02230671\n",
            "Iteration 1214, loss = 0.02228058\n",
            "Iteration 1215, loss = 0.02222679\n",
            "Iteration 1216, loss = 0.02220707\n",
            "Iteration 1217, loss = 0.02217636\n",
            "Iteration 1218, loss = 0.02215161\n",
            "Iteration 1219, loss = 0.02213033\n",
            "Iteration 1220, loss = 0.02210061\n",
            "Iteration 1221, loss = 0.02210832\n",
            "Iteration 1222, loss = 0.02202895\n",
            "Iteration 1223, loss = 0.02200728\n",
            "Iteration 1224, loss = 0.02200497\n",
            "Iteration 1225, loss = 0.02193849\n",
            "Iteration 1226, loss = 0.02191673\n",
            "Iteration 1227, loss = 0.02187354\n",
            "Iteration 1228, loss = 0.02182171\n",
            "Iteration 1229, loss = 0.02182239\n",
            "Iteration 1230, loss = 0.02178121\n",
            "Iteration 1231, loss = 0.02176221\n",
            "Iteration 1232, loss = 0.02170957\n",
            "Iteration 1233, loss = 0.02170161\n",
            "Iteration 1234, loss = 0.02167217\n",
            "Iteration 1235, loss = 0.02163544\n",
            "Iteration 1236, loss = 0.02159041\n",
            "Iteration 1237, loss = 0.02160688\n",
            "Iteration 1238, loss = 0.02155233\n",
            "Iteration 1239, loss = 0.02150409\n",
            "Iteration 1240, loss = 0.02147870\n",
            "Iteration 1241, loss = 0.02148199\n",
            "Iteration 1242, loss = 0.02142599\n",
            "Iteration 1243, loss = 0.02142158\n",
            "Iteration 1244, loss = 0.02136809\n",
            "Iteration 1245, loss = 0.02134329\n",
            "Iteration 1246, loss = 0.02126214\n",
            "Iteration 1247, loss = 0.02126569\n",
            "Iteration 1248, loss = 0.02126976\n",
            "Iteration 1249, loss = 0.02125434\n",
            "Iteration 1250, loss = 0.02124813\n",
            "Iteration 1251, loss = 0.02115761\n",
            "Iteration 1252, loss = 0.02113287\n",
            "Iteration 1253, loss = 0.02110509\n",
            "Iteration 1254, loss = 0.02106288\n",
            "Iteration 1255, loss = 0.02104162\n",
            "Iteration 1256, loss = 0.02099848\n",
            "Iteration 1257, loss = 0.02098409\n",
            "Iteration 1258, loss = 0.02099324\n",
            "Iteration 1259, loss = 0.02090486\n",
            "Iteration 1260, loss = 0.02089556\n",
            "Iteration 1261, loss = 0.02085108\n",
            "Iteration 1262, loss = 0.02089596\n",
            "Iteration 1263, loss = 0.02078014\n",
            "Iteration 1264, loss = 0.02078177\n",
            "Iteration 1265, loss = 0.02074549\n",
            "Iteration 1266, loss = 0.02069369\n",
            "Iteration 1267, loss = 0.02069541\n",
            "Iteration 1268, loss = 0.02065100\n",
            "Iteration 1269, loss = 0.02065252\n",
            "Iteration 1270, loss = 0.02068347\n",
            "Iteration 1271, loss = 0.02056326\n",
            "Iteration 1272, loss = 0.02051431\n",
            "Iteration 1273, loss = 0.02052404\n",
            "Iteration 1274, loss = 0.02051230\n",
            "Iteration 1275, loss = 0.02044798\n",
            "Iteration 1276, loss = 0.02041629\n",
            "Iteration 1277, loss = 0.02040152\n",
            "Iteration 1278, loss = 0.02040778\n",
            "Iteration 1279, loss = 0.02034651\n",
            "Iteration 1280, loss = 0.02032929\n",
            "Iteration 1281, loss = 0.02026519\n",
            "Iteration 1282, loss = 0.02027109\n",
            "Iteration 1283, loss = 0.02025310\n",
            "Iteration 1284, loss = 0.02017604\n",
            "Iteration 1285, loss = 0.02018132\n",
            "Iteration 1286, loss = 0.02012885\n",
            "Iteration 1287, loss = 0.02016883\n",
            "Iteration 1288, loss = 0.02009215\n",
            "Iteration 1289, loss = 0.02007261\n",
            "Iteration 1290, loss = 0.02015460\n",
            "Iteration 1291, loss = 0.02013439\n",
            "Iteration 1292, loss = 0.01999716\n",
            "Iteration 1293, loss = 0.02001432\n",
            "Iteration 1294, loss = 0.01994099\n",
            "Iteration 1295, loss = 0.01987778\n",
            "Iteration 1296, loss = 0.01993035\n",
            "Iteration 1297, loss = 0.01983070\n",
            "Iteration 1298, loss = 0.01984579\n",
            "Iteration 1299, loss = 0.01983268\n",
            "Iteration 1300, loss = 0.01975541\n",
            "Iteration 1301, loss = 0.01971344\n",
            "Iteration 1302, loss = 0.01969502\n",
            "Iteration 1303, loss = 0.01976978\n",
            "Iteration 1304, loss = 0.01966398\n",
            "Iteration 1305, loss = 0.01962412\n",
            "Iteration 1306, loss = 0.01960039\n",
            "Iteration 1307, loss = 0.01956486\n",
            "Iteration 1308, loss = 0.01956397\n",
            "Iteration 1309, loss = 0.01954197\n",
            "Iteration 1310, loss = 0.01957838\n",
            "Iteration 1311, loss = 0.01948914\n",
            "Iteration 1312, loss = 0.01942755\n",
            "Iteration 1313, loss = 0.01949900\n",
            "Iteration 1314, loss = 0.01938913\n",
            "Iteration 1315, loss = 0.01933403\n",
            "Iteration 1316, loss = 0.01937065\n",
            "Iteration 1317, loss = 0.01934759\n",
            "Iteration 1318, loss = 0.01928634\n",
            "Iteration 1319, loss = 0.01935884\n",
            "Iteration 1320, loss = 0.01923410\n",
            "Iteration 1321, loss = 0.01919977\n",
            "Iteration 1322, loss = 0.01917972\n",
            "Iteration 1323, loss = 0.01915957\n",
            "Iteration 1324, loss = 0.01912170\n",
            "Iteration 1325, loss = 0.01915804\n",
            "Iteration 1326, loss = 0.01910984\n",
            "Iteration 1327, loss = 0.01904834\n",
            "Iteration 1328, loss = 0.01906295\n",
            "Iteration 1329, loss = 0.01903317\n",
            "Iteration 1330, loss = 0.01897392\n",
            "Iteration 1331, loss = 0.01895568\n",
            "Iteration 1332, loss = 0.01898757\n",
            "Iteration 1333, loss = 0.01896381\n",
            "Iteration 1334, loss = 0.01894195\n",
            "Iteration 1335, loss = 0.01889401\n",
            "Iteration 1336, loss = 0.01887665\n",
            "Iteration 1337, loss = 0.01881494\n",
            "Iteration 1338, loss = 0.01880074\n",
            "Iteration 1339, loss = 0.01879209\n",
            "Iteration 1340, loss = 0.01873887\n",
            "Iteration 1341, loss = 0.01871800\n",
            "Iteration 1342, loss = 0.01871996\n",
            "Iteration 1343, loss = 0.01875840\n",
            "Iteration 1344, loss = 0.01867400\n",
            "Iteration 1345, loss = 0.01862430\n",
            "Iteration 1346, loss = 0.01862652\n",
            "Iteration 1347, loss = 0.01858940\n",
            "Iteration 1348, loss = 0.01855810\n",
            "Iteration 1349, loss = 0.01855273\n",
            "Iteration 1350, loss = 0.01851935\n",
            "Iteration 1351, loss = 0.01852059\n",
            "Iteration 1352, loss = 0.01847142\n",
            "Iteration 1353, loss = 0.01846419\n",
            "Iteration 1354, loss = 0.01844366\n",
            "Iteration 1355, loss = 0.01836635\n",
            "Iteration 1356, loss = 0.01835647\n",
            "Iteration 1357, loss = 0.01836561\n",
            "Iteration 1358, loss = 0.01830389\n",
            "Iteration 1359, loss = 0.01831091\n",
            "Iteration 1360, loss = 0.01827005\n",
            "Iteration 1361, loss = 0.01823771\n",
            "Iteration 1362, loss = 0.01821122\n",
            "Iteration 1363, loss = 0.01822139\n",
            "Iteration 1364, loss = 0.01815773\n",
            "Iteration 1365, loss = 0.01818740\n",
            "Iteration 1366, loss = 0.01814412\n",
            "Iteration 1367, loss = 0.01810672\n",
            "Iteration 1368, loss = 0.01812896\n",
            "Iteration 1369, loss = 0.01805492\n",
            "Iteration 1370, loss = 0.01806413\n",
            "Iteration 1371, loss = 0.01802005\n",
            "Iteration 1372, loss = 0.01799816\n",
            "Iteration 1373, loss = 0.01795810\n",
            "Iteration 1374, loss = 0.01795040\n",
            "Iteration 1375, loss = 0.01793239\n",
            "Iteration 1376, loss = 0.01790310\n",
            "Iteration 1377, loss = 0.01788399\n",
            "Iteration 1378, loss = 0.01788716\n",
            "Iteration 1379, loss = 0.01781744\n",
            "Iteration 1380, loss = 0.01779656\n",
            "Iteration 1381, loss = 0.01778996\n",
            "Iteration 1382, loss = 0.01776837\n",
            "Iteration 1383, loss = 0.01780738\n",
            "Iteration 1384, loss = 0.01779217\n",
            "Iteration 1385, loss = 0.01770624\n",
            "Iteration 1386, loss = 0.01771192\n",
            "Iteration 1387, loss = 0.01763050\n",
            "Iteration 1388, loss = 0.01761447\n",
            "Iteration 1389, loss = 0.01764209\n",
            "Iteration 1390, loss = 0.01759789\n",
            "Iteration 1391, loss = 0.01760807\n",
            "Iteration 1392, loss = 0.01751463\n",
            "Iteration 1393, loss = 0.01748706\n",
            "Iteration 1394, loss = 0.01753497\n",
            "Iteration 1395, loss = 0.01750010\n",
            "Iteration 1396, loss = 0.01747512\n",
            "Iteration 1397, loss = 0.01749049\n",
            "Iteration 1398, loss = 0.01744019\n",
            "Iteration 1399, loss = 0.01741420\n",
            "Iteration 1400, loss = 0.01738477\n",
            "Iteration 1401, loss = 0.01734044\n",
            "Iteration 1402, loss = 0.01731585\n",
            "Iteration 1403, loss = 0.01732304\n",
            "Iteration 1404, loss = 0.01729560\n",
            "Iteration 1405, loss = 0.01730546\n",
            "Iteration 1406, loss = 0.01725391\n",
            "Iteration 1407, loss = 0.01740328\n",
            "Iteration 1408, loss = 0.01721262\n",
            "Iteration 1409, loss = 0.01722257\n",
            "Iteration 1410, loss = 0.01716244\n",
            "Iteration 1411, loss = 0.01713894\n",
            "Iteration 1412, loss = 0.01715522\n",
            "Iteration 1413, loss = 0.01710003\n",
            "Iteration 1414, loss = 0.01705725\n",
            "Iteration 1415, loss = 0.01705377\n",
            "Iteration 1416, loss = 0.01703925\n",
            "Iteration 1417, loss = 0.01702277\n",
            "Iteration 1418, loss = 0.01698953\n",
            "Iteration 1419, loss = 0.01699248\n",
            "Iteration 1420, loss = 0.01694593\n",
            "Iteration 1421, loss = 0.01694824\n",
            "Iteration 1422, loss = 0.01690673\n",
            "Iteration 1423, loss = 0.01690232\n",
            "Iteration 1424, loss = 0.01690125\n",
            "Iteration 1425, loss = 0.01683252\n",
            "Iteration 1426, loss = 0.01688606\n",
            "Iteration 1427, loss = 0.01680947\n",
            "Iteration 1428, loss = 0.01679089\n",
            "Iteration 1429, loss = 0.01676013\n",
            "Iteration 1430, loss = 0.01677211\n",
            "Iteration 1431, loss = 0.01670938\n",
            "Iteration 1432, loss = 0.01669672\n",
            "Iteration 1433, loss = 0.01676937\n",
            "Iteration 1434, loss = 0.01663570\n",
            "Iteration 1435, loss = 0.01663339\n",
            "Iteration 1436, loss = 0.01665104\n",
            "Iteration 1437, loss = 0.01660897\n",
            "Iteration 1438, loss = 0.01659490\n",
            "Iteration 1439, loss = 0.01658127\n",
            "Iteration 1440, loss = 0.01655797\n",
            "Iteration 1441, loss = 0.01653985\n",
            "Iteration 1442, loss = 0.01651289\n",
            "Iteration 1443, loss = 0.01648647\n",
            "Iteration 1444, loss = 0.01647433\n",
            "Iteration 1445, loss = 0.01648109\n",
            "Iteration 1446, loss = 0.01640945\n",
            "Iteration 1447, loss = 0.01641773\n",
            "Iteration 1448, loss = 0.01646478\n",
            "Iteration 1449, loss = 0.01638345\n",
            "Iteration 1450, loss = 0.01636291\n",
            "Iteration 1451, loss = 0.01633428\n",
            "Iteration 1452, loss = 0.01629973\n",
            "Iteration 1453, loss = 0.01633559\n",
            "Iteration 1454, loss = 0.01626989\n",
            "Iteration 1455, loss = 0.01624559\n",
            "Iteration 1456, loss = 0.01622519\n",
            "Iteration 1457, loss = 0.01622572\n",
            "Iteration 1458, loss = 0.01620344\n",
            "Iteration 1459, loss = 0.01619249\n",
            "Iteration 1460, loss = 0.01616342\n",
            "Iteration 1461, loss = 0.01619192\n",
            "Iteration 1462, loss = 0.01614950\n",
            "Iteration 1463, loss = 0.01617497\n",
            "Iteration 1464, loss = 0.01613933\n",
            "Iteration 1465, loss = 0.01609423\n",
            "Iteration 1466, loss = 0.01605103\n",
            "Iteration 1467, loss = 0.01601593\n",
            "Iteration 1468, loss = 0.01614524\n",
            "Iteration 1469, loss = 0.01599441\n",
            "Iteration 1470, loss = 0.01596568\n",
            "Iteration 1471, loss = 0.01596378\n",
            "Iteration 1472, loss = 0.01600386\n",
            "Iteration 1473, loss = 0.01591535\n",
            "Iteration 1474, loss = 0.01598166\n",
            "Iteration 1475, loss = 0.01594385\n",
            "Iteration 1476, loss = 0.01589443\n",
            "Iteration 1477, loss = 0.01590819\n",
            "Iteration 1478, loss = 0.01581420\n",
            "Iteration 1479, loss = 0.01577719\n",
            "Iteration 1480, loss = 0.01581291\n",
            "Iteration 1481, loss = 0.01576884\n",
            "Iteration 1482, loss = 0.01574732\n",
            "Iteration 1483, loss = 0.01574580\n",
            "Iteration 1484, loss = 0.01572910\n",
            "Iteration 1485, loss = 0.01568405\n",
            "Iteration 1486, loss = 0.01568635\n",
            "Iteration 1487, loss = 0.01564944\n",
            "Iteration 1488, loss = 0.01561940\n",
            "Iteration 1489, loss = 0.01560355\n",
            "Iteration 1490, loss = 0.01561085\n",
            "Iteration 1491, loss = 0.01560953\n",
            "Iteration 1492, loss = 0.01553385\n",
            "Iteration 1493, loss = 0.01555444\n",
            "Iteration 1494, loss = 0.01553663\n",
            "Iteration 1495, loss = 0.01552295\n",
            "Iteration 1496, loss = 0.01548263\n",
            "Iteration 1497, loss = 0.01548906\n",
            "Iteration 1498, loss = 0.01546137\n",
            "Iteration 1499, loss = 0.01541970\n",
            "Iteration 1500, loss = 0.01539562\n",
            "Iteration 1501, loss = 0.01538464\n",
            "Iteration 1502, loss = 0.01537487\n",
            "Iteration 1503, loss = 0.01538151\n",
            "Iteration 1504, loss = 0.01536094\n",
            "Iteration 1505, loss = 0.01533308\n",
            "Iteration 1506, loss = 0.01533982\n",
            "Iteration 1507, loss = 0.01529621\n",
            "Iteration 1508, loss = 0.01527311\n",
            "Iteration 1509, loss = 0.01525675\n",
            "Iteration 1510, loss = 0.01524859\n",
            "Iteration 1511, loss = 0.01521795\n",
            "Iteration 1512, loss = 0.01523065\n",
            "Iteration 1513, loss = 0.01519627\n",
            "Iteration 1514, loss = 0.01522160\n",
            "Iteration 1515, loss = 0.01520164\n",
            "Iteration 1516, loss = 0.01514704\n",
            "Iteration 1517, loss = 0.01510627\n",
            "Iteration 1518, loss = 0.01513729\n",
            "Iteration 1519, loss = 0.01512278\n",
            "Iteration 1520, loss = 0.01506057\n",
            "Iteration 1521, loss = 0.01509026\n",
            "Iteration 1522, loss = 0.01512179\n",
            "Iteration 1523, loss = 0.01503070\n",
            "Iteration 1524, loss = 0.01501137\n",
            "Iteration 1525, loss = 0.01498514\n",
            "Iteration 1526, loss = 0.01498243\n",
            "Iteration 1527, loss = 0.01495627\n",
            "Iteration 1528, loss = 0.01494730\n",
            "Iteration 1529, loss = 0.01495148\n",
            "Iteration 1530, loss = 0.01491161\n",
            "Iteration 1531, loss = 0.01488971\n",
            "Iteration 1532, loss = 0.01485146\n",
            "Iteration 1533, loss = 0.01487922\n",
            "Iteration 1534, loss = 0.01482126\n",
            "Iteration 1535, loss = 0.01485671\n",
            "Iteration 1536, loss = 0.01481509\n",
            "Iteration 1537, loss = 0.01477381\n",
            "Iteration 1538, loss = 0.01476468\n",
            "Iteration 1539, loss = 0.01477835\n",
            "Iteration 1540, loss = 0.01472799\n",
            "Iteration 1541, loss = 0.01472132\n",
            "Iteration 1542, loss = 0.01474047\n",
            "Iteration 1543, loss = 0.01468441\n",
            "Iteration 1544, loss = 0.01471535\n",
            "Iteration 1545, loss = 0.01465794\n",
            "Iteration 1546, loss = 0.01474643\n",
            "Iteration 1547, loss = 0.01468579\n",
            "Iteration 1548, loss = 0.01460603\n",
            "Iteration 1549, loss = 0.01460294\n",
            "Iteration 1550, loss = 0.01456527\n",
            "Iteration 1551, loss = 0.01459439\n",
            "Iteration 1552, loss = 0.01456236\n",
            "Iteration 1553, loss = 0.01454544\n",
            "Iteration 1554, loss = 0.01450589\n",
            "Iteration 1555, loss = 0.01451516\n",
            "Iteration 1556, loss = 0.01445698\n",
            "Iteration 1557, loss = 0.01447050\n",
            "Iteration 1558, loss = 0.01448473\n",
            "Iteration 1559, loss = 0.01446361\n",
            "Iteration 1560, loss = 0.01449215\n",
            "Iteration 1561, loss = 0.01441757\n",
            "Iteration 1562, loss = 0.01445308\n",
            "Iteration 1563, loss = 0.01436496\n",
            "Iteration 1564, loss = 0.01435327\n",
            "Iteration 1565, loss = 0.01434950\n",
            "Iteration 1566, loss = 0.01432675\n",
            "Iteration 1567, loss = 0.01433361\n",
            "Iteration 1568, loss = 0.01427049\n",
            "Iteration 1569, loss = 0.01438151\n",
            "Iteration 1570, loss = 0.01430969\n",
            "Iteration 1571, loss = 0.01429852\n",
            "Iteration 1572, loss = 0.01430746\n",
            "Iteration 1573, loss = 0.01421717\n",
            "Iteration 1574, loss = 0.01426400\n",
            "Iteration 1575, loss = 0.01423319\n",
            "Iteration 1576, loss = 0.01419656\n",
            "Iteration 1577, loss = 0.01415932\n",
            "Iteration 1578, loss = 0.01416514\n",
            "Iteration 1579, loss = 0.01415968\n",
            "Iteration 1580, loss = 0.01410402\n",
            "Iteration 1581, loss = 0.01408338\n",
            "Iteration 1582, loss = 0.01405566\n",
            "Iteration 1583, loss = 0.01406579\n",
            "Iteration 1584, loss = 0.01408685\n",
            "Iteration 1585, loss = 0.01402562\n",
            "Iteration 1586, loss = 0.01412813\n",
            "Iteration 1587, loss = 0.01402765\n",
            "Iteration 1588, loss = 0.01398816\n",
            "Iteration 1589, loss = 0.01399031\n",
            "Iteration 1590, loss = 0.01398275\n",
            "Iteration 1591, loss = 0.01393528\n",
            "Iteration 1592, loss = 0.01389856\n",
            "Iteration 1593, loss = 0.01391795\n",
            "Iteration 1594, loss = 0.01389129\n",
            "Iteration 1595, loss = 0.01388330\n",
            "Iteration 1596, loss = 0.01388617\n",
            "Iteration 1597, loss = 0.01387026\n",
            "Iteration 1598, loss = 0.01382063\n",
            "Iteration 1599, loss = 0.01383587\n",
            "Iteration 1600, loss = 0.01378820\n",
            "Iteration 1601, loss = 0.01380468\n",
            "Iteration 1602, loss = 0.01375923\n",
            "Iteration 1603, loss = 0.01375874\n",
            "Iteration 1604, loss = 0.01374204\n",
            "Iteration 1605, loss = 0.01375799\n",
            "Iteration 1606, loss = 0.01371005\n",
            "Iteration 1607, loss = 0.01370624\n",
            "Iteration 1608, loss = 0.01369676\n",
            "Iteration 1609, loss = 0.01368927\n",
            "Iteration 1610, loss = 0.01367097\n",
            "Iteration 1611, loss = 0.01364642\n",
            "Iteration 1612, loss = 0.01364422\n",
            "Iteration 1613, loss = 0.01362764\n",
            "Iteration 1614, loss = 0.01365315\n",
            "Iteration 1615, loss = 0.01359784\n",
            "Iteration 1616, loss = 0.01358547\n",
            "Iteration 1617, loss = 0.01362359\n",
            "Iteration 1618, loss = 0.01355790\n",
            "Iteration 1619, loss = 0.01353585\n",
            "Iteration 1620, loss = 0.01352449\n",
            "Iteration 1621, loss = 0.01349706\n",
            "Iteration 1622, loss = 0.01348459\n",
            "Iteration 1623, loss = 0.01348182\n",
            "Iteration 1624, loss = 0.01345492\n",
            "Iteration 1625, loss = 0.01345781\n",
            "Iteration 1626, loss = 0.01344261\n",
            "Iteration 1627, loss = 0.01342766\n",
            "Iteration 1628, loss = 0.01340853\n",
            "Iteration 1629, loss = 0.01339866\n",
            "Iteration 1630, loss = 0.01335029\n",
            "Iteration 1631, loss = 0.01335317\n",
            "Iteration 1632, loss = 0.01336157\n",
            "Iteration 1633, loss = 0.01333867\n",
            "Iteration 1634, loss = 0.01329292\n",
            "Iteration 1635, loss = 0.01330705\n",
            "Iteration 1636, loss = 0.01330568\n",
            "Iteration 1637, loss = 0.01330288\n",
            "Iteration 1638, loss = 0.01328199\n",
            "Iteration 1639, loss = 0.01324229\n",
            "Iteration 1640, loss = 0.01328324\n",
            "Iteration 1641, loss = 0.01328261\n",
            "Iteration 1642, loss = 0.01326363\n",
            "Iteration 1643, loss = 0.01320146\n",
            "Iteration 1644, loss = 0.01319734\n",
            "Iteration 1645, loss = 0.01326756\n",
            "Iteration 1646, loss = 0.01315978\n",
            "Iteration 1647, loss = 0.01316417\n",
            "Iteration 1648, loss = 0.01312618\n",
            "Iteration 1649, loss = 0.01312702\n",
            "Iteration 1650, loss = 0.01315516\n",
            "Iteration 1651, loss = 0.01310108\n",
            "Iteration 1652, loss = 0.01307255\n",
            "Iteration 1653, loss = 0.01305839\n",
            "Iteration 1654, loss = 0.01306636\n",
            "Iteration 1655, loss = 0.01303767\n",
            "Iteration 1656, loss = 0.01301686\n",
            "Iteration 1657, loss = 0.01303786\n",
            "Iteration 1658, loss = 0.01300553\n",
            "Iteration 1659, loss = 0.01300178\n",
            "Iteration 1660, loss = 0.01295998\n",
            "Iteration 1661, loss = 0.01295040\n",
            "Iteration 1662, loss = 0.01293769\n",
            "Iteration 1663, loss = 0.01293483\n",
            "Iteration 1664, loss = 0.01296072\n",
            "Iteration 1665, loss = 0.01294778\n",
            "Iteration 1666, loss = 0.01290987\n",
            "Iteration 1667, loss = 0.01286840\n",
            "Iteration 1668, loss = 0.01286367\n",
            "Iteration 1669, loss = 0.01282440\n",
            "Iteration 1670, loss = 0.01284160\n",
            "Iteration 1671, loss = 0.01283157\n",
            "Iteration 1672, loss = 0.01281082\n",
            "Iteration 1673, loss = 0.01282364\n",
            "Iteration 1674, loss = 0.01292188\n",
            "Iteration 1675, loss = 0.01281529\n",
            "Iteration 1676, loss = 0.01277340\n",
            "Iteration 1677, loss = 0.01274374\n",
            "Iteration 1678, loss = 0.01272656\n",
            "Iteration 1679, loss = 0.01272359\n",
            "Iteration 1680, loss = 0.01273981\n",
            "Iteration 1681, loss = 0.01269435\n",
            "Iteration 1682, loss = 0.01270710\n",
            "Iteration 1683, loss = 0.01270435\n",
            "Iteration 1684, loss = 0.01263920\n",
            "Iteration 1685, loss = 0.01265276\n",
            "Iteration 1686, loss = 0.01267233\n",
            "Iteration 1687, loss = 0.01264522\n",
            "Iteration 1688, loss = 0.01259579\n",
            "Iteration 1689, loss = 0.01261260\n",
            "Iteration 1690, loss = 0.01260550\n",
            "Iteration 1691, loss = 0.01255237\n",
            "Iteration 1692, loss = 0.01256036\n",
            "Iteration 1693, loss = 0.01259793\n",
            "Iteration 1694, loss = 0.01258534\n",
            "Iteration 1695, loss = 0.01251065\n",
            "Iteration 1696, loss = 0.01253867\n",
            "Iteration 1697, loss = 0.01249648\n",
            "Iteration 1698, loss = 0.01246696\n",
            "Iteration 1699, loss = 0.01246289\n",
            "Iteration 1700, loss = 0.01246194\n",
            "Iteration 1701, loss = 0.01248668\n",
            "Iteration 1702, loss = 0.01244710\n",
            "Iteration 1703, loss = 0.01241298\n",
            "Iteration 1704, loss = 0.01243090\n",
            "Iteration 1705, loss = 0.01239204\n",
            "Iteration 1706, loss = 0.01239855\n",
            "Iteration 1707, loss = 0.01239838\n",
            "Iteration 1708, loss = 0.01241405\n",
            "Iteration 1709, loss = 0.01235729\n",
            "Iteration 1710, loss = 0.01233273\n",
            "Iteration 1711, loss = 0.01234737\n",
            "Iteration 1712, loss = 0.01231005\n",
            "Iteration 1713, loss = 0.01233145\n",
            "Iteration 1714, loss = 0.01235622\n",
            "Iteration 1715, loss = 0.01228044\n",
            "Iteration 1716, loss = 0.01226505\n",
            "Iteration 1717, loss = 0.01226707\n",
            "Iteration 1718, loss = 0.01225427\n",
            "Iteration 1719, loss = 0.01222496\n",
            "Iteration 1720, loss = 0.01220875\n",
            "Iteration 1721, loss = 0.01222393\n",
            "Iteration 1722, loss = 0.01217369\n",
            "Iteration 1723, loss = 0.01221315\n",
            "Iteration 1724, loss = 0.01220647\n",
            "Iteration 1725, loss = 0.01216504\n",
            "Iteration 1726, loss = 0.01214986\n",
            "Iteration 1727, loss = 0.01222458\n",
            "Iteration 1728, loss = 0.01209869\n",
            "Iteration 1729, loss = 0.01211064\n",
            "Iteration 1730, loss = 0.01212283\n",
            "Iteration 1731, loss = 0.01211498\n",
            "Iteration 1732, loss = 0.01209043\n",
            "Iteration 1733, loss = 0.01217691\n",
            "Iteration 1734, loss = 0.01211250\n",
            "Iteration 1735, loss = 0.01203890\n",
            "Iteration 1736, loss = 0.01206714\n",
            "Iteration 1737, loss = 0.01206204\n",
            "Iteration 1738, loss = 0.01207053\n",
            "Iteration 1739, loss = 0.01204245\n",
            "Iteration 1740, loss = 0.01197791\n",
            "Iteration 1741, loss = 0.01197516\n",
            "Iteration 1742, loss = 0.01196122\n",
            "Iteration 1743, loss = 0.01201004\n",
            "Iteration 1744, loss = 0.01194181\n",
            "Iteration 1745, loss = 0.01192233\n",
            "Iteration 1746, loss = 0.01202986\n",
            "Iteration 1747, loss = 0.01191834\n",
            "Iteration 1748, loss = 0.01192730\n",
            "Iteration 1749, loss = 0.01189504\n",
            "Iteration 1750, loss = 0.01189485\n",
            "Iteration 1751, loss = 0.01191181\n",
            "Iteration 1752, loss = 0.01183194\n",
            "Iteration 1753, loss = 0.01183323\n",
            "Iteration 1754, loss = 0.01188840\n",
            "Iteration 1755, loss = 0.01186348\n",
            "Iteration 1756, loss = 0.01182862\n",
            "Iteration 1757, loss = 0.01179484\n",
            "Iteration 1758, loss = 0.01178022\n",
            "Iteration 1759, loss = 0.01181404\n",
            "Iteration 1760, loss = 0.01177496\n",
            "Iteration 1761, loss = 0.01175631\n",
            "Iteration 1762, loss = 0.01171374\n",
            "Iteration 1763, loss = 0.01171430\n",
            "Iteration 1764, loss = 0.01173792\n",
            "Iteration 1765, loss = 0.01167765\n",
            "Iteration 1766, loss = 0.01179050\n",
            "Iteration 1767, loss = 0.01170274\n",
            "Iteration 1768, loss = 0.01165837\n",
            "Iteration 1769, loss = 0.01166566\n",
            "Iteration 1770, loss = 0.01166633\n",
            "Iteration 1771, loss = 0.01162348\n",
            "Iteration 1772, loss = 0.01164262\n",
            "Iteration 1773, loss = 0.01161471\n",
            "Iteration 1774, loss = 0.01159501\n",
            "Iteration 1775, loss = 0.01160609\n",
            "Iteration 1776, loss = 0.01155961\n",
            "Iteration 1777, loss = 0.01154584\n",
            "Iteration 1778, loss = 0.01156118\n",
            "Iteration 1779, loss = 0.01153987\n",
            "Iteration 1780, loss = 0.01156674\n",
            "Iteration 1781, loss = 0.01153724\n",
            "Iteration 1782, loss = 0.01152547\n",
            "Iteration 1783, loss = 0.01147645\n",
            "Iteration 1784, loss = 0.01151021\n",
            "Iteration 1785, loss = 0.01148948\n",
            "Iteration 1786, loss = 0.01146080\n",
            "Iteration 1787, loss = 0.01143436\n",
            "Iteration 1788, loss = 0.01146487\n",
            "Iteration 1789, loss = 0.01143089\n",
            "Iteration 1790, loss = 0.01147931\n",
            "Iteration 1791, loss = 0.01138925\n",
            "Iteration 1792, loss = 0.01146101\n",
            "Iteration 1793, loss = 0.01146589\n",
            "Iteration 1794, loss = 0.01143253\n",
            "Iteration 1795, loss = 0.01136587\n",
            "Iteration 1796, loss = 0.01134204\n",
            "Iteration 1797, loss = 0.01143669\n",
            "Iteration 1798, loss = 0.01137141\n",
            "Iteration 1799, loss = 0.01134005\n",
            "Iteration 1800, loss = 0.01130948\n",
            "Iteration 1801, loss = 0.01130043\n",
            "Iteration 1802, loss = 0.01128875\n",
            "Iteration 1803, loss = 0.01128514\n",
            "Iteration 1804, loss = 0.01128147\n",
            "Iteration 1805, loss = 0.01124763\n",
            "Iteration 1806, loss = 0.01127828\n",
            "Iteration 1807, loss = 0.01126736\n",
            "Iteration 1808, loss = 0.01122430\n",
            "Iteration 1809, loss = 0.01123552\n",
            "Iteration 1810, loss = 0.01122595\n",
            "Iteration 1811, loss = 0.01117966\n",
            "Iteration 1812, loss = 0.01118259\n",
            "Iteration 1813, loss = 0.01120950\n",
            "Iteration 1814, loss = 0.01117466\n",
            "Iteration 1815, loss = 0.01114857\n",
            "Iteration 1816, loss = 0.01113938\n",
            "Iteration 1817, loss = 0.01113484\n",
            "Iteration 1818, loss = 0.01112522\n",
            "Iteration 1819, loss = 0.01111585\n",
            "Iteration 1820, loss = 0.01109893\n",
            "Iteration 1821, loss = 0.01112379\n",
            "Iteration 1822, loss = 0.01117446\n",
            "Iteration 1823, loss = 0.01112147\n",
            "Iteration 1824, loss = 0.01111162\n",
            "Iteration 1825, loss = 0.01106719\n",
            "Iteration 1826, loss = 0.01102717\n",
            "Iteration 1827, loss = 0.01105277\n",
            "Iteration 1828, loss = 0.01104224\n",
            "Iteration 1829, loss = 0.01100492\n",
            "Iteration 1830, loss = 0.01101609\n",
            "Iteration 1831, loss = 0.01098088\n",
            "Iteration 1832, loss = 0.01097040\n",
            "Iteration 1833, loss = 0.01096863\n",
            "Iteration 1834, loss = 0.01095345\n",
            "Iteration 1835, loss = 0.01098865\n",
            "Iteration 1836, loss = 0.01096080\n",
            "Iteration 1837, loss = 0.01094110\n",
            "Iteration 1838, loss = 0.01091970\n",
            "Iteration 1839, loss = 0.01090843\n",
            "Iteration 1840, loss = 0.01093086\n",
            "Iteration 1841, loss = 0.01090610\n",
            "Iteration 1842, loss = 0.01088565\n",
            "Iteration 1843, loss = 0.01085712\n",
            "Iteration 1844, loss = 0.01086683\n",
            "Iteration 1845, loss = 0.01087613\n",
            "Iteration 1846, loss = 0.01091809\n",
            "Iteration 1847, loss = 0.01085157\n",
            "Iteration 1848, loss = 0.01082618\n",
            "Iteration 1849, loss = 0.01082593\n",
            "Iteration 1850, loss = 0.01079383\n",
            "Iteration 1851, loss = 0.01078490\n",
            "Iteration 1852, loss = 0.01079638\n",
            "Iteration 1853, loss = 0.01079201\n",
            "Iteration 1854, loss = 0.01074805\n",
            "Iteration 1855, loss = 0.01074615\n",
            "Iteration 1856, loss = 0.01080085\n",
            "Iteration 1857, loss = 0.01074944\n",
            "Iteration 1858, loss = 0.01074910\n",
            "Iteration 1859, loss = 0.01070351\n",
            "Iteration 1860, loss = 0.01070104\n",
            "Iteration 1861, loss = 0.01073397\n",
            "Iteration 1862, loss = 0.01067879\n",
            "Iteration 1863, loss = 0.01070850\n",
            "Iteration 1864, loss = 0.01066289\n",
            "Iteration 1865, loss = 0.01068136\n",
            "Iteration 1866, loss = 0.01065926\n",
            "Iteration 1867, loss = 0.01062747\n",
            "Iteration 1868, loss = 0.01065599\n",
            "Iteration 1869, loss = 0.01065246\n",
            "Iteration 1870, loss = 0.01062448\n",
            "Iteration 1871, loss = 0.01060580\n",
            "Iteration 1872, loss = 0.01058271\n",
            "Iteration 1873, loss = 0.01058232\n",
            "Iteration 1874, loss = 0.01064302\n",
            "Iteration 1875, loss = 0.01059413\n",
            "Iteration 1876, loss = 0.01056522\n",
            "Iteration 1877, loss = 0.01056882\n",
            "Iteration 1878, loss = 0.01056035\n",
            "Iteration 1879, loss = 0.01052330\n",
            "Iteration 1880, loss = 0.01049649\n",
            "Iteration 1881, loss = 0.01050446\n",
            "Iteration 1882, loss = 0.01051364\n",
            "Iteration 1883, loss = 0.01047058\n",
            "Iteration 1884, loss = 0.01052170\n",
            "Iteration 1885, loss = 0.01055508\n",
            "Iteration 1886, loss = 0.01044286\n",
            "Iteration 1887, loss = 0.01046592\n",
            "Iteration 1888, loss = 0.01048514\n",
            "Iteration 1889, loss = 0.01041500\n",
            "Iteration 1890, loss = 0.01040493\n",
            "Iteration 1891, loss = 0.01043187\n",
            "Iteration 1892, loss = 0.01038331\n",
            "Iteration 1893, loss = 0.01047978\n",
            "Iteration 1894, loss = 0.01039142\n",
            "Iteration 1895, loss = 0.01043388\n",
            "Iteration 1896, loss = 0.01035718\n",
            "Iteration 1897, loss = 0.01035922\n",
            "Iteration 1898, loss = 0.01034738\n",
            "Iteration 1899, loss = 0.01031285\n",
            "Iteration 1900, loss = 0.01034717\n",
            "Iteration 1901, loss = 0.01032209\n",
            "Iteration 1902, loss = 0.01029021\n",
            "Iteration 1903, loss = 0.01032076\n",
            "Iteration 1904, loss = 0.01038081\n",
            "Iteration 1905, loss = 0.01026003\n",
            "Iteration 1906, loss = 0.01029184\n",
            "Iteration 1907, loss = 0.01026406\n",
            "Iteration 1908, loss = 0.01027549\n",
            "Iteration 1909, loss = 0.01025182\n",
            "Iteration 1910, loss = 0.01027438\n",
            "Iteration 1911, loss = 0.01022320\n",
            "Iteration 1912, loss = 0.01019698\n",
            "Iteration 1913, loss = 0.01019928\n",
            "Iteration 1914, loss = 0.01022547\n",
            "Iteration 1915, loss = 0.01020474\n",
            "Iteration 1916, loss = 0.01016729\n",
            "Iteration 1917, loss = 0.01018304\n",
            "Iteration 1918, loss = 0.01016081\n",
            "Iteration 1919, loss = 0.01019583\n",
            "Iteration 1920, loss = 0.01013840\n",
            "Iteration 1921, loss = 0.01012109\n",
            "Iteration 1922, loss = 0.01011805\n",
            "Iteration 1923, loss = 0.01014097\n",
            "Iteration 1924, loss = 0.01011833\n",
            "Iteration 1925, loss = 0.01010580\n",
            "Iteration 1926, loss = 0.01008938\n",
            "Iteration 1927, loss = 0.01009814\n",
            "Iteration 1928, loss = 0.01006392\n",
            "Iteration 1929, loss = 0.01003476\n",
            "Iteration 1930, loss = 0.01005579\n",
            "Iteration 1931, loss = 0.01003509\n",
            "Iteration 1932, loss = 0.01005830\n",
            "Iteration 1933, loss = 0.01006361\n",
            "Iteration 1934, loss = 0.01004810\n",
            "Iteration 1935, loss = 0.01004897\n",
            "Iteration 1936, loss = 0.01002988\n",
            "Iteration 1937, loss = 0.01001173\n",
            "Iteration 1938, loss = 0.00994826\n",
            "Iteration 1939, loss = 0.01000827\n",
            "Iteration 1940, loss = 0.00999293\n",
            "Iteration 1941, loss = 0.00996916\n",
            "Iteration 1942, loss = 0.00996314\n",
            "Iteration 1943, loss = 0.00991907\n",
            "Iteration 1944, loss = 0.00991349\n",
            "Iteration 1945, loss = 0.00995676\n",
            "Iteration 1946, loss = 0.00991864\n",
            "Iteration 1947, loss = 0.00988375\n",
            "Iteration 1948, loss = 0.00991653\n",
            "Iteration 1949, loss = 0.00989372\n",
            "Iteration 1950, loss = 0.00986083\n",
            "Iteration 1951, loss = 0.00986706\n",
            "Iteration 1952, loss = 0.00986217\n",
            "Iteration 1953, loss = 0.00984090\n",
            "Iteration 1954, loss = 0.00983815\n",
            "Iteration 1955, loss = 0.00983752\n",
            "Iteration 1956, loss = 0.00981570\n",
            "Iteration 1957, loss = 0.00985457\n",
            "Iteration 1958, loss = 0.00983169\n",
            "Iteration 1959, loss = 0.00977296\n",
            "Iteration 1960, loss = 0.00980370\n",
            "Iteration 1961, loss = 0.00977727\n",
            "Iteration 1962, loss = 0.00976918\n",
            "Iteration 1963, loss = 0.00976019\n",
            "Iteration 1964, loss = 0.00974871\n",
            "Iteration 1965, loss = 0.00977516\n",
            "Iteration 1966, loss = 0.00982388\n",
            "Iteration 1967, loss = 0.00971172\n",
            "Iteration 1968, loss = 0.00971711\n",
            "Iteration 1969, loss = 0.00982475\n",
            "Iteration 1970, loss = 0.00969772\n",
            "Iteration 1971, loss = 0.00971669\n",
            "Iteration 1972, loss = 0.00975378\n",
            "Iteration 1973, loss = 0.00968670\n",
            "Iteration 1974, loss = 0.00965950\n",
            "Iteration 1975, loss = 0.00966644\n",
            "Iteration 1976, loss = 0.00968642\n",
            "Iteration 1977, loss = 0.00971555\n",
            "Iteration 1978, loss = 0.00964172\n",
            "Iteration 1979, loss = 0.00963215\n",
            "Iteration 1980, loss = 0.00961769\n",
            "Iteration 1981, loss = 0.00959584\n",
            "Iteration 1982, loss = 0.00965648\n",
            "Iteration 1983, loss = 0.00956781\n",
            "Iteration 1984, loss = 0.00959391\n",
            "Iteration 1985, loss = 0.00965424\n",
            "Iteration 1986, loss = 0.00956467\n",
            "Iteration 1987, loss = 0.00956365\n",
            "Iteration 1988, loss = 0.00959772\n",
            "Iteration 1989, loss = 0.00952702\n",
            "Iteration 1990, loss = 0.00951368\n",
            "Iteration 1991, loss = 0.00951232\n",
            "Iteration 1992, loss = 0.00953775\n",
            "Iteration 1993, loss = 0.00953577\n",
            "Iteration 1994, loss = 0.00950424\n",
            "Iteration 1995, loss = 0.00957283\n",
            "Iteration 1996, loss = 0.00949134\n",
            "Iteration 1997, loss = 0.00946569\n",
            "Iteration 1998, loss = 0.00948245\n",
            "Iteration 1999, loss = 0.00945684\n",
            "Iteration 2000, loss = 0.00944158\n",
            "Iteration 2001, loss = 0.00945960\n",
            "Iteration 2002, loss = 0.00942964\n",
            "Iteration 2003, loss = 0.00941871\n",
            "Iteration 2004, loss = 0.00943092\n",
            "Iteration 2005, loss = 0.00940276\n",
            "Iteration 2006, loss = 0.00942572\n",
            "Iteration 2007, loss = 0.00940550\n",
            "Iteration 2008, loss = 0.00942195\n",
            "Iteration 2009, loss = 0.00936920\n",
            "Iteration 2010, loss = 0.00938905\n",
            "Iteration 2011, loss = 0.00935527\n",
            "Iteration 2012, loss = 0.00935691\n",
            "Iteration 2013, loss = 0.00934763\n",
            "Iteration 2014, loss = 0.00933751\n",
            "Iteration 2015, loss = 0.00934606\n",
            "Iteration 2016, loss = 0.00932843\n",
            "Iteration 2017, loss = 0.00935868\n",
            "Iteration 2018, loss = 0.00929099\n",
            "Iteration 2019, loss = 0.00929826\n",
            "Iteration 2020, loss = 0.00929567\n",
            "Iteration 2021, loss = 0.00930562\n",
            "Iteration 2022, loss = 0.00927586\n",
            "Iteration 2023, loss = 0.00925756\n",
            "Iteration 2024, loss = 0.00924386\n",
            "Iteration 2025, loss = 0.00926524\n",
            "Iteration 2026, loss = 0.00924042\n",
            "Iteration 2027, loss = 0.00922459\n",
            "Iteration 2028, loss = 0.00922362\n",
            "Iteration 2029, loss = 0.00923967\n",
            "Iteration 2030, loss = 0.00925338\n",
            "Iteration 2031, loss = 0.00933221\n",
            "Iteration 2032, loss = 0.00917981\n",
            "Iteration 2033, loss = 0.00919965\n",
            "Iteration 2034, loss = 0.00920962\n",
            "Iteration 2035, loss = 0.00916379\n",
            "Iteration 2036, loss = 0.00916773\n",
            "Iteration 2037, loss = 0.00916149\n",
            "Iteration 2038, loss = 0.00916943\n",
            "Iteration 2039, loss = 0.00920191\n",
            "Iteration 2040, loss = 0.00912419\n",
            "Iteration 2041, loss = 0.00912090\n",
            "Iteration 2042, loss = 0.00912094\n",
            "Iteration 2043, loss = 0.00911916\n",
            "Iteration 2044, loss = 0.00909194\n",
            "Iteration 2045, loss = 0.00906070\n",
            "Iteration 2046, loss = 0.00914309\n",
            "Iteration 2047, loss = 0.00906251\n",
            "Iteration 2048, loss = 0.00910449\n",
            "Iteration 2049, loss = 0.00911643\n",
            "Iteration 2050, loss = 0.00909826\n",
            "Iteration 2051, loss = 0.00909541\n",
            "Iteration 2052, loss = 0.00906974\n",
            "Iteration 2053, loss = 0.00904439\n",
            "Iteration 2054, loss = 0.00907775\n",
            "Iteration 2055, loss = 0.00902593\n",
            "Iteration 2056, loss = 0.00900783\n",
            "Iteration 2057, loss = 0.00901747\n",
            "Iteration 2058, loss = 0.00901141\n",
            "Iteration 2059, loss = 0.00899780\n",
            "Iteration 2060, loss = 0.00911411\n",
            "Iteration 2061, loss = 0.00896145\n",
            "Iteration 2062, loss = 0.00896150\n",
            "Iteration 2063, loss = 0.00895105\n",
            "Iteration 2064, loss = 0.00897313\n",
            "Iteration 2065, loss = 0.00905005\n",
            "Iteration 2066, loss = 0.00896277\n",
            "Iteration 2067, loss = 0.00894120\n",
            "Iteration 2068, loss = 0.00893141\n",
            "Iteration 2069, loss = 0.00898482\n",
            "Iteration 2070, loss = 0.00896453\n",
            "Iteration 2071, loss = 0.00891821\n",
            "Iteration 2072, loss = 0.00887304\n",
            "Iteration 2073, loss = 0.00891514\n",
            "Iteration 2074, loss = 0.00886371\n",
            "Iteration 2075, loss = 0.00886011\n",
            "Iteration 2076, loss = 0.00885777\n",
            "Iteration 2077, loss = 0.00885691\n",
            "Iteration 2078, loss = 0.00889067\n",
            "Iteration 2079, loss = 0.00883586\n",
            "Iteration 2080, loss = 0.00881987\n",
            "Iteration 2081, loss = 0.00879536\n",
            "Iteration 2082, loss = 0.00882182\n",
            "Iteration 2083, loss = 0.00889062\n",
            "Iteration 2084, loss = 0.00878000\n",
            "Iteration 2085, loss = 0.00879484\n",
            "Iteration 2086, loss = 0.00881277\n",
            "Iteration 2087, loss = 0.00877703\n",
            "Iteration 2088, loss = 0.00875273\n",
            "Iteration 2089, loss = 0.00888517\n",
            "Iteration 2090, loss = 0.00874661\n",
            "Iteration 2091, loss = 0.00875295\n",
            "Iteration 2092, loss = 0.00873609\n",
            "Iteration 2093, loss = 0.00876675\n",
            "Iteration 2094, loss = 0.00874727\n",
            "Iteration 2095, loss = 0.00873434\n",
            "Iteration 2096, loss = 0.00870201\n",
            "Iteration 2097, loss = 0.00870629\n",
            "Iteration 2098, loss = 0.00869672\n",
            "Iteration 2099, loss = 0.00872330\n",
            "Iteration 2100, loss = 0.00865684\n",
            "Iteration 2101, loss = 0.00867075\n",
            "Iteration 2102, loss = 0.00870657\n",
            "Iteration 2103, loss = 0.00869841\n",
            "Iteration 2104, loss = 0.00865137\n",
            "Iteration 2105, loss = 0.00878297\n",
            "Iteration 2106, loss = 0.00859726\n",
            "Iteration 2107, loss = 0.00863878\n",
            "Iteration 2108, loss = 0.00872490\n",
            "Iteration 2109, loss = 0.00862160\n",
            "Iteration 2110, loss = 0.00874453\n",
            "Iteration 2111, loss = 0.00860570\n",
            "Iteration 2112, loss = 0.00860204\n",
            "Iteration 2113, loss = 0.00860627\n",
            "Iteration 2114, loss = 0.00862170\n",
            "Iteration 2115, loss = 0.00868630\n",
            "Iteration 2116, loss = 0.00857042\n",
            "Iteration 2117, loss = 0.00854192\n",
            "Iteration 2118, loss = 0.00855227\n",
            "Iteration 2119, loss = 0.00854151\n",
            "Iteration 2120, loss = 0.00861856\n",
            "Iteration 2121, loss = 0.00859410\n",
            "Iteration 2122, loss = 0.00858265\n",
            "Iteration 2123, loss = 0.00852340\n",
            "Iteration 2124, loss = 0.00851087\n",
            "Iteration 2125, loss = 0.00850622\n",
            "Iteration 2126, loss = 0.00850718\n",
            "Iteration 2127, loss = 0.00849422\n",
            "Iteration 2128, loss = 0.00847447\n",
            "Iteration 2129, loss = 0.00849297\n",
            "Iteration 2130, loss = 0.00847199\n",
            "Iteration 2131, loss = 0.00847759\n",
            "Iteration 2132, loss = 0.00844161\n",
            "Iteration 2133, loss = 0.00846590\n",
            "Iteration 2134, loss = 0.00842931\n",
            "Iteration 2135, loss = 0.00843325\n",
            "Iteration 2136, loss = 0.00844044\n",
            "Iteration 2137, loss = 0.00843242\n",
            "Iteration 2138, loss = 0.00841804\n",
            "Iteration 2139, loss = 0.00846074\n",
            "Iteration 2140, loss = 0.00841828\n",
            "Iteration 2141, loss = 0.00839921\n",
            "Iteration 2142, loss = 0.00837304\n",
            "Iteration 2143, loss = 0.00840243\n",
            "Iteration 2144, loss = 0.00840134\n",
            "Iteration 2145, loss = 0.00837614\n",
            "Iteration 2146, loss = 0.00835638\n",
            "Iteration 2147, loss = 0.00836580\n",
            "Iteration 2148, loss = 0.00834642\n",
            "Iteration 2149, loss = 0.00833017\n",
            "Iteration 2150, loss = 0.00835369\n",
            "Iteration 2151, loss = 0.00830889\n",
            "Iteration 2152, loss = 0.00832624\n",
            "Iteration 2153, loss = 0.00839604\n",
            "Iteration 2154, loss = 0.00832571\n",
            "Iteration 2155, loss = 0.00833050\n",
            "Iteration 2156, loss = 0.00830388\n",
            "Iteration 2157, loss = 0.00831705\n",
            "Iteration 2158, loss = 0.00828238\n",
            "Iteration 2159, loss = 0.00830041\n",
            "Iteration 2160, loss = 0.00836285\n",
            "Iteration 2161, loss = 0.00830050\n",
            "Iteration 2162, loss = 0.00826466\n",
            "Iteration 2163, loss = 0.00823730\n",
            "Iteration 2164, loss = 0.00823225\n",
            "Iteration 2165, loss = 0.00821795\n",
            "Iteration 2166, loss = 0.00821967\n",
            "Iteration 2167, loss = 0.00824209\n",
            "Iteration 2168, loss = 0.00821513\n",
            "Iteration 2169, loss = 0.00824642\n",
            "Iteration 2170, loss = 0.00820834\n",
            "Iteration 2171, loss = 0.00820533\n",
            "Iteration 2172, loss = 0.00818854\n",
            "Iteration 2173, loss = 0.00828990\n",
            "Iteration 2174, loss = 0.00818099\n",
            "Iteration 2175, loss = 0.00813684\n",
            "Iteration 2176, loss = 0.00821260\n",
            "Iteration 2177, loss = 0.00821645\n",
            "Iteration 2178, loss = 0.00814594\n",
            "Iteration 2179, loss = 0.00814014\n",
            "Iteration 2180, loss = 0.00815232\n",
            "Iteration 2181, loss = 0.00822866\n",
            "Iteration 2182, loss = 0.00811238\n",
            "Iteration 2183, loss = 0.00812405\n",
            "Iteration 2184, loss = 0.00809702\n",
            "Iteration 2185, loss = 0.00818995\n",
            "Iteration 2186, loss = 0.00812799\n",
            "Iteration 2187, loss = 0.00813066\n",
            "Iteration 2188, loss = 0.00808268\n",
            "Iteration 2189, loss = 0.00810303\n",
            "Iteration 2190, loss = 0.00807890\n",
            "Iteration 2191, loss = 0.00804848\n",
            "Iteration 2192, loss = 0.00807424\n",
            "Iteration 2193, loss = 0.00808988\n",
            "Iteration 2194, loss = 0.00806519\n",
            "Iteration 2195, loss = 0.00806967\n",
            "Iteration 2196, loss = 0.00804332\n",
            "Iteration 2197, loss = 0.00805130\n",
            "Iteration 2198, loss = 0.00802857\n",
            "Iteration 2199, loss = 0.00807233\n",
            "Iteration 2200, loss = 0.00804022\n",
            "Iteration 2201, loss = 0.00802152\n",
            "Iteration 2202, loss = 0.00811601\n",
            "Iteration 2203, loss = 0.00801782\n",
            "Iteration 2204, loss = 0.00799274\n",
            "Iteration 2205, loss = 0.00798255\n",
            "Iteration 2206, loss = 0.00798246\n",
            "Iteration 2207, loss = 0.00797504\n",
            "Iteration 2208, loss = 0.00795557\n",
            "Iteration 2209, loss = 0.00794470\n",
            "Iteration 2210, loss = 0.00794186\n",
            "Iteration 2211, loss = 0.00796340\n",
            "Iteration 2212, loss = 0.00791429\n",
            "Iteration 2213, loss = 0.00793948\n",
            "Iteration 2214, loss = 0.00793271\n",
            "Iteration 2215, loss = 0.00792572\n",
            "Iteration 2216, loss = 0.00791244\n",
            "Iteration 2217, loss = 0.00789472\n",
            "Iteration 2218, loss = 0.00793191\n",
            "Iteration 2219, loss = 0.00793398\n",
            "Iteration 2220, loss = 0.00790410\n",
            "Iteration 2221, loss = 0.00788884\n",
            "Iteration 2222, loss = 0.00793972\n",
            "Iteration 2223, loss = 0.00790446\n",
            "Iteration 2224, loss = 0.00785323\n",
            "Iteration 2225, loss = 0.00785963\n",
            "Iteration 2226, loss = 0.00786451\n",
            "Iteration 2227, loss = 0.00784025\n",
            "Iteration 2228, loss = 0.00783728\n",
            "Iteration 2229, loss = 0.00785253\n",
            "Iteration 2230, loss = 0.00782503\n",
            "Iteration 2231, loss = 0.00785475\n",
            "Iteration 2232, loss = 0.00780932\n",
            "Iteration 2233, loss = 0.00777919\n",
            "Iteration 2234, loss = 0.00788283\n",
            "Iteration 2235, loss = 0.00779264\n",
            "Iteration 2236, loss = 0.00779051\n",
            "Iteration 2237, loss = 0.00777580\n",
            "Iteration 2238, loss = 0.00777546\n",
            "Iteration 2239, loss = 0.00779568\n",
            "Iteration 2240, loss = 0.00774999\n",
            "Iteration 2241, loss = 0.00777999\n",
            "Iteration 2242, loss = 0.00778790\n",
            "Iteration 2243, loss = 0.00775238\n",
            "Iteration 2244, loss = 0.00778502\n",
            "Iteration 2245, loss = 0.00773598\n",
            "Iteration 2246, loss = 0.00773344\n",
            "Iteration 2247, loss = 0.00773875\n",
            "Iteration 2248, loss = 0.00770434\n",
            "Iteration 2249, loss = 0.00770235\n",
            "Iteration 2250, loss = 0.00768530\n",
            "Iteration 2251, loss = 0.00767651\n",
            "Iteration 2252, loss = 0.00768645\n",
            "Iteration 2253, loss = 0.00767582\n",
            "Iteration 2254, loss = 0.00771789\n",
            "Iteration 2255, loss = 0.00772676\n",
            "Iteration 2256, loss = 0.00766208\n",
            "Iteration 2257, loss = 0.00764443\n",
            "Iteration 2258, loss = 0.00770660\n",
            "Iteration 2259, loss = 0.00761974\n",
            "Iteration 2260, loss = 0.00761884\n",
            "Iteration 2261, loss = 0.00762485\n",
            "Iteration 2262, loss = 0.00766082\n",
            "Iteration 2263, loss = 0.00761389\n",
            "Iteration 2264, loss = 0.00761855\n",
            "Iteration 2265, loss = 0.00760722\n",
            "Iteration 2266, loss = 0.00759843\n",
            "Iteration 2267, loss = 0.00762288\n",
            "Iteration 2268, loss = 0.00762925\n",
            "Iteration 2269, loss = 0.00756856\n",
            "Iteration 2270, loss = 0.00758947\n",
            "Iteration 2271, loss = 0.00756069\n",
            "Iteration 2272, loss = 0.00756636\n",
            "Iteration 2273, loss = 0.00763171\n",
            "Iteration 2274, loss = 0.00760743\n",
            "Iteration 2275, loss = 0.00752933\n",
            "Iteration 2276, loss = 0.00752971\n",
            "Iteration 2277, loss = 0.00753138\n",
            "Iteration 2278, loss = 0.00753804\n",
            "Iteration 2279, loss = 0.00758697\n",
            "Iteration 2280, loss = 0.00752089\n",
            "Iteration 2281, loss = 0.00752594\n",
            "Iteration 2282, loss = 0.00752331\n",
            "Iteration 2283, loss = 0.00751170\n",
            "Iteration 2284, loss = 0.00749199\n",
            "Iteration 2285, loss = 0.00753346\n",
            "Iteration 2286, loss = 0.00748755\n",
            "Iteration 2287, loss = 0.00748168\n",
            "Iteration 2288, loss = 0.00746677\n",
            "Iteration 2289, loss = 0.00746443\n",
            "Iteration 2290, loss = 0.00748238\n",
            "Iteration 2291, loss = 0.00745598\n",
            "Iteration 2292, loss = 0.00745241\n",
            "Iteration 2293, loss = 0.00747569\n",
            "Iteration 2294, loss = 0.00742265\n",
            "Iteration 2295, loss = 0.00742341\n",
            "Iteration 2296, loss = 0.00742189\n",
            "Iteration 2297, loss = 0.00741540\n",
            "Iteration 2298, loss = 0.00740994\n",
            "Iteration 2299, loss = 0.00742024\n",
            "Iteration 2300, loss = 0.00742062\n",
            "Iteration 2301, loss = 0.00738458\n",
            "Iteration 2302, loss = 0.00738979\n",
            "Iteration 2303, loss = 0.00737327\n",
            "Iteration 2304, loss = 0.00745156\n",
            "Iteration 2305, loss = 0.00740176\n",
            "Iteration 2306, loss = 0.00740199\n",
            "Iteration 2307, loss = 0.00733700\n",
            "Iteration 2308, loss = 0.00735263\n",
            "Iteration 2309, loss = 0.00733791\n",
            "Iteration 2310, loss = 0.00734454\n",
            "Iteration 2311, loss = 0.00732552\n",
            "Iteration 2312, loss = 0.00736120\n",
            "Iteration 2313, loss = 0.00732666\n",
            "Iteration 2314, loss = 0.00734984\n",
            "Iteration 2315, loss = 0.00737920\n",
            "Iteration 2316, loss = 0.00732154\n",
            "Iteration 2317, loss = 0.00733419\n",
            "Iteration 2318, loss = 0.00728464\n",
            "Iteration 2319, loss = 0.00728294\n",
            "Iteration 2320, loss = 0.00728344\n",
            "Iteration 2321, loss = 0.00728463\n",
            "Iteration 2322, loss = 0.00726881\n",
            "Iteration 2323, loss = 0.00741314\n",
            "Iteration 2324, loss = 0.00733557\n",
            "Iteration 2325, loss = 0.00726874\n",
            "Iteration 2326, loss = 0.00733691\n",
            "Iteration 2327, loss = 0.00725324\n",
            "Iteration 2328, loss = 0.00725325\n",
            "Iteration 2329, loss = 0.00723552\n",
            "Iteration 2330, loss = 0.00720368\n",
            "Iteration 2331, loss = 0.00722298\n",
            "Iteration 2332, loss = 0.00729296\n",
            "Iteration 2333, loss = 0.00719568\n",
            "Iteration 2334, loss = 0.00723041\n",
            "Iteration 2335, loss = 0.00731712\n",
            "Iteration 2336, loss = 0.00717953\n",
            "Iteration 2337, loss = 0.00718823\n",
            "Iteration 2338, loss = 0.00719777\n",
            "Iteration 2339, loss = 0.00716905\n",
            "Iteration 2340, loss = 0.00719642\n",
            "Iteration 2341, loss = 0.00715653\n",
            "Iteration 2342, loss = 0.00716767\n",
            "Iteration 2343, loss = 0.00713371\n",
            "Iteration 2344, loss = 0.00717468\n",
            "Iteration 2345, loss = 0.00713521\n",
            "Iteration 2346, loss = 0.00716096\n",
            "Iteration 2347, loss = 0.00711987\n",
            "Iteration 2348, loss = 0.00713945\n",
            "Iteration 2349, loss = 0.00715065\n",
            "Iteration 2350, loss = 0.00712985\n",
            "Iteration 2351, loss = 0.00709500\n",
            "Iteration 2352, loss = 0.00709298\n",
            "Iteration 2353, loss = 0.00709198\n",
            "Iteration 2354, loss = 0.00709575\n",
            "Iteration 2355, loss = 0.00709184\n",
            "Iteration 2356, loss = 0.00710031\n",
            "Iteration 2357, loss = 0.00709826\n",
            "Iteration 2358, loss = 0.00708244\n",
            "Iteration 2359, loss = 0.00710201\n",
            "Iteration 2360, loss = 0.00707602\n",
            "Iteration 2361, loss = 0.00711726\n",
            "Iteration 2362, loss = 0.00704229\n",
            "Iteration 2363, loss = 0.00704438\n",
            "Iteration 2364, loss = 0.00708628\n",
            "Iteration 2365, loss = 0.00703823\n",
            "Iteration 2366, loss = 0.00705791\n",
            "Iteration 2367, loss = 0.00701390\n",
            "Iteration 2368, loss = 0.00702635\n",
            "Iteration 2369, loss = 0.00699699\n",
            "Iteration 2370, loss = 0.00702171\n",
            "Iteration 2371, loss = 0.00702734\n",
            "Iteration 2372, loss = 0.00701207\n",
            "Iteration 2373, loss = 0.00698110\n",
            "Iteration 2374, loss = 0.00697745\n",
            "Iteration 2375, loss = 0.00696383\n",
            "Iteration 2376, loss = 0.00696530\n",
            "Iteration 2377, loss = 0.00694555\n",
            "Iteration 2378, loss = 0.00696192\n",
            "Iteration 2379, loss = 0.00696026\n",
            "Iteration 2380, loss = 0.00699247\n",
            "Iteration 2381, loss = 0.00694614\n",
            "Iteration 2382, loss = 0.00696815\n",
            "Iteration 2383, loss = 0.00692519\n",
            "Iteration 2384, loss = 0.00691800\n",
            "Iteration 2385, loss = 0.00693085\n",
            "Iteration 2386, loss = 0.00691531\n",
            "Iteration 2387, loss = 0.00691762\n",
            "Iteration 2388, loss = 0.00690529\n",
            "Iteration 2389, loss = 0.00687869\n",
            "Iteration 2390, loss = 0.00693005\n",
            "Iteration 2391, loss = 0.00687561\n",
            "Iteration 2392, loss = 0.00690092\n",
            "Iteration 2393, loss = 0.00686904\n",
            "Iteration 2394, loss = 0.00689278\n",
            "Iteration 2395, loss = 0.00686591\n",
            "Iteration 2396, loss = 0.00687721\n",
            "Iteration 2397, loss = 0.00688670\n",
            "Iteration 2398, loss = 0.00688449\n",
            "Iteration 2399, loss = 0.00683782\n",
            "Iteration 2400, loss = 0.00686986\n",
            "Iteration 2401, loss = 0.00682262\n",
            "Iteration 2402, loss = 0.00687486\n",
            "Iteration 2403, loss = 0.00680775\n",
            "Iteration 2404, loss = 0.00683851\n",
            "Iteration 2405, loss = 0.00680062\n",
            "Iteration 2406, loss = 0.00685255\n",
            "Iteration 2407, loss = 0.00682959\n",
            "Iteration 2408, loss = 0.00683339\n",
            "Iteration 2409, loss = 0.00686795\n",
            "Iteration 2410, loss = 0.00676187\n",
            "Iteration 2411, loss = 0.00678043\n",
            "Iteration 2412, loss = 0.00678396\n",
            "Iteration 2413, loss = 0.00677900\n",
            "Iteration 2414, loss = 0.00679156\n",
            "Iteration 2415, loss = 0.00678389\n",
            "Iteration 2416, loss = 0.00675962\n",
            "Iteration 2417, loss = 0.00678787\n",
            "Iteration 2418, loss = 0.00673967\n",
            "Iteration 2419, loss = 0.00675662\n",
            "Iteration 2420, loss = 0.00675376\n",
            "Iteration 2421, loss = 0.00671789\n",
            "Iteration 2422, loss = 0.00671490\n",
            "Iteration 2423, loss = 0.00673960\n",
            "Iteration 2424, loss = 0.00673211\n",
            "Iteration 2425, loss = 0.00671425\n",
            "Iteration 2426, loss = 0.00669380\n",
            "Iteration 2427, loss = 0.00672424\n",
            "Iteration 2428, loss = 0.00671301\n",
            "Iteration 2429, loss = 0.00669935\n",
            "Iteration 2430, loss = 0.00666696\n",
            "Iteration 2431, loss = 0.00666882\n",
            "Iteration 2432, loss = 0.00665625\n",
            "Iteration 2433, loss = 0.00671975\n",
            "Iteration 2434, loss = 0.00665079\n",
            "Iteration 2435, loss = 0.00669680\n",
            "Iteration 2436, loss = 0.00662549\n",
            "Iteration 2437, loss = 0.00666742\n",
            "Iteration 2438, loss = 0.00665456\n",
            "Iteration 2439, loss = 0.00662211\n",
            "Iteration 2440, loss = 0.00662533\n",
            "Iteration 2441, loss = 0.00663724\n",
            "Iteration 2442, loss = 0.00672588\n",
            "Iteration 2443, loss = 0.00662836\n",
            "Iteration 2444, loss = 0.00663883\n",
            "Iteration 2445, loss = 0.00663835\n",
            "Iteration 2446, loss = 0.00659335\n",
            "Iteration 2447, loss = 0.00658648\n",
            "Iteration 2448, loss = 0.00658234\n",
            "Iteration 2449, loss = 0.00656834\n",
            "Iteration 2450, loss = 0.00656266\n",
            "Iteration 2451, loss = 0.00657136\n",
            "Iteration 2452, loss = 0.00660936\n",
            "Iteration 2453, loss = 0.00658106\n",
            "Iteration 2454, loss = 0.00655671\n",
            "Iteration 2455, loss = 0.00659693\n",
            "Iteration 2456, loss = 0.00654160\n",
            "Iteration 2457, loss = 0.00652760\n",
            "Iteration 2458, loss = 0.00658931\n",
            "Iteration 2459, loss = 0.00651204\n",
            "Iteration 2460, loss = 0.00655725\n",
            "Iteration 2461, loss = 0.00656314\n",
            "Iteration 2462, loss = 0.00652237\n",
            "Iteration 2463, loss = 0.00653256\n",
            "Iteration 2464, loss = 0.00651158\n",
            "Iteration 2465, loss = 0.00654523\n",
            "Iteration 2466, loss = 0.00648027\n",
            "Iteration 2467, loss = 0.00647002\n",
            "Iteration 2468, loss = 0.00649561\n",
            "Iteration 2469, loss = 0.00650862\n",
            "Iteration 2470, loss = 0.00646917\n",
            "Iteration 2471, loss = 0.00645248\n",
            "Iteration 2472, loss = 0.00652503\n",
            "Iteration 2473, loss = 0.00643964\n",
            "Iteration 2474, loss = 0.00645735\n",
            "Iteration 2475, loss = 0.00649954\n",
            "Iteration 2476, loss = 0.00645057\n",
            "Iteration 2477, loss = 0.00652370\n",
            "Iteration 2478, loss = 0.00646205\n",
            "Iteration 2479, loss = 0.00645545\n",
            "Iteration 2480, loss = 0.00644689\n",
            "Iteration 2481, loss = 0.00640729\n",
            "Iteration 2482, loss = 0.00642783\n",
            "Iteration 2483, loss = 0.00641512\n",
            "Iteration 2484, loss = 0.00639910\n",
            "Iteration 2485, loss = 0.00638843\n",
            "Iteration 2486, loss = 0.00638302\n",
            "Iteration 2487, loss = 0.00637624\n",
            "Iteration 2488, loss = 0.00638227\n",
            "Iteration 2489, loss = 0.00636435\n",
            "Iteration 2490, loss = 0.00638562\n",
            "Iteration 2491, loss = 0.00636823\n",
            "Iteration 2492, loss = 0.00642618\n",
            "Iteration 2493, loss = 0.00641407\n",
            "Iteration 2494, loss = 0.00635780\n",
            "Iteration 2495, loss = 0.00638217\n",
            "Iteration 2496, loss = 0.00634684\n",
            "Iteration 2497, loss = 0.00633745\n",
            "Iteration 2498, loss = 0.00633946\n",
            "Iteration 2499, loss = 0.00635806\n",
            "Iteration 2500, loss = 0.00635200\n",
            "Iteration 2501, loss = 0.00630626\n",
            "Iteration 2502, loss = 0.00631544\n",
            "Iteration 2503, loss = 0.00629181\n",
            "Iteration 2504, loss = 0.00629749\n",
            "Iteration 2505, loss = 0.00635108\n",
            "Iteration 2506, loss = 0.00629930\n",
            "Iteration 2507, loss = 0.00629662\n",
            "Iteration 2508, loss = 0.00629642\n",
            "Iteration 2509, loss = 0.00631198\n",
            "Iteration 2510, loss = 0.00629352\n",
            "Iteration 2511, loss = 0.00625986\n",
            "Iteration 2512, loss = 0.00626810\n",
            "Iteration 2513, loss = 0.00626435\n",
            "Iteration 2514, loss = 0.00627539\n",
            "Iteration 2515, loss = 0.00628339\n",
            "Iteration 2516, loss = 0.00627322\n",
            "Iteration 2517, loss = 0.00627685\n",
            "Iteration 2518, loss = 0.00622558\n",
            "Iteration 2519, loss = 0.00622050\n",
            "Iteration 2520, loss = 0.00627946\n",
            "Iteration 2521, loss = 0.00628175\n",
            "Iteration 2522, loss = 0.00623465\n",
            "Iteration 2523, loss = 0.00626166\n",
            "Iteration 2524, loss = 0.00623842\n",
            "Iteration 2525, loss = 0.00621650\n",
            "Iteration 2526, loss = 0.00618378\n",
            "Iteration 2527, loss = 0.00623333\n",
            "Iteration 2528, loss = 0.00619962\n",
            "Iteration 2529, loss = 0.00616494\n",
            "Iteration 2530, loss = 0.00619976\n",
            "Iteration 2531, loss = 0.00618554\n",
            "Iteration 2532, loss = 0.00616233\n",
            "Iteration 2533, loss = 0.00617409\n",
            "Iteration 2534, loss = 0.00617935\n",
            "Iteration 2535, loss = 0.00618578\n",
            "Iteration 2536, loss = 0.00614190\n",
            "Iteration 2537, loss = 0.00613350\n",
            "Iteration 2538, loss = 0.00614800\n",
            "Iteration 2539, loss = 0.00615471\n",
            "Iteration 2540, loss = 0.00613770\n",
            "Iteration 2541, loss = 0.00611092\n",
            "Iteration 2542, loss = 0.00612800\n",
            "Iteration 2543, loss = 0.00611915\n",
            "Iteration 2544, loss = 0.00613936\n",
            "Iteration 2545, loss = 0.00612615\n",
            "Iteration 2546, loss = 0.00609774\n",
            "Iteration 2547, loss = 0.00610225\n",
            "Iteration 2548, loss = 0.00612775\n",
            "Iteration 2549, loss = 0.00615829\n",
            "Iteration 2550, loss = 0.00606736\n",
            "Iteration 2551, loss = 0.00605929\n",
            "Iteration 2552, loss = 0.00625027\n",
            "Iteration 2553, loss = 0.00609027\n",
            "Iteration 2554, loss = 0.00609011\n",
            "Iteration 2555, loss = 0.00616625\n",
            "Iteration 2556, loss = 0.00615603\n",
            "Iteration 2557, loss = 0.00606876\n",
            "Iteration 2558, loss = 0.00605993\n",
            "Iteration 2559, loss = 0.00603708\n",
            "Iteration 2560, loss = 0.00607116\n",
            "Iteration 2561, loss = 0.00603566\n",
            "Iteration 2562, loss = 0.00612118\n",
            "Iteration 2563, loss = 0.00603047\n",
            "Iteration 2564, loss = 0.00603073\n",
            "Iteration 2565, loss = 0.00607534\n",
            "Iteration 2566, loss = 0.00603045\n",
            "Iteration 2567, loss = 0.00599270\n",
            "Iteration 2568, loss = 0.00599741\n",
            "Iteration 2569, loss = 0.00602095\n",
            "Iteration 2570, loss = 0.00600811\n",
            "Iteration 2571, loss = 0.00598258\n",
            "Iteration 2572, loss = 0.00602603\n",
            "Iteration 2573, loss = 0.00598807\n",
            "Iteration 2574, loss = 0.00596450\n",
            "Iteration 2575, loss = 0.00597043\n",
            "Iteration 2576, loss = 0.00595641\n",
            "Iteration 2577, loss = 0.00596156\n",
            "Iteration 2578, loss = 0.00600589\n",
            "Iteration 2579, loss = 0.00593882\n",
            "Iteration 2580, loss = 0.00593931\n",
            "Iteration 2581, loss = 0.00596494\n",
            "Iteration 2582, loss = 0.00603226\n",
            "Iteration 2583, loss = 0.00596538\n",
            "Iteration 2584, loss = 0.00592626\n",
            "Iteration 2585, loss = 0.00598492\n",
            "Iteration 2586, loss = 0.00590294\n",
            "Iteration 2587, loss = 0.00592093\n",
            "Iteration 2588, loss = 0.00590549\n",
            "Iteration 2589, loss = 0.00592506\n",
            "Iteration 2590, loss = 0.00592947\n",
            "Iteration 2591, loss = 0.00597886\n",
            "Iteration 2592, loss = 0.00586486\n",
            "Iteration 2593, loss = 0.00589093\n",
            "Iteration 2594, loss = 0.00588762\n",
            "Iteration 2595, loss = 0.00588854\n",
            "Iteration 2596, loss = 0.00589311\n",
            "Iteration 2597, loss = 0.00587740\n",
            "Iteration 2598, loss = 0.00586681\n",
            "Iteration 2599, loss = 0.00584619\n",
            "Iteration 2600, loss = 0.00586047\n",
            "Iteration 2601, loss = 0.00591505\n",
            "Iteration 2602, loss = 0.00585339\n",
            "Iteration 2603, loss = 0.00586335\n",
            "Iteration 2604, loss = 0.00584314\n",
            "Iteration 2605, loss = 0.00587433\n",
            "Iteration 2606, loss = 0.00583396\n",
            "Iteration 2607, loss = 0.00585879\n",
            "Iteration 2608, loss = 0.00586166\n",
            "Iteration 2609, loss = 0.00581566\n",
            "Iteration 2610, loss = 0.00581456\n",
            "Iteration 2611, loss = 0.00580483\n",
            "Iteration 2612, loss = 0.00579777\n",
            "Iteration 2613, loss = 0.00580119\n",
            "Iteration 2614, loss = 0.00579663\n",
            "Iteration 2615, loss = 0.00580780\n",
            "Iteration 2616, loss = 0.00577267\n",
            "Iteration 2617, loss = 0.00579937\n",
            "Iteration 2618, loss = 0.00578122\n",
            "Iteration 2619, loss = 0.00577202\n",
            "Iteration 2620, loss = 0.00576864\n",
            "Iteration 2621, loss = 0.00575993\n",
            "Iteration 2622, loss = 0.00577263\n",
            "Iteration 2623, loss = 0.00580318\n",
            "Iteration 2624, loss = 0.00575837\n",
            "Iteration 2625, loss = 0.00576597\n",
            "Iteration 2626, loss = 0.00576137\n",
            "Iteration 2627, loss = 0.00574805\n",
            "Iteration 2628, loss = 0.00572628\n",
            "Iteration 2629, loss = 0.00572111\n",
            "Iteration 2630, loss = 0.00571685\n",
            "Iteration 2631, loss = 0.00573427\n",
            "Iteration 2632, loss = 0.00573824\n",
            "Iteration 2633, loss = 0.00571961\n",
            "Iteration 2634, loss = 0.00576817\n",
            "Iteration 2635, loss = 0.00572967\n",
            "Iteration 2636, loss = 0.00570213\n",
            "Iteration 2637, loss = 0.00572446\n",
            "Iteration 2638, loss = 0.00574051\n",
            "Iteration 2639, loss = 0.00568966\n",
            "Iteration 2640, loss = 0.00570477\n",
            "Iteration 2641, loss = 0.00568189\n",
            "Iteration 2642, loss = 0.00568407\n",
            "Iteration 2643, loss = 0.00566044\n",
            "Iteration 2644, loss = 0.00565448\n",
            "Iteration 2645, loss = 0.00565122\n",
            "Iteration 2646, loss = 0.00564974\n",
            "Iteration 2647, loss = 0.00565005\n",
            "Iteration 2648, loss = 0.00565791\n",
            "Iteration 2649, loss = 0.00565524\n",
            "Iteration 2650, loss = 0.00562577\n",
            "Iteration 2651, loss = 0.00565863\n",
            "Iteration 2652, loss = 0.00565219\n",
            "Iteration 2653, loss = 0.00562006\n",
            "Iteration 2654, loss = 0.00560626\n",
            "Iteration 2655, loss = 0.00563281\n",
            "Iteration 2656, loss = 0.00566372\n",
            "Iteration 2657, loss = 0.00562313\n",
            "Iteration 2658, loss = 0.00564884\n",
            "Iteration 2659, loss = 0.00561062\n",
            "Iteration 2660, loss = 0.00560587\n",
            "Iteration 2661, loss = 0.00565076\n",
            "Iteration 2662, loss = 0.00562285\n",
            "Iteration 2663, loss = 0.00558881\n",
            "Iteration 2664, loss = 0.00559738\n",
            "Iteration 2665, loss = 0.00558363\n",
            "Iteration 2666, loss = 0.00557006\n",
            "Iteration 2667, loss = 0.00569124\n",
            "Iteration 2668, loss = 0.00557244\n",
            "Iteration 2669, loss = 0.00555946\n",
            "Iteration 2670, loss = 0.00554675\n",
            "Iteration 2671, loss = 0.00555115\n",
            "Iteration 2672, loss = 0.00557239\n",
            "Iteration 2673, loss = 0.00554409\n",
            "Iteration 2674, loss = 0.00552846\n",
            "Iteration 2675, loss = 0.00554959\n",
            "Iteration 2676, loss = 0.00558013\n",
            "Iteration 2677, loss = 0.00555886\n",
            "Iteration 2678, loss = 0.00551915\n",
            "Iteration 2679, loss = 0.00551619\n",
            "Iteration 2680, loss = 0.00555862\n",
            "Iteration 2681, loss = 0.00558514\n",
            "Iteration 2682, loss = 0.00549723\n",
            "Iteration 2683, loss = 0.00553467\n",
            "Iteration 2684, loss = 0.00548442\n",
            "Iteration 2685, loss = 0.00547882\n",
            "Iteration 2686, loss = 0.00548479\n",
            "Iteration 2687, loss = 0.00548380\n",
            "Iteration 2688, loss = 0.00548138\n",
            "Iteration 2689, loss = 0.00547370\n",
            "Iteration 2690, loss = 0.00546517\n",
            "Iteration 2691, loss = 0.00546337\n",
            "Iteration 2692, loss = 0.00546652\n",
            "Iteration 2693, loss = 0.00553174\n",
            "Iteration 2694, loss = 0.00545080\n",
            "Iteration 2695, loss = 0.00544008\n",
            "Iteration 2696, loss = 0.00546044\n",
            "Iteration 2697, loss = 0.00548721\n",
            "Iteration 2698, loss = 0.00544329\n",
            "Iteration 2699, loss = 0.00549380\n",
            "Iteration 2700, loss = 0.00541724\n",
            "Iteration 2701, loss = 0.00539838\n",
            "Iteration 2702, loss = 0.00547736\n",
            "Iteration 2703, loss = 0.00544044\n",
            "Iteration 2704, loss = 0.00540447\n",
            "Iteration 2705, loss = 0.00542720\n",
            "Iteration 2706, loss = 0.00542187\n",
            "Iteration 2707, loss = 0.00553641\n",
            "Iteration 2708, loss = 0.00539107\n",
            "Iteration 2709, loss = 0.00549161\n",
            "Iteration 2710, loss = 0.00539883\n",
            "Iteration 2711, loss = 0.00540699\n",
            "Iteration 2712, loss = 0.00537320\n",
            "Iteration 2713, loss = 0.00535759\n",
            "Iteration 2714, loss = 0.00537628\n",
            "Iteration 2715, loss = 0.00537919\n",
            "Iteration 2716, loss = 0.00537916\n",
            "Iteration 2717, loss = 0.00535542\n",
            "Iteration 2718, loss = 0.00535429\n",
            "Iteration 2719, loss = 0.00534898\n",
            "Iteration 2720, loss = 0.00534479\n",
            "Iteration 2721, loss = 0.00533699\n",
            "Iteration 2722, loss = 0.00536670\n",
            "Iteration 2723, loss = 0.00534929\n",
            "Iteration 2724, loss = 0.00531133\n",
            "Iteration 2725, loss = 0.00531541\n",
            "Iteration 2726, loss = 0.00530427\n",
            "Iteration 2727, loss = 0.00530987\n",
            "Iteration 2728, loss = 0.00529599\n",
            "Iteration 2729, loss = 0.00531163\n",
            "Iteration 2730, loss = 0.00535741\n",
            "Iteration 2731, loss = 0.00533053\n",
            "Iteration 2732, loss = 0.00529590\n",
            "Iteration 2733, loss = 0.00529964\n",
            "Iteration 2734, loss = 0.00529122\n",
            "Iteration 2735, loss = 0.00529630\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFHCAYAAAAGHI0yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOIklEQVR4nO3cf5DXBZ3H8deuS4u7oKQIiIE/rmBD06yMsB8sY42/ppjR7pq7/JWlNqJeBzo5Xc5mY8l5GjXXZZde0Y+pm6xx8jLhtMxOk9T8UZRAdgeI6JpgiuyyLOzeH040hYRz7puvsI/HDH98P5+dz7z4hyef7/ez36bBwcHBAAAlmhs9AAD2ZEILAIWEFgAKCS0AFBJaACjUMtQXHBgYyMaNGzNixIg0NTUN9eUB4GVlcHAw/f39aW9vT3Pz9vevQx7ajRs3ZsWKFUN9WQB4WZsyZUpGjx693fEhD+2IESOSJHd98BPZ9OT6ob488Bf8/f/+KMnSRs+AYWXz5mTFij/2788NeWj/8HbxpifXp/fxp4b68sBf0Nra2ugJMGzt6ONSD0MBQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQjtMveakmekaXJ59Dz4ozS0tOelfuzLn4VtywfJFOfnay9Pc0pIkOfP2r+Ujq27PnIdv2fZn9MRxDV4Pe5b+/i2ZN29BmprelDVruhs9hyHW8mJ+6O67785VV12Vnp6eTJw4MVdeeWUmTJhQvY0iLXuPzHHz56Vn3dNJkmMvPjvt4/bLFw4/Oc0jWnLm7V/LG875m9x37TeTJDee8dGsuuOeRk6GPdrs2XNzzDGHN3oGRXZ6R9vT05O5c+fmiiuuyOLFizNr1qx0dXXtim0U6fzEhfnF12/K5g0bkyQr77g3t116TQYHBrK1b3Mevev+jJ16aINXwvBx2WUfyuWXn9foGRTZaWiXLFmSSZMm5fDDn//f1qmnnpq77rorzz33XPk4ht64I6bksHcdmyULFm47tubuB/L0b1cnSUZNOCCvPvEdWfH927ednzH3Azn3/htz3oPfy9EffO+ungx7vBkzjmz0BArt9K3jlStXZtKkSdtet7e3Z8yYMVm9enWmTZtWOo6hd/IXL88tF16RgS1btjt31h3fyMRjXpe7r/lK/ue2nyZJfnPzHVn/29VZduOtOWDaq3Pm7V/L+t+syqqf3LurpwPslnZ6R9vb25vW1tY/Odba2pqenp6yUdR447nvy1O/fiSP3vXzFzy/cOZpuXr8sRn72sPyzvkXJ0l+evW/Z9mNtyZJfvfrR7L0P27Oa07u3FWTAXZ7Ow1tW1tb+vr6/uTYpk2b0t7eXjaKGlNnH5eps4/LvMfvzLzH78w+kw7MOfd+J1Pfc1z2mXRgkmTzho15aOGN+avj35am5uaMP3Lqn1yjuaUlA/39jZgPsFvaaWgPO+ywrF69etvrDRs25JlnnsnBBx9cOoyh982Tz83V44/NNQe+Ldcc+LY8++jjue6Y92bq7OPS+YkLk6amJMlrTu5M9y+WJ0n+9vv/lmnvPSFJss+rJuS1p7wrK26+o2F/B4DdzU5DO3369Kxduzb33XdfkmThwoWZNWtW2traysexa/zXxf+Ulr1bn/892hWLM2rC2Nx6yVUZHBjIt0+5MDPmfSBzli3K+2+5Lj/6x89mzd0PNHoy7DG6u9elo+PUdHScmiTp7DwvHR2n5rHHnmzwMoZK0+Dg4ODOfuhnP/tZPvWpT6W3tzeTJ0/O/Pnzc8ABB7zgz/b19WXp0qX54bsvSu/jTw35YGDHugaXJ3nhz+CBGn19ydKlyRFHHLHdM03Ji/zCiunTp+emm24a8nEAsKfzFYwAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFWqou/JV916d70++qLg+8gK4kyRsbvAKGm74kS3d4tiy0Dz74jbS2Vl0deCH77bdf1j+yoNEzYHjpH5Fk6g5Pe+sYAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0bNPfvyXz5i1IU9ObsmZNd6PnwB5t7eNP512n/HMOef28HPn2j+cnP12eJOmaf2M6pl+aKW/+aN73wS/k989sbPBSXiqhZZvZs+dm1Ki2Rs+AYeHMOdfnxHe+LisfvCaf+/T78/nrb8u3vrskt/74V3ngx5/MsiVXZuvWgXx6wfcbPZWX6EWFtr+/P/Pnz8/UqVPzxBNPVG+iQS677EO5/PLzGj0D9niPPrYuP39oZS48551Jkllvf22+/eU5mTZ1Yq69+ozsvfcr0tzcnM63dmT5b/ybu7t7UaE9//zz09bmTmdPN2PGkY2eAMPCQ0sfzaEHj82ln7whU998aWa++8o88ItVOeqIyTnqiMlJkmee7ckNN92b95z4+gav5aV60aG96KKLqrcADAu/f6Ynv/z1mrxjxtQsv2d+TvvrGTnlzH/Jli1bkyR/d+4Xc+C0j+TVh47LGe97a4PX8lK9qNAeffTR1TsAho1999k74w/YN7NPekOS5EOnz8z6pzdmxSPPv038zS99OOsf+Xza21pz2oe/1MipDAEPQwHsYgdPGpsNz/VmYGAgSdLU1JTm5qb895IV+dWyx5IkI0e+IuecMTOLf/TLRk5lCAgtwC72ummvysQJr8z1X/9JkuSG792TV45pT/fvns3cj38rfX39SZL/XPRgjjx8UiOnMgRaGj2Al4fu7nWZOfPcba87O89LS8te+eEPr81BB41r4DLY8zQ1NeU7X5mTsy64PvM/d3PGjR2dG748J4d3TMw/PPH7HPn2yzKYwUyauF+u/+zZjZ7LSyS0JEnGj98/y5Z9t9EzYNiY1nFQ7rmta7vj115zZgPWUGmnoX3qqady2mmnbXt9+umnZ6+99spXv/rVjB8/vnQcAOzudhrasWPHZtGiRbtiCwDscTwMBQCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAo1DLUFxwcHEySbN481FcGdmb8+PHp6x/R6BkwrGze8nxK/9C/P9c0uKMz/08bNmzIihUrhvKSAPCyN2XKlIwePXq740Me2oGBgWzcuDEjRoxIU1PTUF4aAF52BgcH09/fn/b29jQ3b/+J7JCHFgD4Iw9DAUAhoQWAQkILAIWEFgAKCS0AFBryL6xg99LT05PVq1enp6cnbW1tOeSQQzJy5MhGz4Jh7cknn8y4ceMaPYMh4td7hqnu7u50dXXlzjvvzJgxYzJy5Mhs2rQpzz77bDo7O9PV1ZX999+/0TNhWDrppJPygx/8oNEzGCLuaIepj33sY+ns7MxnPvOZtLW1bTu+YcOGLFy4MJdeemmuu+66Bi6EPVd3d/dfPL9169ZdtIRdwR3tMHXCCSdk0aJFOzx//PHHZ/HixbtwEQwfHR0daWpq2vF34zY15eGHH97Fq6jijnaYamtry7Jly9LR0bHdufvvv9/ntFDorLPOyqhRo3LBBRe84PkTTzxxFy+iktAOU5dccknOPvvsTJ48OZMmTUpra2v6+vqyatWqrF27NgsWLGj0RNhjXXzxxTn//PPz0EMP5aijjmr0HIp563gY6+3tzZIlS7Jy5cr09vamra0thx56aN7ylrektbW10fNg2Fq3bp2HEfcgQgsAhXxhBQAUEloAKCS0AFBIaAGgkNACQKH/A93Jt0+xPIt8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmsUTNpRjezi",
        "outputId": "b675bd47-257f-4cb4-f9b0-db9a728bc7d4"
      },
      "source": [
        "print(classification_report(y_credit_teste,previsoes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       1.00      1.00      1.00        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       1.00      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6JRHz8BsGc4"
      },
      "source": [
        "Redes neurais - base de dados census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G1_gL04mFIl"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUJKliyfsJec"
      },
      "source": [
        "import pickle\n",
        "with open('census.pkl','rb') as f:\n",
        "  x_census_treinamento, y_census_treinamento, x_census_teste, y_census_teste = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTTOzJqam6IR",
        "outputId": "829e94ea-a880-4b59-d95c-4c6f67868633"
      },
      "source": [
        "x_census_treinamento.shape, y_census_treinamento.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIK-iWknAFU",
        "outputId": "dfd929af-5185-4211-dfc5-fc83b1392e72"
      },
      "source": [
        "x_census_treinamento"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.2444502 , -0.17429511, -0.26209736, ..., -0.14592048,\n",
              "        -0.21665953, -0.03542945],\n",
              "       [-0.2444502 , -0.17429511, -0.26209736, ..., -0.14592048,\n",
              "        -0.21665953,  3.20416118],\n",
              "       [-0.2444502 , -0.17429511,  3.81537614, ..., -0.14592048,\n",
              "        -0.21665953, -0.03542945],\n",
              "       ...,\n",
              "       [-0.2444502 , -0.17429511, -0.26209736, ..., -0.14592048,\n",
              "        -0.21665953, -0.03542945],\n",
              "       [-0.2444502 , -0.17429511,  3.81537614, ...,  0.89508344,\n",
              "        -0.21665953,  0.36951938],\n",
              "       [-0.2444502 , -0.17429511, -0.26209736, ...,  0.20572978,\n",
              "        -0.21665953,  0.61248868]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piZlx5gunFCh",
        "outputId": "da3f9713-99fb-4c41-986d-b5b26c4a7249"
      },
      "source": [
        "y_census_treinamento"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' >50K', ' <=50K'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjqSI9osnI19",
        "outputId": "fb26899d-5072-4f2c-b43a-e3e1e1048b2a"
      },
      "source": [
        "#Nessa rede neural temos 108 neurônios na camada de entrada, correspondendo as 108 colunas de atributos previsores\n",
        "#e 1 neurônio de saída, que irá classificar se a pessoas recebe menos ou mais que 50k\n",
        "#Calculando a quantidade de neuronios na camada oculta, temos: (108+1/2) = 54.5\n",
        "#LOogo, devemos ter 55 neurônios na camada oculta. Para esse problema específico, criamos duas camadas ocultas com 55 neurônios.\n",
        "#Configuramos usando o parâmetro 'hidden_layer_sizes'\n",
        "neural_census = MLPClassifier(verbose=True,activation='relu', max_iter = 1000, tol = 0.00001, hidden_layer_sizes=(55,55))\n",
        "neural_census.fit(x_census_treinamento,y_census_treinamento)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.41568097\n",
            "Iteration 2, loss = 0.33161498\n",
            "Iteration 3, loss = 0.31813156\n",
            "Iteration 4, loss = 0.30986333\n",
            "Iteration 5, loss = 0.30458879\n",
            "Iteration 6, loss = 0.29975877\n",
            "Iteration 7, loss = 0.29605508\n",
            "Iteration 8, loss = 0.29377891\n",
            "Iteration 9, loss = 0.29087853\n",
            "Iteration 10, loss = 0.28888463\n",
            "Iteration 11, loss = 0.28604894\n",
            "Iteration 12, loss = 0.28471828\n",
            "Iteration 13, loss = 0.28222898\n",
            "Iteration 14, loss = 0.28059337\n",
            "Iteration 15, loss = 0.27930851\n",
            "Iteration 16, loss = 0.27676136\n",
            "Iteration 17, loss = 0.27579941\n",
            "Iteration 18, loss = 0.27344315\n",
            "Iteration 19, loss = 0.27224797\n",
            "Iteration 20, loss = 0.27066335\n",
            "Iteration 21, loss = 0.26959784\n",
            "Iteration 22, loss = 0.26795286\n",
            "Iteration 23, loss = 0.26661914\n",
            "Iteration 24, loss = 0.26496015\n",
            "Iteration 25, loss = 0.26290846\n",
            "Iteration 26, loss = 0.26134835\n",
            "Iteration 27, loss = 0.26002821\n",
            "Iteration 28, loss = 0.25916711\n",
            "Iteration 29, loss = 0.25754308\n",
            "Iteration 30, loss = 0.25574850\n",
            "Iteration 31, loss = 0.25500747\n",
            "Iteration 32, loss = 0.25340676\n",
            "Iteration 33, loss = 0.25200525\n",
            "Iteration 34, loss = 0.25065713\n",
            "Iteration 35, loss = 0.25033497\n",
            "Iteration 36, loss = 0.24829125\n",
            "Iteration 37, loss = 0.24740101\n",
            "Iteration 38, loss = 0.24678316\n",
            "Iteration 39, loss = 0.24575674\n",
            "Iteration 40, loss = 0.24409649\n",
            "Iteration 41, loss = 0.24188159\n",
            "Iteration 42, loss = 0.24168292\n",
            "Iteration 43, loss = 0.24103414\n",
            "Iteration 44, loss = 0.23956207\n",
            "Iteration 45, loss = 0.23843411\n",
            "Iteration 46, loss = 0.23723403\n",
            "Iteration 47, loss = 0.23571086\n",
            "Iteration 48, loss = 0.23555243\n",
            "Iteration 49, loss = 0.23462783\n",
            "Iteration 50, loss = 0.23411360\n",
            "Iteration 51, loss = 0.23286584\n",
            "Iteration 52, loss = 0.23201495\n",
            "Iteration 53, loss = 0.23008174\n",
            "Iteration 54, loss = 0.22946047\n",
            "Iteration 55, loss = 0.22904162\n",
            "Iteration 56, loss = 0.22774802\n",
            "Iteration 57, loss = 0.22749109\n",
            "Iteration 58, loss = 0.22705557\n",
            "Iteration 59, loss = 0.22521290\n",
            "Iteration 60, loss = 0.22413210\n",
            "Iteration 61, loss = 0.22386931\n",
            "Iteration 62, loss = 0.22309011\n",
            "Iteration 63, loss = 0.22183658\n",
            "Iteration 64, loss = 0.22172459\n",
            "Iteration 65, loss = 0.21960687\n",
            "Iteration 66, loss = 0.21868307\n",
            "Iteration 67, loss = 0.21802851\n",
            "Iteration 68, loss = 0.21782178\n",
            "Iteration 69, loss = 0.21712642\n",
            "Iteration 70, loss = 0.21576178\n",
            "Iteration 71, loss = 0.21463635\n",
            "Iteration 72, loss = 0.21458288\n",
            "Iteration 73, loss = 0.21333001\n",
            "Iteration 74, loss = 0.21235634\n",
            "Iteration 75, loss = 0.21238168\n",
            "Iteration 76, loss = 0.21164374\n",
            "Iteration 77, loss = 0.21078059\n",
            "Iteration 78, loss = 0.20978850\n",
            "Iteration 79, loss = 0.21052352\n",
            "Iteration 80, loss = 0.20838005\n",
            "Iteration 81, loss = 0.20860656\n",
            "Iteration 82, loss = 0.20767399\n",
            "Iteration 83, loss = 0.20664884\n",
            "Iteration 84, loss = 0.20628098\n",
            "Iteration 85, loss = 0.20601053\n",
            "Iteration 86, loss = 0.20513403\n",
            "Iteration 87, loss = 0.20515061\n",
            "Iteration 88, loss = 0.20521598\n",
            "Iteration 89, loss = 0.20407604\n",
            "Iteration 90, loss = 0.20285907\n",
            "Iteration 91, loss = 0.20236848\n",
            "Iteration 92, loss = 0.20283190\n",
            "Iteration 93, loss = 0.20264079\n",
            "Iteration 94, loss = 0.20147125\n",
            "Iteration 95, loss = 0.20036866\n",
            "Iteration 96, loss = 0.19978036\n",
            "Iteration 97, loss = 0.19847376\n",
            "Iteration 98, loss = 0.19885822\n",
            "Iteration 99, loss = 0.19910762\n",
            "Iteration 100, loss = 0.19791282\n",
            "Iteration 101, loss = 0.19710634\n",
            "Iteration 102, loss = 0.19749026\n",
            "Iteration 103, loss = 0.19583551\n",
            "Iteration 104, loss = 0.19568255\n",
            "Iteration 105, loss = 0.19605055\n",
            "Iteration 106, loss = 0.19571198\n",
            "Iteration 107, loss = 0.19426331\n",
            "Iteration 108, loss = 0.19355838\n",
            "Iteration 109, loss = 0.19413854\n",
            "Iteration 110, loss = 0.19319579\n",
            "Iteration 111, loss = 0.19328665\n",
            "Iteration 112, loss = 0.19204357\n",
            "Iteration 113, loss = 0.19292748\n",
            "Iteration 114, loss = 0.19286451\n",
            "Iteration 115, loss = 0.19268889\n",
            "Iteration 116, loss = 0.18965737\n",
            "Iteration 117, loss = 0.19102528\n",
            "Iteration 118, loss = 0.19007799\n",
            "Iteration 119, loss = 0.18923871\n",
            "Iteration 120, loss = 0.18941427\n",
            "Iteration 121, loss = 0.18817534\n",
            "Iteration 122, loss = 0.18861452\n",
            "Iteration 123, loss = 0.18855057\n",
            "Iteration 124, loss = 0.18773712\n",
            "Iteration 125, loss = 0.18724271\n",
            "Iteration 126, loss = 0.18738170\n",
            "Iteration 127, loss = 0.18685467\n",
            "Iteration 128, loss = 0.18685524\n",
            "Iteration 129, loss = 0.18596047\n",
            "Iteration 130, loss = 0.18511928\n",
            "Iteration 131, loss = 0.18473668\n",
            "Iteration 132, loss = 0.18490295\n",
            "Iteration 133, loss = 0.18482302\n",
            "Iteration 134, loss = 0.18408774\n",
            "Iteration 135, loss = 0.18444477\n",
            "Iteration 136, loss = 0.18399439\n",
            "Iteration 137, loss = 0.18439016\n",
            "Iteration 138, loss = 0.18178192\n",
            "Iteration 139, loss = 0.18221082\n",
            "Iteration 140, loss = 0.18347944\n",
            "Iteration 141, loss = 0.18142424\n",
            "Iteration 142, loss = 0.18200528\n",
            "Iteration 143, loss = 0.18032503\n",
            "Iteration 144, loss = 0.18078723\n",
            "Iteration 145, loss = 0.18155108\n",
            "Iteration 146, loss = 0.17951762\n",
            "Iteration 147, loss = 0.17890520\n",
            "Iteration 148, loss = 0.18113142\n",
            "Iteration 149, loss = 0.17929769\n",
            "Iteration 150, loss = 0.17871055\n",
            "Iteration 151, loss = 0.17862009\n",
            "Iteration 152, loss = 0.17887082\n",
            "Iteration 153, loss = 0.17840197\n",
            "Iteration 154, loss = 0.17848206\n",
            "Iteration 155, loss = 0.17778735\n",
            "Iteration 156, loss = 0.17733397\n",
            "Iteration 157, loss = 0.17644667\n",
            "Iteration 158, loss = 0.17554633\n",
            "Iteration 159, loss = 0.17587041\n",
            "Iteration 160, loss = 0.17475777\n",
            "Iteration 161, loss = 0.17550916\n",
            "Iteration 162, loss = 0.17412855\n",
            "Iteration 163, loss = 0.17368788\n",
            "Iteration 164, loss = 0.17440153\n",
            "Iteration 165, loss = 0.17604285\n",
            "Iteration 166, loss = 0.17454130\n",
            "Iteration 167, loss = 0.17441338\n",
            "Iteration 168, loss = 0.17248155\n",
            "Iteration 169, loss = 0.17286822\n",
            "Iteration 170, loss = 0.17258030\n",
            "Iteration 171, loss = 0.17456726\n",
            "Iteration 172, loss = 0.17215375\n",
            "Iteration 173, loss = 0.17116488\n",
            "Iteration 174, loss = 0.17170454\n",
            "Iteration 175, loss = 0.17129310\n",
            "Iteration 176, loss = 0.17214266\n",
            "Iteration 177, loss = 0.17129178\n",
            "Iteration 178, loss = 0.17078886\n",
            "Iteration 179, loss = 0.17005067\n",
            "Iteration 180, loss = 0.17026816\n",
            "Iteration 181, loss = 0.17071253\n",
            "Iteration 182, loss = 0.16907772\n",
            "Iteration 183, loss = 0.16926637\n",
            "Iteration 184, loss = 0.16868587\n",
            "Iteration 185, loss = 0.16686514\n",
            "Iteration 186, loss = 0.16759799\n",
            "Iteration 187, loss = 0.16724557\n",
            "Iteration 188, loss = 0.16888593\n",
            "Iteration 189, loss = 0.16591826\n",
            "Iteration 190, loss = 0.16667772\n",
            "Iteration 191, loss = 0.16704150\n",
            "Iteration 192, loss = 0.16642772\n",
            "Iteration 193, loss = 0.16581704\n",
            "Iteration 194, loss = 0.16514195\n",
            "Iteration 195, loss = 0.16618048\n",
            "Iteration 196, loss = 0.16595042\n",
            "Iteration 197, loss = 0.16544503\n",
            "Iteration 198, loss = 0.16461826\n",
            "Iteration 199, loss = 0.16440377\n",
            "Iteration 200, loss = 0.16354965\n",
            "Iteration 201, loss = 0.16392970\n",
            "Iteration 202, loss = 0.16441634\n",
            "Iteration 203, loss = 0.16545731\n",
            "Iteration 204, loss = 0.16347652\n",
            "Iteration 205, loss = 0.16411981\n",
            "Iteration 206, loss = 0.16324831\n",
            "Iteration 207, loss = 0.16313402\n",
            "Iteration 208, loss = 0.16392744\n",
            "Iteration 209, loss = 0.16268535\n",
            "Iteration 210, loss = 0.16295466\n",
            "Iteration 211, loss = 0.16177569\n",
            "Iteration 212, loss = 0.16151281\n",
            "Iteration 213, loss = 0.16327306\n",
            "Iteration 214, loss = 0.16006547\n",
            "Iteration 215, loss = 0.16037648\n",
            "Iteration 216, loss = 0.16044205\n",
            "Iteration 217, loss = 0.15917314\n",
            "Iteration 218, loss = 0.15919146\n",
            "Iteration 219, loss = 0.15839155\n",
            "Iteration 220, loss = 0.16019796\n",
            "Iteration 221, loss = 0.16040164\n",
            "Iteration 222, loss = 0.15934198\n",
            "Iteration 223, loss = 0.15973058\n",
            "Iteration 224, loss = 0.15785435\n",
            "Iteration 225, loss = 0.15960972\n",
            "Iteration 226, loss = 0.15787227\n",
            "Iteration 227, loss = 0.15784584\n",
            "Iteration 228, loss = 0.15833214\n",
            "Iteration 229, loss = 0.15728091\n",
            "Iteration 230, loss = 0.15739344\n",
            "Iteration 231, loss = 0.15706466\n",
            "Iteration 232, loss = 0.15700148\n",
            "Iteration 233, loss = 0.15790003\n",
            "Iteration 234, loss = 0.15725744\n",
            "Iteration 235, loss = 0.15580075\n",
            "Iteration 236, loss = 0.15839458\n",
            "Iteration 237, loss = 0.15726111\n",
            "Iteration 238, loss = 0.15591657\n",
            "Iteration 239, loss = 0.15515643\n",
            "Iteration 240, loss = 0.15539868\n",
            "Iteration 241, loss = 0.15497427\n",
            "Iteration 242, loss = 0.15643511\n",
            "Iteration 243, loss = 0.15624539\n",
            "Iteration 244, loss = 0.15531670\n",
            "Iteration 245, loss = 0.15499462\n",
            "Iteration 246, loss = 0.15398057\n",
            "Iteration 247, loss = 0.15450666\n",
            "Iteration 248, loss = 0.15353015\n",
            "Iteration 249, loss = 0.15543841\n",
            "Iteration 250, loss = 0.15459709\n",
            "Iteration 251, loss = 0.15535490\n",
            "Iteration 252, loss = 0.15203566\n",
            "Iteration 253, loss = 0.15318394\n",
            "Iteration 254, loss = 0.15362450\n",
            "Iteration 255, loss = 0.15273071\n",
            "Iteration 256, loss = 0.15223483\n",
            "Iteration 257, loss = 0.15220835\n",
            "Iteration 258, loss = 0.15080556\n",
            "Iteration 259, loss = 0.15362253\n",
            "Iteration 260, loss = 0.15286427\n",
            "Iteration 261, loss = 0.15170626\n",
            "Iteration 262, loss = 0.15288784\n",
            "Iteration 263, loss = 0.15267648\n",
            "Iteration 264, loss = 0.15083648\n",
            "Iteration 265, loss = 0.15025947\n",
            "Iteration 266, loss = 0.15095798\n",
            "Iteration 267, loss = 0.15028002\n",
            "Iteration 268, loss = 0.15140006\n",
            "Iteration 269, loss = 0.14954584\n",
            "Iteration 270, loss = 0.15064396\n",
            "Iteration 271, loss = 0.14936227\n",
            "Iteration 272, loss = 0.15300583\n",
            "Iteration 273, loss = 0.15130118\n",
            "Iteration 274, loss = 0.14948061\n",
            "Iteration 275, loss = 0.14894491\n",
            "Iteration 276, loss = 0.14974307\n",
            "Iteration 277, loss = 0.14888415\n",
            "Iteration 278, loss = 0.14940663\n",
            "Iteration 279, loss = 0.14861697\n",
            "Iteration 280, loss = 0.14952714\n",
            "Iteration 281, loss = 0.14926837\n",
            "Iteration 282, loss = 0.14846091\n",
            "Iteration 283, loss = 0.14907308\n",
            "Iteration 284, loss = 0.14816165\n",
            "Iteration 285, loss = 0.14814744\n",
            "Iteration 286, loss = 0.14685994\n",
            "Iteration 287, loss = 0.14689855\n",
            "Iteration 288, loss = 0.14893894\n",
            "Iteration 289, loss = 0.14705700\n",
            "Iteration 290, loss = 0.14649735\n",
            "Iteration 291, loss = 0.14656319\n",
            "Iteration 292, loss = 0.14704970\n",
            "Iteration 293, loss = 0.14789408\n",
            "Iteration 294, loss = 0.14787214\n",
            "Iteration 295, loss = 0.14498737\n",
            "Iteration 296, loss = 0.14697498\n",
            "Iteration 297, loss = 0.14606695\n",
            "Iteration 298, loss = 0.14561783\n",
            "Iteration 299, loss = 0.14719301\n",
            "Iteration 300, loss = 0.14976261\n",
            "Iteration 301, loss = 0.14591644\n",
            "Iteration 302, loss = 0.14553866\n",
            "Iteration 303, loss = 0.14619684\n",
            "Iteration 304, loss = 0.14546559\n",
            "Iteration 305, loss = 0.14939933\n",
            "Iteration 306, loss = 0.14570063\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(55, 55), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMdFEQlmoh2K"
      },
      "source": [
        "previsoes = neural_census.predict(x_census_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijk4IcE0owXM"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1x-C3l0o1UA",
        "outputId": "dd5d126b-4fb5-401f-983b-b4614b67f302"
      },
      "source": [
        "accuracy_score(y_census_teste,previsoes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8153531218014329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8HevOmipLgq",
        "outputId": "51f6291b-4b59-42ea-f3d3-8b20d162ea93"
      },
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cn = ConfusionMatrix(neural_census)\n",
        "cn.fit(x_census_treinamento,y_census_treinamento)\n",
        "cn.score(x_census_teste,y_census_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.38523792\n",
            "Iteration 2, loss = 0.32561202\n",
            "Iteration 3, loss = 0.31436127\n",
            "Iteration 4, loss = 0.30687054\n",
            "Iteration 5, loss = 0.30241272\n",
            "Iteration 6, loss = 0.29868457\n",
            "Iteration 7, loss = 0.29601869\n",
            "Iteration 8, loss = 0.29277627\n",
            "Iteration 9, loss = 0.29027744\n",
            "Iteration 10, loss = 0.28790650\n",
            "Iteration 11, loss = 0.28544269\n",
            "Iteration 12, loss = 0.28371880\n",
            "Iteration 13, loss = 0.28178484\n",
            "Iteration 14, loss = 0.28081370\n",
            "Iteration 15, loss = 0.27838650\n",
            "Iteration 16, loss = 0.27687318\n",
            "Iteration 17, loss = 0.27392052\n",
            "Iteration 18, loss = 0.27327569\n",
            "Iteration 19, loss = 0.27159851\n",
            "Iteration 20, loss = 0.26942476\n",
            "Iteration 21, loss = 0.26737394\n",
            "Iteration 22, loss = 0.26565909\n",
            "Iteration 23, loss = 0.26355290\n",
            "Iteration 24, loss = 0.26253445\n",
            "Iteration 25, loss = 0.26041084\n",
            "Iteration 26, loss = 0.25936926\n",
            "Iteration 27, loss = 0.25921371\n",
            "Iteration 28, loss = 0.25584082\n",
            "Iteration 29, loss = 0.25530552\n",
            "Iteration 30, loss = 0.25325201\n",
            "Iteration 31, loss = 0.25210655\n",
            "Iteration 32, loss = 0.25109447\n",
            "Iteration 33, loss = 0.24981341\n",
            "Iteration 34, loss = 0.24795819\n",
            "Iteration 35, loss = 0.24637270\n",
            "Iteration 36, loss = 0.24539805\n",
            "Iteration 37, loss = 0.24503360\n",
            "Iteration 38, loss = 0.24238644\n",
            "Iteration 39, loss = 0.24195510\n",
            "Iteration 40, loss = 0.24029998\n",
            "Iteration 41, loss = 0.23892670\n",
            "Iteration 42, loss = 0.23787986\n",
            "Iteration 43, loss = 0.23705332\n",
            "Iteration 44, loss = 0.23688455\n",
            "Iteration 45, loss = 0.23547350\n",
            "Iteration 46, loss = 0.23545347\n",
            "Iteration 47, loss = 0.23299159\n",
            "Iteration 48, loss = 0.23266658\n",
            "Iteration 49, loss = 0.23179378\n",
            "Iteration 50, loss = 0.23049046\n",
            "Iteration 51, loss = 0.22949379\n",
            "Iteration 52, loss = 0.22893412\n",
            "Iteration 53, loss = 0.22785173\n",
            "Iteration 54, loss = 0.22635986\n",
            "Iteration 55, loss = 0.22571938\n",
            "Iteration 56, loss = 0.22395603\n",
            "Iteration 57, loss = 0.22355493\n",
            "Iteration 58, loss = 0.22277129\n",
            "Iteration 59, loss = 0.22266072\n",
            "Iteration 60, loss = 0.22154990\n",
            "Iteration 61, loss = 0.21971600\n",
            "Iteration 62, loss = 0.21926949\n",
            "Iteration 63, loss = 0.21900191\n",
            "Iteration 64, loss = 0.21821121\n",
            "Iteration 65, loss = 0.21658895\n",
            "Iteration 66, loss = 0.21570348\n",
            "Iteration 67, loss = 0.21619508\n",
            "Iteration 68, loss = 0.21502533\n",
            "Iteration 69, loss = 0.21401885\n",
            "Iteration 70, loss = 0.21382554\n",
            "Iteration 71, loss = 0.21268500\n",
            "Iteration 72, loss = 0.21261496\n",
            "Iteration 73, loss = 0.21104173\n",
            "Iteration 74, loss = 0.21136330\n",
            "Iteration 75, loss = 0.21110492\n",
            "Iteration 76, loss = 0.20867916\n",
            "Iteration 77, loss = 0.20807052\n",
            "Iteration 78, loss = 0.20844810\n",
            "Iteration 79, loss = 0.20685353\n",
            "Iteration 80, loss = 0.20704598\n",
            "Iteration 81, loss = 0.20611633\n",
            "Iteration 82, loss = 0.20571218\n",
            "Iteration 83, loss = 0.20532299\n",
            "Iteration 84, loss = 0.20292245\n",
            "Iteration 85, loss = 0.20218701\n",
            "Iteration 86, loss = 0.20270326\n",
            "Iteration 87, loss = 0.20247423\n",
            "Iteration 88, loss = 0.20401007\n",
            "Iteration 89, loss = 0.20103230\n",
            "Iteration 90, loss = 0.20100090\n",
            "Iteration 91, loss = 0.19990080\n",
            "Iteration 92, loss = 0.19939217\n",
            "Iteration 93, loss = 0.19875706\n",
            "Iteration 94, loss = 0.19722754\n",
            "Iteration 95, loss = 0.19782047\n",
            "Iteration 96, loss = 0.19696578\n",
            "Iteration 97, loss = 0.19632348\n",
            "Iteration 98, loss = 0.19664373\n",
            "Iteration 99, loss = 0.19607558\n",
            "Iteration 100, loss = 0.19581873\n",
            "Iteration 101, loss = 0.19581963\n",
            "Iteration 102, loss = 0.19574969\n",
            "Iteration 103, loss = 0.19447818\n",
            "Iteration 104, loss = 0.19291926\n",
            "Iteration 105, loss = 0.19398671\n",
            "Iteration 106, loss = 0.19350370\n",
            "Iteration 107, loss = 0.19214511\n",
            "Iteration 108, loss = 0.19110117\n",
            "Iteration 109, loss = 0.19149303\n",
            "Iteration 110, loss = 0.19091025\n",
            "Iteration 111, loss = 0.19097739\n",
            "Iteration 112, loss = 0.18988957\n",
            "Iteration 113, loss = 0.19004566\n",
            "Iteration 114, loss = 0.18884989\n",
            "Iteration 115, loss = 0.18803324\n",
            "Iteration 116, loss = 0.18818166\n",
            "Iteration 117, loss = 0.18667345\n",
            "Iteration 118, loss = 0.18760112\n",
            "Iteration 119, loss = 0.18708511\n",
            "Iteration 120, loss = 0.18654026\n",
            "Iteration 121, loss = 0.18625879\n",
            "Iteration 122, loss = 0.18592963\n",
            "Iteration 123, loss = 0.18536181\n",
            "Iteration 124, loss = 0.18396218\n",
            "Iteration 125, loss = 0.18372630\n",
            "Iteration 126, loss = 0.18449367\n",
            "Iteration 127, loss = 0.18290874\n",
            "Iteration 128, loss = 0.18251948\n",
            "Iteration 129, loss = 0.18284652\n",
            "Iteration 130, loss = 0.18215721\n",
            "Iteration 131, loss = 0.18311571\n",
            "Iteration 132, loss = 0.18068037\n",
            "Iteration 133, loss = 0.18045074\n",
            "Iteration 134, loss = 0.18158015\n",
            "Iteration 135, loss = 0.18095515\n",
            "Iteration 136, loss = 0.18101209\n",
            "Iteration 137, loss = 0.18070074\n",
            "Iteration 138, loss = 0.17852029\n",
            "Iteration 139, loss = 0.18024932\n",
            "Iteration 140, loss = 0.17838708\n",
            "Iteration 141, loss = 0.17896740\n",
            "Iteration 142, loss = 0.17772216\n",
            "Iteration 143, loss = 0.17712444\n",
            "Iteration 144, loss = 0.17801364\n",
            "Iteration 145, loss = 0.17804265\n",
            "Iteration 146, loss = 0.17689148\n",
            "Iteration 147, loss = 0.17640074\n",
            "Iteration 148, loss = 0.17628615\n",
            "Iteration 149, loss = 0.17530467\n",
            "Iteration 150, loss = 0.17492434\n",
            "Iteration 151, loss = 0.17501772\n",
            "Iteration 152, loss = 0.17428896\n",
            "Iteration 153, loss = 0.17552873\n",
            "Iteration 154, loss = 0.17351676\n",
            "Iteration 155, loss = 0.17544547\n",
            "Iteration 156, loss = 0.17390068\n",
            "Iteration 157, loss = 0.17345385\n",
            "Iteration 158, loss = 0.17319189\n",
            "Iteration 159, loss = 0.17344744\n",
            "Iteration 160, loss = 0.17358118\n",
            "Iteration 161, loss = 0.17040180\n",
            "Iteration 162, loss = 0.17118684\n",
            "Iteration 163, loss = 0.17120780\n",
            "Iteration 164, loss = 0.17176278\n",
            "Iteration 165, loss = 0.17172255\n",
            "Iteration 166, loss = 0.16965847\n",
            "Iteration 167, loss = 0.17219009\n",
            "Iteration 168, loss = 0.17017948\n",
            "Iteration 169, loss = 0.16989686\n",
            "Iteration 170, loss = 0.16879910\n",
            "Iteration 171, loss = 0.16986005\n",
            "Iteration 172, loss = 0.16952865\n",
            "Iteration 173, loss = 0.16975258\n",
            "Iteration 174, loss = 0.16995048\n",
            "Iteration 175, loss = 0.16752309\n",
            "Iteration 176, loss = 0.16868368\n",
            "Iteration 177, loss = 0.16861707\n",
            "Iteration 178, loss = 0.16863157\n",
            "Iteration 179, loss = 0.16778361\n",
            "Iteration 180, loss = 0.16890969\n",
            "Iteration 181, loss = 0.16669405\n",
            "Iteration 182, loss = 0.16707751\n",
            "Iteration 183, loss = 0.16840574\n",
            "Iteration 184, loss = 0.16652534\n",
            "Iteration 185, loss = 0.16618571\n",
            "Iteration 186, loss = 0.16586997\n",
            "Iteration 187, loss = 0.16682605\n",
            "Iteration 188, loss = 0.16427977\n",
            "Iteration 189, loss = 0.16478663\n",
            "Iteration 190, loss = 0.16338963\n",
            "Iteration 191, loss = 0.16482217\n",
            "Iteration 192, loss = 0.16393570\n",
            "Iteration 193, loss = 0.16438969\n",
            "Iteration 194, loss = 0.16377706\n",
            "Iteration 195, loss = 0.16463877\n",
            "Iteration 196, loss = 0.16562786\n",
            "Iteration 197, loss = 0.16417260\n",
            "Iteration 198, loss = 0.16270728\n",
            "Iteration 199, loss = 0.16278592\n",
            "Iteration 200, loss = 0.16447645\n",
            "Iteration 201, loss = 0.16248226\n",
            "Iteration 202, loss = 0.16317624\n",
            "Iteration 203, loss = 0.16096773\n",
            "Iteration 204, loss = 0.16257642\n",
            "Iteration 205, loss = 0.16110998\n",
            "Iteration 206, loss = 0.16163221\n",
            "Iteration 207, loss = 0.16219924\n",
            "Iteration 208, loss = 0.16223972\n",
            "Iteration 209, loss = 0.16201830\n",
            "Iteration 210, loss = 0.16130316\n",
            "Iteration 211, loss = 0.15969895\n",
            "Iteration 212, loss = 0.16043145\n",
            "Iteration 213, loss = 0.16155822\n",
            "Iteration 214, loss = 0.15921262\n",
            "Iteration 215, loss = 0.15985928\n",
            "Iteration 216, loss = 0.15902880\n",
            "Iteration 217, loss = 0.15845407\n",
            "Iteration 218, loss = 0.15945402\n",
            "Iteration 219, loss = 0.16001468\n",
            "Iteration 220, loss = 0.15836739\n",
            "Iteration 221, loss = 0.15893507\n",
            "Iteration 222, loss = 0.15914416\n",
            "Iteration 223, loss = 0.15840178\n",
            "Iteration 224, loss = 0.15617444\n",
            "Iteration 225, loss = 0.15649439\n",
            "Iteration 226, loss = 0.15659408\n",
            "Iteration 227, loss = 0.15827300\n",
            "Iteration 228, loss = 0.15735566\n",
            "Iteration 229, loss = 0.15803428\n",
            "Iteration 230, loss = 0.15771382\n",
            "Iteration 231, loss = 0.15650972\n",
            "Iteration 232, loss = 0.15607771\n",
            "Iteration 233, loss = 0.15598860\n",
            "Iteration 234, loss = 0.15642606\n",
            "Iteration 235, loss = 0.15543718\n",
            "Iteration 236, loss = 0.15548101\n",
            "Iteration 237, loss = 0.15616132\n",
            "Iteration 238, loss = 0.15790499\n",
            "Iteration 239, loss = 0.15653172\n",
            "Iteration 240, loss = 0.15537606\n",
            "Iteration 241, loss = 0.15618775\n",
            "Iteration 242, loss = 0.15418270\n",
            "Iteration 243, loss = 0.15520813\n",
            "Iteration 244, loss = 0.15549391\n",
            "Iteration 245, loss = 0.15367144\n",
            "Iteration 246, loss = 0.15557906\n",
            "Iteration 247, loss = 0.15491975\n",
            "Iteration 248, loss = 0.15384749\n",
            "Iteration 249, loss = 0.15386438\n",
            "Iteration 250, loss = 0.15306658\n",
            "Iteration 251, loss = 0.15314662\n",
            "Iteration 252, loss = 0.15210593\n",
            "Iteration 253, loss = 0.15287587\n",
            "Iteration 254, loss = 0.15422591\n",
            "Iteration 255, loss = 0.15428159\n",
            "Iteration 256, loss = 0.15367401\n",
            "Iteration 257, loss = 0.15321845\n",
            "Iteration 258, loss = 0.15266324\n",
            "Iteration 259, loss = 0.15235307\n",
            "Iteration 260, loss = 0.15237891\n",
            "Iteration 261, loss = 0.15503112\n",
            "Iteration 262, loss = 0.15202371\n",
            "Iteration 263, loss = 0.15140749\n",
            "Iteration 264, loss = 0.15204971\n",
            "Iteration 265, loss = 0.15194583\n",
            "Iteration 266, loss = 0.15011000\n",
            "Iteration 267, loss = 0.15073583\n",
            "Iteration 268, loss = 0.15103280\n",
            "Iteration 269, loss = 0.15102166\n",
            "Iteration 272, loss = 0.14993103\n",
            "Iteration 273, loss = 0.14961714\n",
            "Iteration 274, loss = 0.15092041\n",
            "Iteration 275, loss = 0.14895374\n",
            "Iteration 276, loss = 0.14961166\n",
            "Iteration 277, loss = 0.14867388\n",
            "Iteration 278, loss = 0.15038969\n",
            "Iteration 279, loss = 0.14954760\n",
            "Iteration 280, loss = 0.15136967\n",
            "Iteration 281, loss = 0.14904796\n",
            "Iteration 282, loss = 0.14910670\n",
            "Iteration 283, loss = 0.14974466\n",
            "Iteration 284, loss = 0.14803891\n",
            "Iteration 285, loss = 0.14797504\n",
            "Iteration 286, loss = 0.14869065\n",
            "Iteration 287, loss = 0.14799867\n",
            "Iteration 288, loss = 0.15045721\n",
            "Iteration 289, loss = 0.14824452\n",
            "Iteration 290, loss = 0.14884062\n",
            "Iteration 291, loss = 0.14848881\n",
            "Iteration 292, loss = 0.14705337\n",
            "Iteration 293, loss = 0.14746368\n",
            "Iteration 294, loss = 0.14736506\n",
            "Iteration 295, loss = 0.14679137\n",
            "Iteration 296, loss = 0.14798935\n",
            "Iteration 297, loss = 0.14841294\n",
            "Iteration 298, loss = 0.14658229\n",
            "Iteration 299, loss = 0.14830907\n",
            "Iteration 300, loss = 0.14606175\n",
            "Iteration 301, loss = 0.14585169\n",
            "Iteration 302, loss = 0.14512948\n",
            "Iteration 303, loss = 0.14638628\n",
            "Iteration 304, loss = 0.14545325\n",
            "Iteration 305, loss = 0.14765372\n",
            "Iteration 306, loss = 0.14673207\n",
            "Iteration 307, loss = 0.14735320\n",
            "Iteration 308, loss = 0.14732466\n",
            "Iteration 309, loss = 0.14690837\n",
            "Iteration 310, loss = 0.14636363\n",
            "Iteration 311, loss = 0.14447660\n",
            "Iteration 312, loss = 0.14640163\n",
            "Iteration 313, loss = 0.14520147\n",
            "Iteration 314, loss = 0.14458584\n",
            "Iteration 315, loss = 0.14396129\n",
            "Iteration 316, loss = 0.14415576\n",
            "Iteration 317, loss = 0.14653020\n",
            "Iteration 318, loss = 0.14454349\n",
            "Iteration 319, loss = 0.14519474\n",
            "Iteration 320, loss = 0.14379469\n",
            "Iteration 321, loss = 0.14430741\n",
            "Iteration 322, loss = 0.14262978\n",
            "Iteration 323, loss = 0.14320640\n",
            "Iteration 324, loss = 0.14452900\n",
            "Iteration 325, loss = 0.14356388\n",
            "Iteration 326, loss = 0.14411719\n",
            "Iteration 327, loss = 0.14322501\n",
            "Iteration 328, loss = 0.14398677\n",
            "Iteration 329, loss = 0.14434578\n",
            "Iteration 330, loss = 0.14213265\n",
            "Iteration 331, loss = 0.14264019\n",
            "Iteration 332, loss = 0.14425550\n",
            "Iteration 333, loss = 0.14276169\n",
            "Iteration 334, loss = 0.14328380\n",
            "Iteration 335, loss = 0.14194047\n",
            "Iteration 336, loss = 0.14199265\n",
            "Iteration 337, loss = 0.14173106\n",
            "Iteration 338, loss = 0.14223845\n",
            "Iteration 339, loss = 0.13950685\n",
            "Iteration 340, loss = 0.14124593\n",
            "Iteration 341, loss = 0.14089094\n",
            "Iteration 342, loss = 0.14172034\n",
            "Iteration 343, loss = 0.14025817\n",
            "Iteration 344, loss = 0.14362718\n",
            "Iteration 345, loss = 0.14335004\n",
            "Iteration 346, loss = 0.14523695\n",
            "Iteration 347, loss = 0.14106868\n",
            "Iteration 348, loss = 0.14111729\n",
            "Iteration 349, loss = 0.14055624\n",
            "Iteration 350, loss = 0.14116046\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8204708290685773"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}